<!DOCTYPE html>
<html lang=en>
<head>
    <meta charset="utf-8">
    
    <title>Mutual Information | NLPwShiyi Docs</title>
    
    
        <meta name="keywords" content="nlp-theories" />
    
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1" />
    <meta name="description" content="More on Mutual InformationBelow is the code for WAPMI or the Weighted Average Point-wise Mutual Information. And this code measures the distance between two probabilities distributions. It is a method">
<meta property="og:type" content="article">
<meta property="og:title" content="Mutual Information">
<meta property="og:url" content="https://shiyisteezin.github.io/nlp-docs/2024/08/03/mutual-info/index.html">
<meta property="og:site_name" content="NLPwShiyi Docs">
<meta property="og:description" content="More on Mutual InformationBelow is the code for WAPMI or the Weighted Average Point-wise Mutual Information. And this code measures the distance between two probabilities distributions. It is a method">
<meta property="og:locale" content="en_US">
<meta property="article:published_time" content="2024-08-03T12:45:14.000Z">
<meta property="article:modified_time" content="2024-12-05T18:39:15.376Z">
<meta property="article:author" content="Shiyi S">
<meta property="article:tag" content="nlp-theories">
<meta name="twitter:card" content="summary">
    

    

    
        <link rel="icon" href="/nlp-docs/favicon.ico" />
    

    
<link rel="stylesheet" href="/nlp-docs/libs/font-awesome/css/font-awesome.min.css">

    
<link rel="stylesheet" href="/nlp-docs/libs/open-sans/styles.css">

    
<link rel="stylesheet" href="/nlp-docs/libs/source-code-pro/styles.css">


    
<link rel="stylesheet" href="/nlp-docs/css/style.css">

    
<script src="/nlp-docs/libs/jquery/2.1.3/jquery.min.js"></script>

    
<script src="/nlp-docs/libs/jquery/plugins/cookie/1.4.1/jquery.cookie.js"></script>

    
    
        
<link rel="stylesheet" href="/nlp-docs/libs/lightgallery/css/lightgallery.min.css">

    
    
        
<link rel="stylesheet" href="/nlp-docs/libs/justified-gallery/justifiedGallery.min.css">

    
    
    


    


<meta name="generator" content="Hexo 7.3.0"></head>

<body>
    <div id="container">
        <header id="header">
    <div id="header-main" class="header-inner">
        <div class="outer">
            <a href="/nlp-docs/" id="logo">
                
                <span class="site-title">NLPwShiyi Docs</span>
            </a>
            <nav id="main-nav">
                
            </nav>
            
            <div id="search-form-wrap">

    <form class="search-form">
        <input type="text" class="ins-search-input search-form-input" placeholder="Search" />
        <button type="submit" class="search-form-submit"></button>
    </form>
    <div class="ins-search">
    <div class="ins-search-mask"></div>
    <div class="ins-search-container">
        <div class="ins-input-wrapper">
            <input type="text" class="ins-search-input" placeholder="Type something..." />
            <span class="ins-close ins-selectable"><i class="fa fa-times-circle"></i></span>
        </div>
        <div class="ins-section-wrapper">
            <div class="ins-section-container"></div>
        </div>
    </div>
</div>
<script>
(function (window) {
    var INSIGHT_CONFIG = {
        TRANSLATION: {
            POSTS: 'Posts',
            PAGES: 'Pages',
            CATEGORIES: 'Categories',
            TAGS: 'Tags',
            UNTITLED: '(Untitled)',
        },
        ROOT_URL: '/nlp-docs/',
        CONTENT_URL: '/nlp-docs/content.json',
    };
    window.INSIGHT_CONFIG = INSIGHT_CONFIG;
})(window);
</script>

<script src="/nlp-docs/js/insight.js"></script>


</div>

        </div>
    </div>
    <div id="main-nav-mobile" class="header-sub header-inner">
        <table class="menu outer">
            <tr>
                
                <td>
                    
    <div class="search-form">
        <input type="text" class="ins-search-input search-form-input" placeholder="Search" />
    </div>


                </td>
            </tr>
        </table>
    </div>
</header>

        <div class="outer">
            <!-- Left Sidebar -->
            
                <aside id="sidebar-left">
                    <aside id="sidebar">
   
        
    <div class="widget-wrap" id='categories'>
        <h3 class="widget-title">
            <span>categories</span>
            &nbsp;
            <a id='allExpand' href="#">
                <i class="fa fa-angle-double-down fa-2x"></i>
            </a>
        </h3>
        
        
        
         <ul class="unstyled" id="tree" > 
                    <li class="directory">
                        <a href="#" data-role="directory">
                            <i class="fa fa-folder"></i>
                            &nbsp;
                            Linguistics
                        </a>
                         <ul class="unstyled" id="tree" >  <li class="file"><a href="/nlp-docs/2024/07/17/problem-solving/">Problem Solving</a></li>  <li class="file"><a href="/nlp-docs/2024/08/13/philo-o-mind/">Philosophy of Mind</a></li>  <li class="file"><a href="/nlp-docs/2024/09/17/compositionality/">Compositionality</a></li>  </ul> 
                    </li> 
                    
                    <li class="directory open">
                        <a href="#" data-role="directory">
                            <i class="fa fa-folder-open"></i>
                            &nbsp;
                            Math
                        </a>
                         <ul class="unstyled" id="tree" >  <li class="file"><a href="/nlp-docs/2024/07/14/audo-diff/">Auto Differentiation</a></li>  <li class="file"><a href="/nlp-docs/2024/07/16/markov-chains/">Markov Processes</a></li>  <li class="file"><a href="/nlp-docs/2024/07/17/jacobian-matrices/">Jacobian Matrices</a></li>  <li class="file"><a href="/nlp-docs/2024/07/23/viterbi/">Viterbi Algorithm</a></li>  <li class="file"><a href="/nlp-docs/2024/07/27/vae/">Variational Families</a></li>  <li class="file active"><a href="/nlp-docs/2024/08/03/mutual-info/">Mutual Information</a></li>  <li class="file"><a href="/nlp-docs/2024/09/02/determinism/">Intro to Determinism</a></li>  <li class="file"><a href="/nlp-docs/2024/11/18/sgd/">Understanding SGD</a></li>  <li class="file"><a href="/nlp-docs/2024/11/23/measuring-subjectivity/">Measuring Subjectivity</a></li>  </ul> 
                    </li> 
                    
                    <li class="directory">
                        <a href="#" data-role="directory">
                            <i class="fa fa-folder"></i>
                            &nbsp;
                            NLP Related
                        </a>
                         <ul class="unstyled" id="tree" >  <li class="file"><a href="/nlp-docs/2024/08/13/mdn-nlp/">Contemporary NLP</a></li>  <li class="file"><a href="/nlp-docs/2024/08/17/transformer/">The GPT Architecture</a></li>  </ul> 
                    </li> 
                    
                    <li class="directory">
                        <a href="#" data-role="directory">
                            <i class="fa fa-folder"></i>
                            &nbsp;
                            Other
                        </a>
                         <ul class="unstyled" id="tree" >  <li class="file"><a href="/nlp-docs/2024/09/02/intentionality/">Naturalizing Intentions</a></li>  <li class="file"><a href="/nlp-docs/2024/09/02/qulia/">Understanding Qualia</a></li>  <li class="file"><a href="/nlp-docs/2024/09/29/quant-belief/">Quantifying Beliefs</a></li>  </ul> 
                    </li> 
                     <li class="file"><a href="/nlp-docs/2024/07/11/place-holder/">Intro & Overview</a></li>  </ul> 
    </div>
    <script>
        $(document).ready(function() {
            var iconFolderOpenClass  = 'fa-folder-open';
            var iconFolderCloseClass = 'fa-folder';
            var iconAllExpandClass = 'fa-angle-double-down';
            var iconAllPackClass = 'fa-angle-double-up';
            // Handle directory-tree expansion:
            // 左键单独展开目录
            $(document).on('click', '#categories a[data-role="directory"]', function (event) {
                event.preventDefault();

                var icon = $(this).children('.fa');
                var expanded = icon.hasClass(iconFolderOpenClass);
                var subtree = $(this).siblings('ul');
                icon.removeClass(iconFolderOpenClass).removeClass(iconFolderCloseClass);
                if (expanded) {
                    if (typeof subtree != 'undefined') {
                        subtree.slideUp({ duration: 100 });
                    }
                    icon.addClass(iconFolderCloseClass);
                } else {
                    if (typeof subtree != 'undefined') {
                        subtree.slideDown({ duration: 100 });
                    }
                    icon.addClass(iconFolderOpenClass);
                }
            });
            // 右键展开下属所有目录
            $('#categories a[data-role="directory"]').bind("contextmenu", function(event){
                event.preventDefault();
                
                var icon = $(this).children('.fa');
                var expanded = icon.hasClass(iconFolderOpenClass);
                var listNode = $(this).siblings('ul');
                var subtrees = $.merge(listNode.find('li ul'), listNode);
                var icons = $.merge(listNode.find('.fa'), icon);
                icons.removeClass(iconFolderOpenClass).removeClass(iconFolderCloseClass);
                if(expanded) {
                    subtrees.slideUp({ duration: 100 });
                    icons.addClass(iconFolderCloseClass);
                } else {
                    subtrees.slideDown({ duration: 100 });
                    icons.addClass(iconFolderOpenClass);
                }
            })
            // 展开关闭所有目录按钮
            $(document).on('click', '#allExpand', function (event) {
                event.preventDefault();
                
                var icon = $(this).children('.fa');
                var expanded = icon.hasClass(iconAllExpandClass);
                icon.removeClass(iconAllExpandClass).removeClass(iconAllPackClass);
                if(expanded) {
                    $('#sidebar .fa.fa-folder').removeClass('fa-folder').addClass('fa-folder-open')
                    $('#categories li ul').slideDown({ duration: 100 });
                    icon.addClass(iconAllPackClass);
                } else {
                    $('#sidebar .fa.fa-folder-open').removeClass('fa-folder-open').addClass('fa-folder')
                    $('#categories li ul').slideUp({ duration: 100 });
                    icon.addClass(iconAllExpandClass);
                }
            });  
        });
    </script>

    
        
    <div class="widget-wrap" id="archives">
        <h3 class="widget-title"><span>archives</span></h3>
        <div class="widget">
            <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/nlp-docs/archives/2024/11/">November 2024</a><span class="archive-list-count">2</span></li><li class="archive-list-item"><a class="archive-list-link" href="/nlp-docs/archives/2024/09/">September 2024</a><span class="archive-list-count">5</span></li><li class="archive-list-item"><a class="archive-list-link" href="/nlp-docs/archives/2024/08/">August 2024</a><span class="archive-list-count">4</span></li><li class="archive-list-item"><a class="archive-list-link" href="/nlp-docs/archives/2024/07/">July 2024</a><span class="archive-list-count">7</span></li></ul>
        </div>


        <script>
          function toggleVisibility(elementId) {
              var element = document.getElementById(elementId);
              if (element.style.display === 'none' || element.style.display === '') {
                  element.style.display = 'block';
              } else {
                  element.style.display = 'none';
              }
          }

          document.addEventListener('DOMContentLoaded', function() {
              var toggleButton = document.getElementById('archive-widget');
              toggleButton.addEventListener('click', function() {
                  toggleVisibility('archive-widget');
              });
          });

          document.addEventListener('DOMContentLoaded', function() {
          // Check if the .timeline-wrap class is present
          if (document.querySelector('.timeline-wrap')) {
              // If present, hide the #right-sidebar and show the #archives
              var rightSidebar = document.getElementById('right-sidebar');
              var archives = document.getElementById('archives');

              if (rightSidebar) {
                  rightSidebar.style.display = 'none';
              }
              if (archives) {
                  archives.style.display = 'block';
              }
          } else {
              // If .timeline-wrap is not present, ensure the default visibility
              if (rightSidebar) {
                  rightSidebar.style.display = 'block';
              }
              if (archives) {
                  archives.style.display = 'none';
              }
          }
      });


    </script>

    </div>


    
        
    <div class="widget-wrap">
        <h3 class="widget-title"><span>tags</span></h3>
        <div class="widget">
            <ul class="tag-list" itemprop="keywords"><li class="tag-list-item"><a class="tag-list-link" href="/nlp-docs/tags/determinism/" rel="tag">determinism</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/nlp-docs/tags/intro/" rel="tag">intro</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/nlp-docs/tags/math/" rel="tag">math</a><span class="tag-list-count">8</span></li><li class="tag-list-item"><a class="tag-list-link" href="/nlp-docs/tags/nlp-theories/" rel="tag">nlp-theories</a><span class="tag-list-count">5</span></li><li class="tag-list-item"><a class="tag-list-link" href="/nlp-docs/tags/semantics/" rel="tag">semantics</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/nlp-docs/tags/stochastic/" rel="tag">stochastic</a><span class="tag-list-count">4</span></li></ul>
        </div>
    </div>

    
    <div id="toTop" class="fa fa-angle-up"></div>
</aside>
                </aside>
            

            <!-- Main Content -->
            <section id="main">
                <article id="post-mutual-info" class="article article-type-post" itemscope itemprop="blogPost">
    <div class="article-inner">
        
        
            <header class="article-header">
                
                    <div class="article-meta">
                        
    <div class="article-category">
    	<i class="fa fa-folder"></i>
        <a class="article-category-link" href="/nlp-docs/categories/Math/">Math</a>
    </div>

                        
    <div class="article-tag">
        <i class="fa fa-tag"></i>
        <a class="tag-link-link" href="/nlp-docs/tags/nlp-theories/" rel="tag">nlp-theories</a>
    </div>

                        
    <div class="article-date">
        <i class="fa fa-calendar"></i>
        <a href="/nlp-docs/2024/08/03/mutual-info/">
            <time datetime="2024-08-03T12:45:14.000Z" itemprop="datePublished">2024-08-03</time>
        </a>
    </div>


                        
                        
                            <div class="article-meta-button">
                                <a target="_blank" rel="noopener" href='https://github.com/shiyis/Wiki-site/raw/writing/source/_posts/mutual-info.md'> Source </a>
                            </div>
                            <div class="article-meta-button">
                                <a target="_blank" rel="noopener" href='https://github.com/shiyis/Wiki-site/edit/writing/source/_posts/mutual-info.md'> Edit </a>
                            </div>
                            <div class="article-meta-button">
                                <a target="_blank" rel="noopener" href='https://github.com/shiyis/Wiki-site/commits/writing/source/_posts/mutual-info.md'> History </a>
                            </div>
                        
                    </div>
                
                
    
        <h1 class="article-title" itemprop="name">
            Mutual Information
        </h1>
    

            </header>
        
        
        <div class="article-entry" itemprop="articleBody">
        
        
            
        
        
            <h3 id="More-on-Mutual-Information"><a href="#More-on-Mutual-Information" class="headerlink" title="More on Mutual Information"></a>More on Mutual Information</h3><p>Below is the code for WAPMI or the Weighted Average Point-wise Mutual Information. And this code measures the distance between two probabilities distributions.</p>
<p>It is a method used in computational linguistics to measure the strength of association between words in a given context, typically in the analysis of text data.</p>
<p>Imagine you have a box of different colored marbles, and you want to know which colors tend to appear together. WAPMI helps you figure out how often certain words (or colors) appear together more often than by random chance. It’s like a smart way to understand word relationships in sentences!</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> math</span><br><span class="line"><span class="keyword">import</span> collections</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">calculate_pmi</span>(<span class="params">joint_prob, marginal_prob1, marginal_prob2</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Calculate the pointwise mutual information (PMI) between two words.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    :param joint_prob: The joint probability of the two words</span></span><br><span class="line"><span class="string">    :param marginal_prob1: The marginal probability of the first word</span></span><br><span class="line"><span class="string">    :param marginal_prob2: The marginal probability of the second word</span></span><br><span class="line"><span class="string">    :return: The PMI score</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">if</span> joint_prob == <span class="number">0</span> <span class="keyword">or</span> marginal_prob1 == <span class="number">0</span> <span class="keyword">or</span> marginal_prob2 == <span class="number">0</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="number">0</span>  <span class="comment"># Avoid division by zero</span></span><br><span class="line">    <span class="keyword">return</span> math.log(joint_prob / (marginal_prob1 * marginal_prob2), <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">calculate_pmi_corpus_optimized</span>(<span class="params">corpus</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Calculate the PMI scores for all pairs of words in a corpus.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    :param corpus: The corpus of text</span></span><br><span class="line"><span class="string">    :return: A dictionary of PMI scores</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    word_counts = collections.defaultdict(<span class="built_in">int</span>)</span><br><span class="line">    cooccurrence_counts = collections.defaultdict(<span class="built_in">int</span>)</span><br><span class="line">    total_sentences = <span class="built_in">len</span>(corpus)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Precompute word counts and co-occurrence counts</span></span><br><span class="line">    <span class="keyword">for</span> sentence <span class="keyword">in</span> corpus:</span><br><span class="line">        unique_words = <span class="built_in">set</span>(sentence)  <span class="comment"># Avoid counting duplicates within the same sentence</span></span><br><span class="line">        <span class="keyword">for</span> word <span class="keyword">in</span> unique_words:</span><br><span class="line">            word_counts[word] += <span class="number">1</span></span><br><span class="line">        <span class="keyword">for</span> word1 <span class="keyword">in</span> unique_words:</span><br><span class="line">            <span class="keyword">for</span> word2 <span class="keyword">in</span> unique_words:</span><br><span class="line">                <span class="keyword">if</span> word1 != word2:</span><br><span class="line">                    cooccurrence_counts[(word1, word2)] += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Calculate PMI scores</span></span><br><span class="line">    pmi_scores = &#123;&#125;</span><br><span class="line">    <span class="keyword">for</span> (word1, word2), joint_count <span class="keyword">in</span> cooccurrence_counts.items():</span><br><span class="line">        joint_prob = joint_count / total_sentences</span><br><span class="line">        marginal_prob1 = word_counts[word1] / total_sentences</span><br><span class="line">        marginal_prob2 = word_counts[word2] / total_sentences</span><br><span class="line">        pmi = calculate_pmi(joint_prob, marginal_prob1, marginal_prob2)</span><br><span class="line">        pmi_scores[(word1, word2)] = pmi</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> pmi_scores</span><br><span class="line"></span><br><span class="line"><span class="comment"># Example usage</span></span><br><span class="line">corpus = [</span><br><span class="line">    [<span class="string">&quot;this&quot;</span>, <span class="string">&quot;is&quot;</span>, <span class="string">&quot;a&quot;</span>, <span class="string">&quot;foo&quot;</span>, <span class="string">&quot;bar&quot;</span>],</span><br><span class="line">    [<span class="string">&quot;bar&quot;</span>, <span class="string">&quot;black&quot;</span>, <span class="string">&quot;sheep&quot;</span>],</span><br><span class="line">    [<span class="string">&quot;foo&quot;</span>, <span class="string">&quot;bar&quot;</span>, <span class="string">&quot;black&quot;</span>, <span class="string">&quot;sheep&quot;</span>],</span><br><span class="line">    [<span class="string">&quot;sheep&quot;</span>, <span class="string">&quot;bar&quot;</span>, <span class="string">&quot;black&quot;</span>]</span><br><span class="line">]</span><br><span class="line"></span><br><span class="line">pmi_scores = calculate_pmi_corpus_optimized(corpus)</span><br><span class="line"><span class="built_in">print</span>(pmi_scores)</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>A walkthrough of the code part by part.</p>
<h3 id="Imports"><a href="#Imports" class="headerlink" title="Imports"></a><strong>Imports</strong></h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> math</span><br><span class="line"><span class="keyword">import</span> collections</span><br></pre></td></tr></table></figure>

<ul>
<li><strong><code>import math</code></strong>: Imports the <code>math</code> module, which provides mathematical functions such as logarithms.</li>
<li><strong><code>import collections</code></strong>: Imports the <code>collections</code> module, which provides specialized container datatypes, like <code>defaultdict</code>.</li>
</ul>
<h3 id="Function-Definitions"><a href="#Function-Definitions" class="headerlink" title="Function Definitions"></a><strong>Function Definitions</strong></h3><h4 id="1-calculate-pmi-Function"><a href="#1-calculate-pmi-Function" class="headerlink" title="1. calculate_pmi Function"></a><strong>1. <code>calculate_pmi</code> Function</strong></h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">calculate_pmi</span>(<span class="params">word1, word2, joint_prob, marginal_prob1, marginal_prob2</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Calculate the point-wise mutual information (PMI) between two words.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    :param word1: The first word</span></span><br><span class="line"><span class="string">    :param word2: The second word</span></span><br><span class="line"><span class="string">    :param joint_prob: The joint probability of the two words</span></span><br><span class="line"><span class="string">    :param marginal_prob1: The marginal probability of the first word</span></span><br><span class="line"><span class="string">    :param marginal_prob2: The marginal probability of the second word</span></span><br><span class="line"><span class="string">    :return: The PMI score</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    pmi = math.log(joint_prob / (marginal_prob1 * marginal_prob2), <span class="number">2</span>)</span><br><span class="line">    <span class="keyword">return</span> pmi</span><br></pre></td></tr></table></figure>

<ul>
<li><strong>Purpose</strong>: Calculates the Pointwise Mutual Information (PMI) score between two words.</li>
<li><strong>Parameters</strong>:<ul>
<li><code>word1</code> and <code>word2</code>: Words for which PMI is calculated.</li>
<li><code>joint_prob</code>: Probability of both words appearing together.</li>
<li><code>marginal_prob1</code>: Probability of <code>word1</code> appearing.</li>
<li><code>marginal_prob2</code>: Probability of <code>word2</code> appearing.</li>
</ul>
</li>
<li><strong><code>pmi</code> Calculation</strong>:<ul>
<li><code>math.log(joint_prob / (marginal_prob1 * marginal_prob2), 2)</code>: Computes the logarithm (base 2) of the ratio of the joint probability to the product of the marginal probabilities.</li>
</ul>
</li>
<li><strong>Returns</strong>: PMI score.</li>
</ul>
<h4 id="2-calculate-joint-prob-Function"><a href="#2-calculate-joint-prob-Function" class="headerlink" title="2. calculate_joint_prob Function"></a><strong>2. <code>calculate_joint_prob</code> Function</strong></h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">calculate_joint_prob</span>(<span class="params">word1, word2, corpus</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Calculate the joint probability of two words in a corpus.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    :param word1: The first word</span></span><br><span class="line"><span class="string">    :param word2: The second word</span></span><br><span class="line"><span class="string">    :param corpus: The corpus of text</span></span><br><span class="line"><span class="string">    :return: The joint probability</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    joint_count = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> sentence <span class="keyword">in</span> corpus:</span><br><span class="line">        <span class="keyword">if</span> word1 <span class="keyword">in</span> sentence <span class="keyword">and</span> word2 <span class="keyword">in</span> sentence:</span><br><span class="line">            joint_count += <span class="number">1</span></span><br><span class="line">    joint_prob = joint_count / <span class="built_in">len</span>(corpus)</span><br><span class="line">    <span class="keyword">return</span> joint_prob</span><br></pre></td></tr></table></figure>

<ul>
<li><strong>Purpose</strong>: Calculates the joint probability of two words appearing together in the same sentence.</li>
<li><strong>Parameters</strong>:<ul>
<li><code>word1</code> and <code>word2</code>: Words to check.</li>
<li><code>corpus</code>: List of sentences (each sentence is a list of words).</li>
</ul>
</li>
<li><strong><code>joint_count</code></strong>: Counts how many sentences contain both <code>word1</code> and <code>word2</code>.</li>
<li><strong><code>joint_prob</code> Calculation</strong>: Divides <code>joint_count</code> by the total number of sentences to get the joint probability.</li>
<li><strong>Returns</strong>: Joint probability of the two words.</li>
</ul>
<h4 id="3-calculate-marginal-prob-Function"><a href="#3-calculate-marginal-prob-Function" class="headerlink" title="3. calculate_marginal_prob Function"></a><strong>3. <code>calculate_marginal_prob</code> Function</strong></h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">calculate_marginal_prob</span>(<span class="params">word, corpus</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Calculate the marginal probability of a word in a corpus.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    :param word: The word</span></span><br><span class="line"><span class="string">    :param corpus: The corpus of text</span></span><br><span class="line"><span class="string">    :return: The marginal probability</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    word_count = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> sentence <span class="keyword">in</span> corpus:</span><br><span class="line">        <span class="keyword">if</span> word <span class="keyword">in</span> sentence:</span><br><span class="line">            word_count += <span class="number">1</span></span><br><span class="line">    marginal_prob = word_count / <span class="built_in">len</span>(corpus)</span><br><span class="line">    <span class="keyword">return</span> marginal_prob</span><br></pre></td></tr></table></figure>

<ul>
<li><strong>Purpose</strong>: Calculates the marginal probability of a single word.</li>
<li><strong>Parameters</strong>:<ul>
<li><code>word</code>: The word for which probability is calculated.</li>
<li><code>corpus</code>: List of sentences.</li>
</ul>
</li>
<li><strong><code>word_count</code></strong>: Counts how many sentences contain <code>word</code>.</li>
<li><strong><code>marginal_prob</code> Calculation</strong>: Divides <code>word_count</code> by the total number of sentences to get the marginal probability.</li>
<li><strong>Returns</strong>: Marginal probability of the word.</li>
</ul>
<h4 id="4-calculate-pmi-corpus-Function"><a href="#4-calculate-pmi-corpus-Function" class="headerlink" title="4. calculate_pmi_corpus Function"></a><strong>4. <code>calculate_pmi_corpus</code> Function</strong></h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">calculate_pmi_corpus_optimized</span>(<span class="params">corpus</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Calculate the PMI scores for all pairs of words in a corpus.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    :param corpus: The corpus of text</span></span><br><span class="line"><span class="string">    :return: A dictionary of PMI scores</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    word_counts = collections.defaultdict(<span class="built_in">int</span>)</span><br><span class="line">    cooccurrence_counts = collections.defaultdict(<span class="built_in">int</span>)</span><br><span class="line">    total_sentences = <span class="built_in">len</span>(corpus)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Precompute word counts and co-occurrence counts</span></span><br><span class="line">    <span class="keyword">for</span> sentence <span class="keyword">in</span> corpus:</span><br><span class="line">        unique_words = <span class="built_in">set</span>(sentence)  <span class="comment"># Avoid counting duplicates within the same sentence</span></span><br><span class="line">        <span class="keyword">for</span> word <span class="keyword">in</span> unique_words:</span><br><span class="line">            word_counts[word] += <span class="number">1</span></span><br><span class="line">        <span class="keyword">for</span> word1 <span class="keyword">in</span> unique_words:</span><br><span class="line">            <span class="keyword">for</span> word2 <span class="keyword">in</span> unique_words:</span><br><span class="line">                <span class="keyword">if</span> word1 != word2:</span><br><span class="line">                    cooccurrence_counts[(word1, word2)] += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Calculate PMI scores</span></span><br><span class="line">    pmi_scores = &#123;&#125;</span><br><span class="line">    <span class="keyword">for</span> (word1, word2), joint_count <span class="keyword">in</span> cooccurrence_counts.items():</span><br><span class="line">        joint_prob = joint_count / total_sentences</span><br><span class="line">        marginal_prob1 = word_counts[word1] / total_sentences</span><br><span class="line">        marginal_prob2 = word_counts[word2] / total_sentences</span><br><span class="line">        pmi = calculate_pmi(joint_prob, marginal_prob1, marginal_prob2)</span><br><span class="line">        pmi_scores[(word1, word2)] = pmi</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> pmi_scores</span><br></pre></td></tr></table></figure>

<ul>
<li><strong>Purpose</strong>: Calculates PMI scores for all pairs of words in the corpus.</li>
<li><strong>Parameters</strong>:<ul>
<li><code>corpus</code>: List of sentences.</li>
</ul>
</li>
<li><strong><code>word_counts</code></strong>: A <code>defaultdict</code> to count occurrences of each word.</li>
<li><strong>Count Words</strong>: Iterates over each sentence to count occurrences of each word.</li>
<li><strong>Compute PMI</strong>:<ul>
<li>Iterates over all pairs of words (excluding pairs where <code>word1</code> is the same as <code>word2</code>).</li>
<li>Calculates joint and marginal probabilities, then computes PMI for each pair.</li>
</ul>
</li>
<li><strong>Returns</strong>: A dictionary of PMI scores for all word pairs.</li>
</ul>
<h3 id="Example-Usage"><a href="#Example-Usage" class="headerlink" title="Example Usage"></a><strong>Example Usage</strong></h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">corpus = [</span><br><span class="line">    [<span class="string">&quot;this&quot;</span>, <span class="string">&quot;is&quot;</span>, <span class="string">&quot;a&quot;</span>, <span class="string">&quot;foo&quot;</span>, <span class="string">&quot;bar&quot;</span>],</span><br><span class="line">    [<span class="string">&quot;bar&quot;</span>, <span class="string">&quot;black&quot;</span>, <span class="string">&quot;sheep&quot;</span>],</span><br><span class="line">    [<span class="string">&quot;foo&quot;</span>, <span class="string">&quot;bar&quot;</span>, <span class="string">&quot;black&quot;</span>, <span class="string">&quot;sheep&quot;</span>],</span><br><span class="line">    [<span class="string">&quot;sheep&quot;</span>, <span class="string">&quot;bar&quot;</span>, <span class="string">&quot;black&quot;</span>]</span><br><span class="line">]</span><br><span class="line"></span><br><span class="line">pmi_scores = calculate_pmi_corpus(corpus)</span><br><span class="line"><span class="built_in">print</span>(pmi_scores)</span><br></pre></td></tr></table></figure>

<ul>
<li><strong>Purpose</strong>: Runs the PMI calculations on a sample corpus and prints the PMI scores for all word pairs.</li>
</ul>
<h3 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a><strong>Summary</strong></h3><ul>
<li><strong><code>calculate_pmi</code></strong> computes PMI given probabilities.</li>
<li><strong><code>calculate_joint_prob</code></strong> finds how often two words appear together.</li>
<li><strong><code>calculate_marginal_prob</code></strong> finds how often one word appears.</li>
<li><strong><code>calculate_pmi_corpus</code></strong> calculates PMI for all word pairs in a corpus.</li>
</ul>
<p>This code helps measure how strongly two words are associated compared to what you would expect by chance.</p>
<h3 id="Comparing-Mutural-Informaiton-WAPMI-VAE-and-KL-Divergence"><a href="#Comparing-Mutural-Informaiton-WAPMI-VAE-and-KL-Divergence" class="headerlink" title="Comparing Mutural Informaiton, WAPMI, VAE, and KL Divergence."></a>Comparing Mutural Informaiton, WAPMI, VAE, and KL Divergence.</h3><p>Let’s break down Wappmi, mutual information, and how they relate to Variational Autoencoders (VAEs) and KL divergence in simple terms, like you’re five.</p>
<h3 id="Mutual-Information"><a href="#Mutual-Information" class="headerlink" title="Mutual Information"></a>Mutual Information</h3><p>Imagine you have two sets of toys, and you want to know how much one set tells you about the other. Mutual information is a way to measure how much knowing about one set of toys helps you predict what’s in the other set. If you always find a red truck when you find a blue car, that’s high mutual information.</p>
<h3 id="Wappmi-Weighted-Average-Prediction-Pointwise-Mutual-Information"><a href="#Wappmi-Weighted-Average-Prediction-Pointwise-Mutual-Information" class="headerlink" title="Wappmi (Weighted Average Prediction Pointwise Mutual Information)"></a>Wappmi (Weighted Average Prediction Pointwise Mutual Information)</h3><p>Now, Wappmi takes this idea of mutual information and looks at words in sentences. It asks, “How often do these words appear together, and is it more than just by chance?” It’s like seeing if certain toys always end up next to each other more than they would randomly.</p>
<h3 id="Variational-Autoencoders-VAEs-and-KL-Divergence"><a href="#Variational-Autoencoders-VAEs-and-KL-Divergence" class="headerlink" title="Variational Autoencoders (VAEs) and KL Divergence"></a>Variational Autoencoders (VAEs) and KL Divergence</h3><p>Imagine you have a machine that tries to guess which toys you might pull out of a box based on previous toys. VAEs are like that machine—they learn patterns to predict what comes next.</p>
<p>The KL Divergence (Kullback-Leibler Divergence) is a way for the machine to measure how different its guesses (predictions) are from what actually happens. It helps the machine get better by adjusting its guesses to be more like what it observes.</p>
<h3 id="How-They-Relate"><a href="#How-They-Relate" class="headerlink" title="How They Relate"></a>How They Relate</h3><ul>
<li><strong>Mutual Information:</strong> Helps understand how related different pieces of data are, like words in a sentence.</li>
<li><strong>Wappmi:</strong> Uses the idea of mutual information to find strong word pairings in text.</li>
<li><strong>VAE:</strong> Tries to model data (like sentences) to understand it better.</li>
<li><strong>KL Divergence:</strong> Helps the VAE improve its understanding by comparing its guesses to reality and adjusting accordingly.</li>
</ul>
<h3 id="Example-Code-Walkthrough"><a href="#Example-Code-Walkthrough" class="headerlink" title="Example Code Walkthrough"></a>Example Code Walkthrough</h3><p>Let’s imagine you have a simple code where you’re trying to guess if two words often appear together:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> collections</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_stats</span>(<span class="params">vocab</span>):</span><br><span class="line">    pairs = collections.defaultdict(<span class="built_in">int</span>)</span><br><span class="line">    <span class="keyword">for</span> word, freq <span class="keyword">in</span> vocab.items():</span><br><span class="line">        symbols = word.split()</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(symbols) - <span class="number">1</span>):</span><br><span class="line">            pairs[symbols[i], symbols[i + <span class="number">1</span>]] += freq</span><br><span class="line">    <span class="keyword">return</span> pairs</span><br><span class="line"></span><br><span class="line">vocab = &#123;<span class="string">&#x27;new&#x27;</span>: <span class="number">2</span>, <span class="string">&#x27;news&#x27;</span>: <span class="number">6</span>, <span class="string">&#x27;newer&#x27;</span>: <span class="number">3</span>&#125;</span><br><span class="line">pairs = get_stats(vocab)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(pairs)</span><br></pre></td></tr></table></figure>

<p>This code checks how often pairs of words (or symbols) appear together. If “new” and “news” show up together a lot, the program will tell you. This is similar to how Wappmi works.</p>
<p>In a VAE, you might use something like KL divergence to measure how far off your model’s guesses are from reality, then adjust it to improve. The model might then get better at understanding the relationships between words (like mutual information).</p>
<p>This approach helps in building smarter models that can understand and predict data more accurately.</p>
<h3 id="Simple-code-demonstration-of-each-one-of-them"><a href="#Simple-code-demonstration-of-each-one-of-them" class="headerlink" title="Simple code demonstration of each one of them"></a>Simple code demonstration of each one of them</h3><h3 id="1-Mutual-Information"><a href="#1-Mutual-Information" class="headerlink" title="1. Mutual Information"></a>1. <strong>Mutual Information</strong></h3><p><strong>Concept</strong>:<br>Mutual Information (MI) measures how much knowing one random variable reduces uncertainty about another. It quantifies the “shared information” between two variables.</p>
<p><strong>Code Example</strong>:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> mutual_info_score</span><br><span class="line"></span><br><span class="line"><span class="comment"># Example data</span></span><br><span class="line">X = [<span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>]</span><br><span class="line">Y = [<span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># Calculate mutual information</span></span><br><span class="line">mi = mutual_info_score(X, Y)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&#x27;Mutual Information: <span class="subst">&#123;mi&#125;</span>&#x27;</span>)</span><br></pre></td></tr></table></figure>

<p><strong>Explanation</strong>:<br>In this example, <code>mutual_info_score</code> from <code>sklearn</code> calculates the MI between two lists <code>X</code> and <code>Y</code>. MI quantifies how much knowing <code>X</code> helps predict <code>Y</code>.</p>
<h3 id="2-WAPPMI-Weighted-Average-Pointwise-Mutual-Information"><a href="#2-WAPPMI-Weighted-Average-Pointwise-Mutual-Information" class="headerlink" title="2. WAPPMI (Weighted Average Pointwise Mutual Information)"></a>2. <strong>WAPPMI (Weighted Average Pointwise Mutual Information)</strong></h3><p><strong>Concept</strong>:<br>WAPPMI is used in NLP to find out how often words co-occur in a text corpus beyond what would be expected by chance. It gives more weight to frequently co-occurring pairs.</p>
<p><strong>Code Example</strong>:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> collections</span><br><span class="line"><span class="keyword">import</span> math</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">pmi</span>(<span class="params">x, y, corpus</span>):</span><br><span class="line">    px = corpus.count(x) / <span class="built_in">len</span>(corpus)</span><br><span class="line">    py = corpus.count(y) / <span class="built_in">len</span>(corpus)</span><br><span class="line">    pxy = corpus.count(x + <span class="string">&#x27; &#x27;</span> + y) / <span class="built_in">len</span>(corpus)</span><br><span class="line">    <span class="keyword">return</span> math.log(pxy / (px * py), <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">corpus = <span class="string">&quot;this is a simple corpus with some simple words in this simple text&quot;</span></span><br><span class="line">pairs = collections.defaultdict(<span class="built_in">int</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Example calculation</span></span><br><span class="line">words = corpus.split()</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(words) - <span class="number">1</span>):</span><br><span class="line">    pairs[(words[i], words[i + <span class="number">1</span>])] += <span class="number">1</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> pair, freq <span class="keyword">in</span> pairs.items():</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&#x27;PMI(<span class="subst">&#123;pair&#125;</span>): <span class="subst">&#123;pmi(pair[<span class="number">0</span>], pair[<span class="number">1</span>], corpus)&#125;</span>&#x27;</span>)</span><br></pre></td></tr></table></figure>

<p><strong>Explanation</strong>:<br>The PMI function calculates the pointwise mutual information between word pairs in the <code>corpus</code>. WAPPMI extends this by weighting these values.</p>
<h3 id="3-Variational-Autoencoder-VAE"><a href="#3-Variational-Autoencoder-VAE" class="headerlink" title="3. Variational Autoencoder (VAE)"></a>3. <strong>Variational Autoencoder (VAE)</strong></h3><p><strong>Concept</strong>:<br>VAEs are used to generate data that’s similar to a given dataset. They work by learning a probabilistic model of the data and then sampling from it.</p>
<p><strong>Code Example</strong>:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">VAE</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, input_dim, hidden_dim, z_dim</span>):</span><br><span class="line">        <span class="built_in">super</span>(VAE, <span class="variable language_">self</span>).__init__()</span><br><span class="line">        <span class="variable language_">self</span>.encoder = nn.Sequential(</span><br><span class="line">            nn.Linear(input_dim, hidden_dim),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.Linear(hidden_dim, z_dim * <span class="number">2</span>)</span><br><span class="line">        )</span><br><span class="line">        <span class="variable language_">self</span>.decoder = nn.Sequential(</span><br><span class="line">            nn.Linear(z_dim, hidden_dim),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.Linear(hidden_dim, input_dim),</span><br><span class="line">            nn.Sigmoid()</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        mu_logvar = <span class="variable language_">self</span>.encoder(x)</span><br><span class="line">        mu, logvar = mu_logvar.chunk(<span class="number">2</span>, dim=-<span class="number">1</span>)</span><br><span class="line">        std = torch.exp(<span class="number">0.5</span> * logvar)</span><br><span class="line">        z = mu + std * torch.randn_like(std)</span><br><span class="line">        <span class="keyword">return</span> <span class="variable language_">self</span>.decoder(z), mu, logvar</span><br><span class="line"></span><br><span class="line"><span class="comment"># Instantiate and run the model</span></span><br><span class="line">vae = VAE(input_dim=<span class="number">784</span>, hidden_dim=<span class="number">400</span>, z_dim=<span class="number">20</span>)</span><br><span class="line">sample_input = torch.randn((<span class="number">64</span>, <span class="number">784</span>))</span><br><span class="line">output, mu, logvar = vae(sample_input)</span><br></pre></td></tr></table></figure>

<p><strong>Explanation</strong>:<br>The VAE class in this example defines an encoder and decoder. The encoder outputs a mean (<code>mu</code>) and log-variance (<code>logvar</code>) for a latent variable <code>z</code>. The decoder reconstructs the input data from <code>z</code>. The KL divergence between the latent distribution and a normal distribution helps regularize the model.</p>
<h3 id="4-KL-Divergence"><a href="#4-KL-Divergence" class="headerlink" title="4. KL Divergence"></a>4. <strong>KL Divergence</strong></h3><p><strong>Concept</strong>:<br>KL Divergence measures how one probability distribution diverges from a second, expected probability distribution. It’s used in VAEs to ensure that the latent variables follow a desired distribution (usually a Gaussian).</p>
<p><strong>Code Example</strong>:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line"><span class="comment"># KL Divergence between the prior (standard normal) and the approximate posterior (q(z|x))</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">kl_divergence</span>(<span class="params">mu, logvar</span>):</span><br><span class="line">    <span class="keyword">return</span> -<span class="number">0.5</span> * torch.<span class="built_in">sum</span>(<span class="number">1</span> + logvar - mu.<span class="built_in">pow</span>(<span class="number">2</span>) - logvar.exp())</span><br><span class="line"></span><br><span class="line">mu = torch.zeros(<span class="number">64</span>, <span class="number">20</span>)</span><br><span class="line">logvar = torch.zeros(<span class="number">64</span>, <span class="number">20</span>)</span><br><span class="line">kl = kl_divergence(mu, logvar)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&#x27;KL Divergence: <span class="subst">&#123;kl&#125;</span>&#x27;</span>)</span><br></pre></td></tr></table></figure>

<p><strong>Explanation</strong>:<br>This code calculates the KL Divergence for a batch of latent variables with <code>mu</code> and <code>logvar</code> as parameters. It shows how much the approximate posterior (the learned distribution) diverges from the prior (standard normal distribution).</p>
<h3 id="Comparison"><a href="#Comparison" class="headerlink" title="Comparison:"></a><strong>Comparison</strong>:</h3><ul>
<li><strong>Mutual Information</strong> tells us how much knowing one variable helps us know another.</li>
<li><strong>WAPPMI</strong> applies this idea to words in text, showing how often they appear together.</li>
<li><strong>VAEs</strong> are generative models that learn to create data similar to a training set. They use <strong>KL Divergence</strong> to ensure the generated data follows a specific distribution.</li>
</ul>
<p><strong>Overall</strong>, these concepts are connected through their use in understanding and modeling the relationships within data, whether through measuring information (MI, WAPPMI) or generating and adjusting data distributions (VAEs, KL Divergence).</p>

            </div>
        
        <footer class="article-footer">
        </footer>
    </div>
</article>


    
<nav id="article-nav">
    
        <a href="/nlp-docs/2024/08/13/philo-o-mind/" id="article-nav-newer" class="article-nav-link-wrap">
            <strong class="article-nav-caption">Newer</strong>
            <div class="article-nav-title">
                
                    Philosophy of Mind
                
            </div>
        </a>
    
    
        <a href="/nlp-docs/2024/07/27/vae/" id="article-nav-older" class="article-nav-link-wrap">
            <strong class="article-nav-caption">Older</strong>
            <div class="article-nav-title">Variational Families</div>
        </a>
    
</nav>





    
    




<!-- baidu url auto push script -->
<script type="text/javascript">
    !function(){var e=/([http|https]:\/\/[a-zA-Z0-9\_\.]+\.baidu\.com)/gi,r=window.location.href,o=document.referrer;if(!e.test(r)){var n="//api.share.baidu.com/s.gif";o?(n+="?r="+encodeURIComponent(document.referrer),r&&(n+="&l="+r)):r&&(n+="?l="+r);var t=new Image;t.src=n}}(window);
</script>     

            </section>

            <!-- Right Sidebar -->
            <aside id="sidebar-right">
                <aside id="right-sidebar">
    <div class="timeline-container">
        
                <!-- Year Marker -->
                <div class="timeline-row timeline-year">
                    <span class="timeline-icon"><i class="fa fa-calendar-o"></i></span>
                    <h2 class="timeline-title">2024</h2>
                </div>
        
                <!-- Month Marker -->
                <div class="timeline-row timeline-month">
                    <span class="timeline-icon"><i class="fa fa-calendar"></i></span>
                    <h3 class="timeline-title">2024-11 (2)</h3>
                </div>
                <div class="timeline-posts"> <!-- Open a new month's timeline-posts -->
        

            <!-- Post Entry -->
            <div class="timeline-row timeline-post">
                <span class="timeline-icon"></span>
                <div class="timeline-content">
                    <h4 class="timeline-post-title"><a href="https://shiyisteezin.github.io/nlp-docs/2024/11/23/measuring-subjectivity/">Measuring Subjectivity</a></h4>
                    <div class="timeline-post-meta">
                        <span><i class="fa fa-calendar"></i> 2024-11-23</span>
                        <span><i class="fa fa-folder-open"></i> Math</span>
                    </div>
                </div>
            </div>

        

            <!-- Post Entry -->
            <div class="timeline-row timeline-post">
                <span class="timeline-icon"></span>
                <div class="timeline-content">
                    <h4 class="timeline-post-title"><a href="https://shiyisteezin.github.io/nlp-docs/2024/11/18/sgd/">Understanding SGD</a></h4>
                    <div class="timeline-post-meta">
                        <span><i class="fa fa-calendar"></i> 2024-11-18</span>
                        <span><i class="fa fa-folder-open"></i> Math</span>
                    </div>
                </div>
            </div>

        
                    </div> <!-- Close previous month's timeline-posts -->
                
                <!-- Month Marker -->
                <div class="timeline-row timeline-month">
                    <span class="timeline-icon"><i class="fa fa-calendar"></i></span>
                    <h3 class="timeline-title">2024-09 (5)</h3>
                </div>
                <div class="timeline-posts"> <!-- Open a new month's timeline-posts -->
        

            <!-- Post Entry -->
            <div class="timeline-row timeline-post">
                <span class="timeline-icon"></span>
                <div class="timeline-content">
                    <h4 class="timeline-post-title"><a href="https://shiyisteezin.github.io/nlp-docs/2024/09/29/quant-belief/">Quantifying Beliefs</a></h4>
                    <div class="timeline-post-meta">
                        <span><i class="fa fa-calendar"></i> 2024-09-29</span>
                        <span><i class="fa fa-folder-open"></i> Other</span>
                    </div>
                </div>
            </div>

        

            <!-- Post Entry -->
            <div class="timeline-row timeline-post">
                <span class="timeline-icon"></span>
                <div class="timeline-content">
                    <h4 class="timeline-post-title"><a href="https://shiyisteezin.github.io/nlp-docs/2024/09/17/compositionality/">Compositionality</a></h4>
                    <div class="timeline-post-meta">
                        <span><i class="fa fa-calendar"></i> 2024-09-17</span>
                        <span><i class="fa fa-folder-open"></i> Linguistics</span>
                    </div>
                </div>
            </div>

        

            <!-- Post Entry -->
            <div class="timeline-row timeline-post">
                <span class="timeline-icon"></span>
                <div class="timeline-content">
                    <h4 class="timeline-post-title"><a href="https://shiyisteezin.github.io/nlp-docs/2024/09/02/determinism/">Intro to Determinism</a></h4>
                    <div class="timeline-post-meta">
                        <span><i class="fa fa-calendar"></i> 2024-09-02</span>
                        <span><i class="fa fa-folder-open"></i> Math</span>
                    </div>
                </div>
            </div>

        

            <!-- Post Entry -->
            <div class="timeline-row timeline-post">
                <span class="timeline-icon"></span>
                <div class="timeline-content">
                    <h4 class="timeline-post-title"><a href="https://shiyisteezin.github.io/nlp-docs/2024/09/02/intentionality/">Naturalizing Intentions</a></h4>
                    <div class="timeline-post-meta">
                        <span><i class="fa fa-calendar"></i> 2024-09-02</span>
                        <span><i class="fa fa-folder-open"></i> Other</span>
                    </div>
                </div>
            </div>

        

            <!-- Post Entry -->
            <div class="timeline-row timeline-post">
                <span class="timeline-icon"></span>
                <div class="timeline-content">
                    <h4 class="timeline-post-title"><a href="https://shiyisteezin.github.io/nlp-docs/2024/09/02/qulia/">Understanding Qualia</a></h4>
                    <div class="timeline-post-meta">
                        <span><i class="fa fa-calendar"></i> 2024-09-02</span>
                        <span><i class="fa fa-folder-open"></i> Other</span>
                    </div>
                </div>
            </div>

        
                    </div> <!-- Close previous month's timeline-posts -->
                
                <!-- Month Marker -->
                <div class="timeline-row timeline-month">
                    <span class="timeline-icon"><i class="fa fa-calendar"></i></span>
                    <h3 class="timeline-title">2024-08 (4)</h3>
                </div>
                <div class="timeline-posts"> <!-- Open a new month's timeline-posts -->
        

            <!-- Post Entry -->
            <div class="timeline-row timeline-post">
                <span class="timeline-icon"></span>
                <div class="timeline-content">
                    <h4 class="timeline-post-title"><a href="https://shiyisteezin.github.io/nlp-docs/2024/08/17/transformer/">The GPT Architecture</a></h4>
                    <div class="timeline-post-meta">
                        <span><i class="fa fa-calendar"></i> 2024-08-17</span>
                        <span><i class="fa fa-folder-open"></i> NLP Related</span>
                    </div>
                </div>
            </div>

        

            <!-- Post Entry -->
            <div class="timeline-row timeline-post">
                <span class="timeline-icon"></span>
                <div class="timeline-content">
                    <h4 class="timeline-post-title"><a href="https://shiyisteezin.github.io/nlp-docs/2024/08/13/mdn-nlp/">Contemporary NLP</a></h4>
                    <div class="timeline-post-meta">
                        <span><i class="fa fa-calendar"></i> 2024-08-13</span>
                        <span><i class="fa fa-folder-open"></i> NLP Related</span>
                    </div>
                </div>
            </div>

        

            <!-- Post Entry -->
            <div class="timeline-row timeline-post">
                <span class="timeline-icon"></span>
                <div class="timeline-content">
                    <h4 class="timeline-post-title"><a href="https://shiyisteezin.github.io/nlp-docs/2024/08/13/philo-o-mind/">Philosophy of Mind</a></h4>
                    <div class="timeline-post-meta">
                        <span><i class="fa fa-calendar"></i> 2024-08-13</span>
                        <span><i class="fa fa-folder-open"></i> Linguistics</span>
                    </div>
                </div>
            </div>

        

            <!-- Post Entry -->
            <div class="timeline-row timeline-post">
                <span class="timeline-icon"></span>
                <div class="timeline-content">
                    <h4 class="timeline-post-title"><a href="https://shiyisteezin.github.io/nlp-docs/2024/08/03/mutual-info/">Mutual Information</a></h4>
                    <div class="timeline-post-meta">
                        <span><i class="fa fa-calendar"></i> 2024-08-03</span>
                        <span><i class="fa fa-folder-open"></i> Math</span>
                    </div>
                </div>
            </div>

        
                    </div> <!-- Close previous month's timeline-posts -->
                
                <!-- Month Marker -->
                <div class="timeline-row timeline-month">
                    <span class="timeline-icon"><i class="fa fa-calendar"></i></span>
                    <h3 class="timeline-title">2024-07 (7)</h3>
                </div>
                <div class="timeline-posts"> <!-- Open a new month's timeline-posts -->
        

            <!-- Post Entry -->
            <div class="timeline-row timeline-post">
                <span class="timeline-icon"></span>
                <div class="timeline-content">
                    <h4 class="timeline-post-title"><a href="https://shiyisteezin.github.io/nlp-docs/2024/07/27/vae/">Variational Families</a></h4>
                    <div class="timeline-post-meta">
                        <span><i class="fa fa-calendar"></i> 2024-07-27</span>
                        <span><i class="fa fa-folder-open"></i> Math</span>
                    </div>
                </div>
            </div>

        

            <!-- Post Entry -->
            <div class="timeline-row timeline-post">
                <span class="timeline-icon"></span>
                <div class="timeline-content">
                    <h4 class="timeline-post-title"><a href="https://shiyisteezin.github.io/nlp-docs/2024/07/23/viterbi/">Viterbi Algorithm</a></h4>
                    <div class="timeline-post-meta">
                        <span><i class="fa fa-calendar"></i> 2024-07-23</span>
                        <span><i class="fa fa-folder-open"></i> Math</span>
                    </div>
                </div>
            </div>

        

            <!-- Post Entry -->
            <div class="timeline-row timeline-post">
                <span class="timeline-icon"></span>
                <div class="timeline-content">
                    <h4 class="timeline-post-title"><a href="https://shiyisteezin.github.io/nlp-docs/2024/07/17/jacobian-matrices/">Jacobian Matrices</a></h4>
                    <div class="timeline-post-meta">
                        <span><i class="fa fa-calendar"></i> 2024-07-17</span>
                        <span><i class="fa fa-folder-open"></i> Math</span>
                    </div>
                </div>
            </div>

        

            <!-- Post Entry -->
            <div class="timeline-row timeline-post">
                <span class="timeline-icon"></span>
                <div class="timeline-content">
                    <h4 class="timeline-post-title"><a href="https://shiyisteezin.github.io/nlp-docs/2024/07/17/problem-solving/">Problem Solving</a></h4>
                    <div class="timeline-post-meta">
                        <span><i class="fa fa-calendar"></i> 2024-07-17</span>
                        <span><i class="fa fa-folder-open"></i> Linguistics</span>
                    </div>
                </div>
            </div>

        

            <!-- Post Entry -->
            <div class="timeline-row timeline-post">
                <span class="timeline-icon"></span>
                <div class="timeline-content">
                    <h4 class="timeline-post-title"><a href="https://shiyisteezin.github.io/nlp-docs/2024/07/16/markov-chains/">Markov Processes</a></h4>
                    <div class="timeline-post-meta">
                        <span><i class="fa fa-calendar"></i> 2024-07-16</span>
                        <span><i class="fa fa-folder-open"></i> Math</span>
                    </div>
                </div>
            </div>

        

            <!-- Post Entry -->
            <div class="timeline-row timeline-post">
                <span class="timeline-icon"></span>
                <div class="timeline-content">
                    <h4 class="timeline-post-title"><a href="https://shiyisteezin.github.io/nlp-docs/2024/07/14/audo-diff/">Auto Differentiation</a></h4>
                    <div class="timeline-post-meta">
                        <span><i class="fa fa-calendar"></i> 2024-07-14</span>
                        <span><i class="fa fa-folder-open"></i> Math</span>
                    </div>
                </div>
            </div>

        

            <!-- Post Entry -->
            <div class="timeline-row timeline-post">
                <span class="timeline-icon"></span>
                <div class="timeline-content">
                    <h4 class="timeline-post-title"><a href="https://shiyisteezin.github.io/nlp-docs/2024/07/11/place-holder/">Intro &amp; Overview</a></h4>
                    <div class="timeline-post-meta">
                        <span><i class="fa fa-calendar"></i> 2024-07-11</span>
                        <span><i class="fa fa-folder-open"></i> </span>
                    </div>
                </div>
            </div>

        
        </div> <!-- Close the last month's timeline-posts -->
    </div>
</aside>

            </aside>

        </div>
        <footer id="footer">
    <div class="outer">
        <div id="footer-info" class="inner">
            Shiyi S &copy; 2024 
            <a rel="license noopener" target="_blank" href="http://creativecommons.org/licenses/by-nc-nd/4.0/"><img alt="Creative Commons License" style="border-width:0" src="https://i.creativecommons.org/l/by-nc-nd/4.0/80x15.png" /></a>
            <br> Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>. Theme - <a target="_blank" rel="noopener" href="https://github.com/zthxxx/hexo-theme-Wikitten">wikitten</a>
            
        </div>
    </div>
</footer>

        
    
        
<script src="/nlp-docs/libs/lightgallery/js/lightgallery.min.js"></script>

        
<script src="/nlp-docs/libs/lightgallery/js/lg-thumbnail.min.js"></script>

        
<script src="/nlp-docs/libs/lightgallery/js/lg-pager.min.js"></script>

        
<script src="/nlp-docs/libs/lightgallery/js/lg-autoplay.min.js"></script>

        
<script src="/nlp-docs/libs/lightgallery/js/lg-fullscreen.min.js"></script>

        
<script src="/nlp-docs/libs/lightgallery/js/lg-zoom.min.js"></script>

        
<script src="/nlp-docs/libs/lightgallery/js/lg-hash.min.js"></script>

        
<script src="/nlp-docs/libs/lightgallery/js/lg-share.min.js"></script>

        
<script src="/nlp-docs/libs/lightgallery/js/lg-video.min.js"></script>

    
    
        
<script src="/nlp-docs/libs/justified-gallery/jquery.justifiedGallery.min.js"></script>

    
    
        <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ["$","$"], ["\\(","\\)"] ],
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
            processEscapes: true,
            TeX: {
                equationNumbers: {
                  autoNumber: 'AMS'
                }
            }
        }
    });
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax();
        for (var i = 0; i < all.length; ++i)
            all[i].SourceElement().parentNode.className += ' has-jax';
    });
</script>
<script async src="//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
    
    <script src="/nlp-docs/js/main.js"></script>
    <script src="/nlp-docs/js/dots.js"></script>
    <script src="/nlp-docs/js/yt-bars.js"></script>




<!-- Custom Scripts -->

<script src="/nlp-docs/js/main.js"></script>


    </div>
</html>
