<!DOCTYPE html>
<html lang=en>
<head>
    <meta charset="utf-8">
    
    <title>The GPT Architecture | NLPwShiyi Docs</title>
    
    
        <meta name="keywords" content="nlp-theories" />
    
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1" />
    <meta name="description" content="Summary and breakdown of the code that form the Generative Pre-trained Transformer architecture continuedLet’s break down the code snippet line by line to understand what each step does in the context">
<meta property="og:type" content="article">
<meta property="og:title" content="The GPT Architecture">
<meta property="og:url" content="https://shiyisteezin.github.io/nlp-docs/2024/08/17/transformer/index.html">
<meta property="og:site_name" content="NLPwShiyi Docs">
<meta property="og:description" content="Summary and breakdown of the code that form the Generative Pre-trained Transformer architecture continuedLet’s break down the code snippet line by line to understand what each step does in the context">
<meta property="og:locale" content="en_US">
<meta property="article:published_time" content="2024-08-17T13:16:00.000Z">
<meta property="article:modified_time" content="2024-12-05T18:39:15.379Z">
<meta property="article:author" content="Shiyi S">
<meta property="article:tag" content="nlp-theories">
<meta name="twitter:card" content="summary">
    

    

    
        <link rel="icon" href="/nlp-docs/favicon.ico" />
    

    
<link rel="stylesheet" href="/nlp-docs/libs/font-awesome/css/font-awesome.min.css">

    
<link rel="stylesheet" href="/nlp-docs/libs/open-sans/styles.css">

    
<link rel="stylesheet" href="/nlp-docs/libs/source-code-pro/styles.css">


    
<link rel="stylesheet" href="/nlp-docs/css/style.css">

    
<script src="/nlp-docs/libs/jquery/2.1.3/jquery.min.js"></script>

    
<script src="/nlp-docs/libs/jquery/plugins/cookie/1.4.1/jquery.cookie.js"></script>

    
    
        
<link rel="stylesheet" href="/nlp-docs/libs/lightgallery/css/lightgallery.min.css">

    
    
        
<link rel="stylesheet" href="/nlp-docs/libs/justified-gallery/justifiedGallery.min.css">

    
    
    


    


<meta name="generator" content="Hexo 7.3.0"></head>

<body>
    <div id="container">
        <header id="header">
    <div id="header-main" class="header-inner">
        <div class="outer">
            <a href="/nlp-docs/" id="logo">
                
                <span class="site-title">NLPwShiyi Docs</span>
            </a>
            <nav id="main-nav">
                
            </nav>
            
            <div id="search-form-wrap">

    <form class="search-form">
        <input type="text" class="ins-search-input search-form-input" placeholder="Search" />
        <button type="submit" class="search-form-submit"></button>
    </form>
    <div class="ins-search">
    <div class="ins-search-mask"></div>
    <div class="ins-search-container">
        <div class="ins-input-wrapper">
            <input type="text" class="ins-search-input" placeholder="Type something..." />
            <span class="ins-close ins-selectable"><i class="fa fa-times-circle"></i></span>
        </div>
        <div class="ins-section-wrapper">
            <div class="ins-section-container"></div>
        </div>
    </div>
</div>
<script>
(function (window) {
    var INSIGHT_CONFIG = {
        TRANSLATION: {
            POSTS: 'Posts',
            PAGES: 'Pages',
            CATEGORIES: 'Categories',
            TAGS: 'Tags',
            UNTITLED: '(Untitled)',
        },
        ROOT_URL: '/nlp-docs/',
        CONTENT_URL: '/nlp-docs/content.json',
    };
    window.INSIGHT_CONFIG = INSIGHT_CONFIG;
})(window);
</script>

<script src="/nlp-docs/js/insight.js"></script>


</div>

        </div>
    </div>
    <div id="main-nav-mobile" class="header-sub header-inner">
        <table class="menu outer">
            <tr>
                
                <td>
                    
    <div class="search-form">
        <input type="text" class="ins-search-input search-form-input" placeholder="Search" />
    </div>


                </td>
            </tr>
        </table>
    </div>
</header>

        <div class="outer">
            <!-- Left Sidebar -->
            
                <aside id="sidebar-left">
                    <aside id="sidebar">
   
        
    <div class="widget-wrap" id='categories'>
        <h3 class="widget-title">
            <span>categories</span>
            &nbsp;
            <a id='allExpand' href="#">
                <i class="fa fa-angle-double-down fa-2x"></i>
            </a>
        </h3>
        
        
        
         <ul class="unstyled" id="tree" > 
                    <li class="directory">
                        <a href="#" data-role="directory">
                            <i class="fa fa-folder"></i>
                            &nbsp;
                            Linguistics
                        </a>
                         <ul class="unstyled" id="tree" >  <li class="file"><a href="/nlp-docs/2024/07/17/problem-solving/">Problem Solving</a></li>  <li class="file"><a href="/nlp-docs/2024/08/13/philo-o-mind/">Philosophy of Mind</a></li>  <li class="file"><a href="/nlp-docs/2024/09/17/compositionality/">Compositionality</a></li>  </ul> 
                    </li> 
                    
                    <li class="directory">
                        <a href="#" data-role="directory">
                            <i class="fa fa-folder"></i>
                            &nbsp;
                            Math
                        </a>
                         <ul class="unstyled" id="tree" >  <li class="file"><a href="/nlp-docs/2024/07/14/audo-diff/">Auto Differentiation</a></li>  <li class="file"><a href="/nlp-docs/2024/07/16/markov-chains/">Markov Processes</a></li>  <li class="file"><a href="/nlp-docs/2024/07/17/jacobian-matrices/">Jacobian Matrices</a></li>  <li class="file"><a href="/nlp-docs/2024/07/23/viterbi/">Viterbi Algorithm</a></li>  <li class="file"><a href="/nlp-docs/2024/07/27/vae/">Variational Families</a></li>  <li class="file"><a href="/nlp-docs/2024/08/03/mutual-info/">Mutual Information</a></li>  <li class="file"><a href="/nlp-docs/2024/09/02/determinism/">Intro to Determinism</a></li>  <li class="file"><a href="/nlp-docs/2024/11/18/sgd/">Understanding SGD</a></li>  <li class="file"><a href="/nlp-docs/2024/11/23/measuring-subjectivity/">Measuring Subjectivity</a></li>  </ul> 
                    </li> 
                    
                    <li class="directory open">
                        <a href="#" data-role="directory">
                            <i class="fa fa-folder-open"></i>
                            &nbsp;
                            NLP Related
                        </a>
                         <ul class="unstyled" id="tree" >  <li class="file"><a href="/nlp-docs/2024/08/13/mdn-nlp/">Contemporary NLP</a></li>  <li class="file active"><a href="/nlp-docs/2024/08/17/transformer/">The GPT Architecture</a></li>  </ul> 
                    </li> 
                    
                    <li class="directory">
                        <a href="#" data-role="directory">
                            <i class="fa fa-folder"></i>
                            &nbsp;
                            Other
                        </a>
                         <ul class="unstyled" id="tree" >  <li class="file"><a href="/nlp-docs/2024/09/02/intentionality/">Naturalizing Intentions</a></li>  <li class="file"><a href="/nlp-docs/2024/09/02/qulia/">Understanding Qualia</a></li>  <li class="file"><a href="/nlp-docs/2024/09/29/quant-belief/">Quantifying Beliefs</a></li>  </ul> 
                    </li> 
                     <li class="file"><a href="/nlp-docs/2024/07/11/place-holder/">Intro & Overview</a></li>  </ul> 
    </div>
    <script>
        $(document).ready(function() {
            var iconFolderOpenClass  = 'fa-folder-open';
            var iconFolderCloseClass = 'fa-folder';
            var iconAllExpandClass = 'fa-angle-double-down';
            var iconAllPackClass = 'fa-angle-double-up';
            // Handle directory-tree expansion:
            // 左键单独展开目录
            $(document).on('click', '#categories a[data-role="directory"]', function (event) {
                event.preventDefault();

                var icon = $(this).children('.fa');
                var expanded = icon.hasClass(iconFolderOpenClass);
                var subtree = $(this).siblings('ul');
                icon.removeClass(iconFolderOpenClass).removeClass(iconFolderCloseClass);
                if (expanded) {
                    if (typeof subtree != 'undefined') {
                        subtree.slideUp({ duration: 100 });
                    }
                    icon.addClass(iconFolderCloseClass);
                } else {
                    if (typeof subtree != 'undefined') {
                        subtree.slideDown({ duration: 100 });
                    }
                    icon.addClass(iconFolderOpenClass);
                }
            });
            // 右键展开下属所有目录
            $('#categories a[data-role="directory"]').bind("contextmenu", function(event){
                event.preventDefault();
                
                var icon = $(this).children('.fa');
                var expanded = icon.hasClass(iconFolderOpenClass);
                var listNode = $(this).siblings('ul');
                var subtrees = $.merge(listNode.find('li ul'), listNode);
                var icons = $.merge(listNode.find('.fa'), icon);
                icons.removeClass(iconFolderOpenClass).removeClass(iconFolderCloseClass);
                if(expanded) {
                    subtrees.slideUp({ duration: 100 });
                    icons.addClass(iconFolderCloseClass);
                } else {
                    subtrees.slideDown({ duration: 100 });
                    icons.addClass(iconFolderOpenClass);
                }
            })
            // 展开关闭所有目录按钮
            $(document).on('click', '#allExpand', function (event) {
                event.preventDefault();
                
                var icon = $(this).children('.fa');
                var expanded = icon.hasClass(iconAllExpandClass);
                icon.removeClass(iconAllExpandClass).removeClass(iconAllPackClass);
                if(expanded) {
                    $('#sidebar .fa.fa-folder').removeClass('fa-folder').addClass('fa-folder-open')
                    $('#categories li ul').slideDown({ duration: 100 });
                    icon.addClass(iconAllPackClass);
                } else {
                    $('#sidebar .fa.fa-folder-open').removeClass('fa-folder-open').addClass('fa-folder')
                    $('#categories li ul').slideUp({ duration: 100 });
                    icon.addClass(iconAllExpandClass);
                }
            });  
        });
    </script>

    
        
    <div class="widget-wrap" id="archives">
        <h3 class="widget-title"><span>archives</span></h3>
        <div class="widget">
            <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/nlp-docs/archives/2024/11/">November 2024</a><span class="archive-list-count">2</span></li><li class="archive-list-item"><a class="archive-list-link" href="/nlp-docs/archives/2024/09/">September 2024</a><span class="archive-list-count">5</span></li><li class="archive-list-item"><a class="archive-list-link" href="/nlp-docs/archives/2024/08/">August 2024</a><span class="archive-list-count">4</span></li><li class="archive-list-item"><a class="archive-list-link" href="/nlp-docs/archives/2024/07/">July 2024</a><span class="archive-list-count">7</span></li></ul>
        </div>


        <script>
          function toggleVisibility(elementId) {
              var element = document.getElementById(elementId);
              if (element.style.display === 'none' || element.style.display === '') {
                  element.style.display = 'block';
              } else {
                  element.style.display = 'none';
              }
          }

          document.addEventListener('DOMContentLoaded', function() {
              var toggleButton = document.getElementById('archive-widget');
              toggleButton.addEventListener('click', function() {
                  toggleVisibility('archive-widget');
              });
          });

          document.addEventListener('DOMContentLoaded', function() {
          // Check if the .timeline-wrap class is present
          if (document.querySelector('.timeline-wrap')) {
              // If present, hide the #right-sidebar and show the #archives
              var rightSidebar = document.getElementById('right-sidebar');
              var archives = document.getElementById('archives');

              if (rightSidebar) {
                  rightSidebar.style.display = 'none';
              }
              if (archives) {
                  archives.style.display = 'block';
              }
          } else {
              // If .timeline-wrap is not present, ensure the default visibility
              if (rightSidebar) {
                  rightSidebar.style.display = 'block';
              }
              if (archives) {
                  archives.style.display = 'none';
              }
          }
      });


    </script>

    </div>


    
        
    <div class="widget-wrap">
        <h3 class="widget-title"><span>tags</span></h3>
        <div class="widget">
            <ul class="tag-list" itemprop="keywords"><li class="tag-list-item"><a class="tag-list-link" href="/nlp-docs/tags/determinism/" rel="tag">determinism</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/nlp-docs/tags/intro/" rel="tag">intro</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/nlp-docs/tags/math/" rel="tag">math</a><span class="tag-list-count">8</span></li><li class="tag-list-item"><a class="tag-list-link" href="/nlp-docs/tags/nlp-theories/" rel="tag">nlp-theories</a><span class="tag-list-count">5</span></li><li class="tag-list-item"><a class="tag-list-link" href="/nlp-docs/tags/semantics/" rel="tag">semantics</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/nlp-docs/tags/stochastic/" rel="tag">stochastic</a><span class="tag-list-count">4</span></li></ul>
        </div>
    </div>

    
    <div id="toTop" class="fa fa-angle-up"></div>
</aside>
                </aside>
            

            <!-- Main Content -->
            <section id="main">
                <article id="search-transformer" class="article article-type-search" itemscope itemprop="blogPost">
    <div class="article-inner">
        
        
            <header class="article-header">
                
                    <div class="article-meta">
                        
    <div class="article-category">
    	<i class="fa fa-folder"></i>
        <a class="article-category-link" href="/nlp-docs/categories/NLP-Related/">NLP Related</a>
    </div>

                        
    <div class="article-tag">
        <i class="fa fa-tag"></i>
        <a class="tag-link-link" href="/nlp-docs/tags/nlp-theories/" rel="tag">nlp-theories</a>
    </div>

                        
    <div class="article-date">
        <i class="fa fa-calendar"></i>
        <a href="/nlp-docs/2024/08/17/transformer/">
            <time datetime="2024-08-17T13:16:00.000Z" itemprop="datePublished">2024-08-17</time>
        </a>
    </div>


                        
                        
                            <div class="article-meta-button">
                                <a target="_blank" rel="noopener" href='https://github.com/shiyis/Wiki-site/raw/writing/source/_posts/transformer.md'> Source </a>
                            </div>
                            <div class="article-meta-button">
                                <a target="_blank" rel="noopener" href='https://github.com/shiyis/Wiki-site/edit/writing/source/_posts/transformer.md'> Edit </a>
                            </div>
                            <div class="article-meta-button">
                                <a target="_blank" rel="noopener" href='https://github.com/shiyis/Wiki-site/commits/writing/source/_posts/transformer.md'> History </a>
                            </div>
                        
                    </div>
                
                
    
        <h1 class="article-title" itemprop="name">
            The GPT Architecture
        </h1>
    

            </header>
        
        
        <div class="article-entry" itemprop="articleBody">
        
        
            
        
        
            <h3 id="Summary-and-breakdown-of-the-code-that-form-the-Generative-Pre-trained-Transformer-architecture-continued"><a href="#Summary-and-breakdown-of-the-code-that-form-the-Generative-Pre-trained-Transformer-architecture-continued" class="headerlink" title="Summary and breakdown of the code that form the Generative Pre-trained Transformer architecture continued"></a>Summary and breakdown of the code that form the Generative Pre-trained Transformer architecture continued</h3><p>Let’s break down the code snippet line by line to understand what each step does in the context of creating positional encodings for a Transformer model using PyTorch.</p>
<h4 id="Code-Snippet"><a href="#Code-Snippet" class="headerlink" title="Code Snippet"></a>Code Snippet</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">div_term = torch.exp(torch.arange(<span class="number">0</span>, d_model, <span class="number">2</span>).<span class="built_in">float</span>() *</span><br><span class="line">                                 (-math.log(<span class="number">10000.0</span>) / d_model))</span><br><span class="line">pe[:, <span class="number">0</span>::<span class="number">2</span>] = torch.sin(position * div_term)</span><br><span class="line">pe[:, <span class="number">1</span>::<span class="number">2</span>] = torch.cos(position * div_term)</span><br><span class="line">pe = pe.unsqueeze(<span class="number">0</span>).transpose(<span class="number">0</span>, <span class="number">1</span>)</span><br></pre></td></tr></table></figure>

<h4 id="Explanation"><a href="#Explanation" class="headerlink" title="Explanation"></a>Explanation</h4><h6 id="1-div-term-torch-exp-torch-arange-0-d-model-2-float-math-log-10000-0-d-model"><a href="#1-div-term-torch-exp-torch-arange-0-d-model-2-float-math-log-10000-0-d-model" class="headerlink" title="1. div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))"></a>1. <code>div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))</code></h6><ul>
<li><p><strong>Purpose</strong>: Calculate the denominator for the sine and cosine functions in the positional encoding formula.</p>
</li>
<li><p><strong>Breakdown</strong>:</p>
<ul>
<li><code>torch.arange(0, d_model, 2)</code>: Creates a tensor with values starting from <code>0</code> to <code>d_model - 1</code> with a step size of <code>2</code>. This gives us indices like <code>[0, 2, 4, ..., d_model-2]</code>.<ul>
<li>If <code>d_model</code> is <code>512</code>, this tensor will have <code>256</code> values: <code>[0, 2, 4, ..., 510]</code>.</li>
</ul>
</li>
<li><code>.float()</code>: Converts the tensor to a floating-point type.</li>
<li><code>(-math.log(10000.0) / d_model)</code>: Computes a scaling factor for the positional encoding formula. The value <code>10000.0</code> is a hyperparameter that determines the rate of change of the sine and cosine functions.</li>
<li><code>*</code>: Multiplies each value in the tensor by the scaling factor.</li>
<li><code>torch.exp()</code>: Applies the exponential function to each element in the tensor, resulting in the <code>div_term</code> tensor which will be used to scale the positions.</li>
</ul>
</li>
</ul>
<h6 id="2-pe-0-2-torch-sin-position-div-term"><a href="#2-pe-0-2-torch-sin-position-div-term" class="headerlink" title="2. pe[:, 0::2] = torch.sin(position * div_term)"></a>2. <code>pe[:, 0::2] = torch.sin(position * div_term)</code></h6><ul>
<li><p><strong>Purpose</strong>: Compute the sine values for even-indexed dimensions in the positional encoding matrix.</p>
</li>
<li><p><strong>Breakdown</strong>:</p>
<ul>
<li><code>position</code>: A tensor representing the positions of the words in the sequence. This could be something like <code>torch.arange(0, max_len).unsqueeze(1)</code>, where <code>max_len</code> is the maximum sequence length.</li>
<li><code>position * div_term</code>: Element-wise multiplication of the <code>position</code> tensor with the <code>div_term</code> tensor calculated earlier. This scales the positions appropriately.</li>
<li><code>torch.sin()</code>: Applies the sine function to each element in the resulting tensor.</li>
<li><code>pe[:, 0::2]</code>: Selects all rows (<code>:</code>) and every second column starting from <code>0</code> (<code>0::2</code>). This targets the even-indexed dimensions of the positional encoding matrix.</li>
<li><code>=</code>: Assigns the computed sine values to these selected positions in the positional encoding matrix <code>pe</code>.</li>
</ul>
</li>
</ul>
<h6 id="3-pe-1-2-torch-cos-position-div-term"><a href="#3-pe-1-2-torch-cos-position-div-term" class="headerlink" title="3. pe[:, 1::2] = torch.cos(position * div_term)"></a>3. <code>pe[:, 1::2] = torch.cos(position * div_term)</code></h6><ul>
<li><p><strong>Purpose</strong>: Compute the cosine values for odd-indexed dimensions in the positional encoding matrix.</p>
</li>
<li><p><strong>Breakdown</strong>:</p>
<ul>
<li><code>position * div_term</code>: Same as above, scales the positions appropriately.</li>
<li><code>torch.cos()</code>: Applies the cosine function to each element in the resulting tensor.</li>
<li><code>pe[:, 1::2]</code>: Selects all rows (<code>:</code>) and every second column starting from <code>1</code> (<code>1::2</code>). This targets the odd-indexed dimensions of the positional encoding matrix.</li>
<li><code>=</code>: Assigns the computed cosine values to these selected positions in the positional encoding matrix <code>pe</code>.</li>
</ul>
</li>
</ul>
<h6 id="4-pe-pe-unsqueeze-0-transpose-0-1"><a href="#4-pe-pe-unsqueeze-0-transpose-0-1" class="headerlink" title="4. pe = pe.unsqueeze(0).transpose(0, 1)"></a>4. <code>pe = pe.unsqueeze(0).transpose(0, 1)</code></h6><ul>
<li><p><strong>Purpose</strong>: Reshape the positional encoding matrix to match the expected input shape for the Transformer model.</p>
</li>
<li><p><strong>Breakdown</strong>:</p>
<ul>
<li><code>pe.unsqueeze(0)</code>: Adds an extra dimension at the <code>0</code>-th position. If <code>pe</code> originally has shape <code>(max_len, d_model)</code>, it will now have shape <code>(1, max_len, d_model)</code>. This extra dimension is often used to represent the batch size, which is <code>1</code> in this case.</li>
<li><code>transpose(0, 1)</code>: Swaps the <code>0</code>-th and <code>1</code>-st dimensions. After this operation, the shape will be <code>(max_len, 1, d_model)</code>. This step ensures that the positional encoding matrix can be correctly broadcasted and added to the input embeddings in the Transformer model.</li>
</ul>
<p>The division by <code>d_model</code> in the expression <code>(-math.log(10000.0) / d_model)</code> is a critical part of the positional encoding design in the Transformer model. This design ensures that different dimensions of the positional encoding vary at different frequencies. Here’s a more detailed explanation:</p>
<h4 id="Positional-Encoding-in-Transformers"><a href="#Positional-Encoding-in-Transformers" class="headerlink" title="Positional Encoding in Transformers"></a>Positional Encoding in Transformers</h4><p>The idea behind positional encoding is to inject information about the position of each token in the sequence into the token’s embedding. This is necessary because the Transformer model, unlike RNNs or CNNs, does not inherently capture the order of tokens.</p>
<h4 id="Frequency-Scaling"><a href="#Frequency-Scaling" class="headerlink" title="Frequency Scaling"></a>Frequency Scaling</h4><ol>
<li><p><strong>Frequency Spectrum</strong>:</p>
<ul>
<li>By dividing by <code>d_model</code>, we spread the frequencies of the sine and cosine functions across the dimensions of the embedding vector.</li>
<li>The lower dimensions correspond to lower frequencies, and the higher dimensions correspond to higher frequencies. This spread allows the model to capture a wide range of positional dependencies.</li>
</ul>
</li>
<li><p><strong>Mathematical Justification</strong>:</p>
<ul>
<li>The formula for positional encoding in the Transformer is designed such that for a given position $ pos $ and dimension $ i $:<ul>
<li>$ PE_{(pos, 2i)} &#x3D; \sin\left(\frac{pos}{10000^{\frac{2i}{d_{\text{model}}}}}\right) $</li>
<li>$ PE_{(pos, 2i+1)} &#x3D; \cos\left(\frac{pos}{10000^{\frac{2i}{d_{\text{model}}}}}\right) $</li>
</ul>
</li>
<li>The term $\frac{1}{10000^{\frac{2i}{d_{\text{model}}}}}$ ensures that the positions are scaled appropriately across different dimensions.</li>
</ul>
</li>
<li><p><strong>Implementation</strong>:</p>
<ul>
<li>The division by <code>d_model</code> normalizes the range of exponents to ensure they vary smoothly between 0 and 1, creating a geometric progression of frequencies.</li>
</ul>
</li>
</ol>
</li>
</ul>
<h4 id="Detailed-Steps"><a href="#Detailed-Steps" class="headerlink" title="Detailed Steps"></a>Detailed Steps</h4><p>  Let’s rewrite the specific part of the code to understand its purpose:</p>
  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">div_term = torch.exp(torch.arange(<span class="number">0</span>, d_model, <span class="number">2</span>).<span class="built_in">float</span>() *</span><br><span class="line">                                 (-math.log(<span class="number">10000.0</span>) / d_model))</span><br></pre></td></tr></table></figure>
<p> Let’s break down the specific line of code <code>div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))</code> and explain its purpose in the context of positional encoding in the Transformer model.</p>
<h4 id="Purpose-of-the-Code"><a href="#Purpose-of-the-Code" class="headerlink" title="Purpose of the Code"></a>Purpose of the Code</h4><p>This line of code is part of the positional encoding generation process in the Transformer model, as described in the paper “Attention is All You Need”. The positional encodings allow the model to utilize the order of the sequence since the Transformer itself is position-agnostic.</p>
<h4 id="Breaking-Down-the-Code"><a href="#Breaking-Down-the-Code" class="headerlink" title="Breaking Down the Code"></a>Breaking Down the Code</h4><h6 id="1-torch-arange-0-d-model-2"><a href="#1-torch-arange-0-d-model-2" class="headerlink" title="1. torch.arange(0, d_model, 2)"></a>1. <code>torch.arange(0, d_model, 2)</code></h6><ul>
<li><strong>Purpose</strong>: Creates a sequence of even integers from 0 to <code>d_model - 2</code>.</li>
<li><strong>Example</strong>: If <code>d_model</code> is 512, <code>torch.arange(0, d_model, 2)</code> generates a tensor containing <code>[0, 2, 4, ..., 510]</code>.</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">indices = torch.arange(<span class="number">0</span>, d_model, <span class="number">2</span>)</span><br></pre></td></tr></table></figure>
<ul>
<li><strong>Output</strong>: A tensor of shape <code>(d_model/2,)</code> containing even integers up to <code>d_model - 2</code>.</li>
</ul>
<h6 id="2-float"><a href="#2-float" class="headerlink" title="2. .float()"></a>2. <code>.float()</code></h6><ul>
<li><strong>Purpose</strong>: Converts the integer tensor to a tensor of floats. This is necessary because we will perform mathematical operations that require floating-point precision.</li>
<li><strong>Example</strong>: Continuing from the previous step, <code>.float()</code> converts the integer tensor to floating-point numbers.</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">indices = indices.<span class="built_in">float</span>()</span><br></pre></td></tr></table></figure>
<ul>
<li><strong>Output</strong>: A tensor of shape <code>(d_model/2,)</code> containing floating-point numbers <code>[0.0, 2.0, 4.0, ..., 510.0]</code>.</li>
</ul>
<h6 id="3-math-log-10000-0-d-model"><a href="#3-math-log-10000-0-d-model" class="headerlink" title="3. (-math.log(10000.0) / d_model)"></a>3. <code>(-math.log(10000.0) / d_model)</code></h6><ul>
<li><strong>Purpose</strong>: Computes a scaling factor for the positional encodings. The value <code>-math.log(10000.0) / d_model</code> ensures the positional encodings have values that decay exponentially.</li>
<li><strong>Value</strong>: If <code>d_model</code> is 512, this term calculates to <code>-math.log(10000.0) / 512 ≈ -0.02302585</code>.</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">scale_factor = -math.log(<span class="number">10000.0</span>) / d_model</span><br></pre></td></tr></table></figure>

<h6 id="4-scale-factor"><a href="#4-scale-factor" class="headerlink" title="4. * scale_factor"></a>4. <code>* scale_factor</code></h6><ul>
<li><strong>Purpose</strong>: Multiplies each element in the tensor of indices by the scale factor. This operation scales the indices to a range suitable for the exponential function, ensuring the positional encodings vary smoothly.</li>
<li><strong>Example</strong>: Continuing from the previous steps, <code>indices * scale_factor</code> scales each index.</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">scaled_indices = indices * scale_factor</span><br></pre></td></tr></table></figure>
<ul>
<li><strong>Output</strong>: A tensor of shape <code>(d_model/2,)</code> with scaled values.</li>
</ul>
<h6 id="5-torch-exp-scaled-indices"><a href="#5-torch-exp-scaled-indices" class="headerlink" title="5. torch.exp(scaled_indices)"></a>5. <code>torch.exp(scaled_indices)</code></h6><ul>
<li><strong>Purpose</strong>: Applies the exponential function to each element in the scaled tensor. The exponential function is used to create a set of frequencies for the positional encodings.</li>
<li><strong>Example</strong>: Applying the exponential function to the scaled indices.</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">div_term = torch.exp(scaled_indices)</span><br></pre></td></tr></table></figure>
<ul>
<li><strong>Output</strong>: A tensor of shape <code>(d_model/2,)</code> containing the calculated frequencies for the positional encodings.</li>
</ul>
<h4 id="Final-Output"><a href="#Final-Output" class="headerlink" title="Final Output"></a>Final Output</h4><p>The variable <code>div_term</code> now contains a series of exponentially scaled values. These values are used to create the positional encodings, which alternate between sine and cosine functions at different frequencies.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> math</span><br><span class="line"></span><br><span class="line">d_model = <span class="number">512</span>  <span class="comment"># Example value</span></span><br><span class="line">indices = torch.arange(<span class="number">0</span>, d_model, <span class="number">2</span>).<span class="built_in">float</span>()</span><br><span class="line">scale_factor = -math.log(<span class="number">10000.0</span>) / d_model</span><br><span class="line">scaled_indices = indices * scale_factor</span><br><span class="line">div_term = torch.exp(scaled_indices)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(div_term)</span><br></pre></td></tr></table></figure>

<h4 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h4><ul>
<li><strong><code>torch.arange(0, d_model, 2).float()</code></strong>: Creates a tensor of even indices from 0 to <code>d_model - 2</code> and converts them to floats.</li>
<li><strong><code>(-math.log(10000.0) / d_model)</code></strong>: Computes a scaling factor.</li>
<li><strong><code>* scale_factor</code></strong>: Scales the indices by the computed factor.</li>
<li><strong><code>torch.exp(scaled_indices)</code></strong>: Applies the exponential function to get the final <code>div_term</code>.</li>
</ul>
<h4 id="Purpose-in-Positional-Encoding"><a href="#Purpose-in-Positional-Encoding" class="headerlink" title="Purpose in Positional Encoding"></a>Purpose in Positional Encoding</h4><p>The <code>div_term</code> tensor represents the denominators for the positional encodings’ sine and cosine functions. These frequencies ensure that different positions in the input sequence have unique encodings, allowing the Transformer model to infer the position of each token. The overall goal is to introduce a form of positional information that helps the model understand the order of the sequence.</p>
<h4 id="Intuitive-Understanding"><a href="#Intuitive-Understanding" class="headerlink" title="Intuitive Understanding"></a>Intuitive Understanding</h4><ul>
<li><p><strong>Varying Frequencies</strong>:</p>
<ul>
<li>Lower dimensions of the embedding vector (e.g., dimensions 0, 2, 4) will vary more slowly (lower frequency).</li>
<li>Higher dimensions (e.g., dimensions 508, 510) will vary more quickly (higher frequency).</li>
</ul>
</li>
<li><p><strong>Why Divide by <code>d_model</code></strong>:</p>
<ul>
<li>To ensure that the entire range of positional encodings uses a range of frequencies from very slow to very fast.</li>
<li>This allows the Transformer to distinguish between different positions effectively.</li>
</ul>
</li>
</ul>
<h4 id="Example-Calculation"><a href="#Example-Calculation" class="headerlink" title="Example Calculation"></a>Example Calculation</h4><p>Let’s assume <code>d_model = 512</code>:</p>
<ul>
<li><p>For dimension <code>i = 0</code>:</p>
<ul>
<li>The exponent would be $\frac{0}{512} &#x3D; 0$.</li>
<li>So, the term would be $10000^{0} &#x3D; 1$.</li>
</ul>
</li>
<li><p>For dimension <code>i = 256</code>:</p>
<ul>
<li>The exponent would be $\frac{256}{512} &#x3D; 0.5$.</li>
<li>So, the term would be $10000^{0.5} &#x3D; 100$.</li>
</ul>
</li>
</ul>
<p>The above steps ensure that the positional encoding matrix has a smooth and gradual change in frequencies across the dimensions, which helps the model to capture the positional information effectively.</p>
<h4 id="Summary-1"><a href="#Summary-1" class="headerlink" title="Summary"></a>Summary</h4><ul>
<li><strong>Dividing by <code>d_model</code></strong> ensures the frequencies of sine and cosine functions used in positional encodings are spread across a wide range.</li>
<li>This design allows the Transformer model to learn and utilize positional information effectively, enhancing its ability to understand the order and relative position of tokens in a sequence.</li>
</ul>

            </div>
        
        <footer class="article-footer">
        </footer>
    </div>
</article>


    
<nav id="article-nav">
    
        <a href="/nlp-docs/2024/09/02/qulia/" id="article-nav-newer" class="article-nav-link-wrap">
            <strong class="article-nav-caption">Newer</strong>
            <div class="article-nav-title">
                
                    Understanding Qualia
                
            </div>
        </a>
    
    
        <a href="/nlp-docs/2024/08/13/mdn-nlp/" id="article-nav-older" class="article-nav-link-wrap">
            <strong class="article-nav-caption">Older</strong>
            <div class="article-nav-title">Contemporary NLP</div>
        </a>
    
</nav>





    
    




<!-- baidu url auto push script -->
<script type="text/javascript">
    !function(){var e=/([http|https]:\/\/[a-zA-Z0-9\_\.]+\.baidu\.com)/gi,r=window.location.href,o=document.referrer;if(!e.test(r)){var n="//api.share.baidu.com/s.gif";o?(n+="?r="+encodeURIComponent(document.referrer),r&&(n+="&l="+r)):r&&(n+="?l="+r);var t=new Image;t.src=n}}(window);
</script>     

            </section>

            <!-- Right Sidebar -->
            <aside id="sidebar-right">
                <aside id="right-sidebar">
    <div class="timeline-container">
        
                <!-- Year Marker -->
                <div class="timeline-row timeline-year">
                    <span class="timeline-icon"><i class="fa fa-calendar-o"></i></span>
                    <h2 class="timeline-title">2024</h2>
                </div>
        
                <!-- Month Marker -->
                <div class="timeline-row timeline-month">
                    <span class="timeline-icon"><i class="fa fa-calendar"></i></span>
                    <h3 class="timeline-title">2024-11 (2)</h3>
                </div>
                <div class="timeline-posts"> <!-- Open a new month's timeline-posts -->
        

            <!-- Post Entry -->
            <div class="timeline-row timeline-post">
                <span class="timeline-icon"></span>
                <div class="timeline-content">
                    <h4 class="timeline-post-title"><a href="https://shiyisteezin.github.io/nlp-docs/2024/11/23/measuring-subjectivity/">Measuring Subjectivity</a></h4>
                    <div class="timeline-post-meta">
                        <span><i class="fa fa-calendar"></i> 2024-11-23</span>
                        <span><i class="fa fa-folder-open"></i> Math</span>
                    </div>
                </div>
            </div>

        

            <!-- Post Entry -->
            <div class="timeline-row timeline-post">
                <span class="timeline-icon"></span>
                <div class="timeline-content">
                    <h4 class="timeline-post-title"><a href="https://shiyisteezin.github.io/nlp-docs/2024/11/18/sgd/">Understanding SGD</a></h4>
                    <div class="timeline-post-meta">
                        <span><i class="fa fa-calendar"></i> 2024-11-18</span>
                        <span><i class="fa fa-folder-open"></i> Math</span>
                    </div>
                </div>
            </div>

        
                    </div> <!-- Close previous month's timeline-posts -->
                
                <!-- Month Marker -->
                <div class="timeline-row timeline-month">
                    <span class="timeline-icon"><i class="fa fa-calendar"></i></span>
                    <h3 class="timeline-title">2024-09 (5)</h3>
                </div>
                <div class="timeline-posts"> <!-- Open a new month's timeline-posts -->
        

            <!-- Post Entry -->
            <div class="timeline-row timeline-post">
                <span class="timeline-icon"></span>
                <div class="timeline-content">
                    <h4 class="timeline-post-title"><a href="https://shiyisteezin.github.io/nlp-docs/2024/09/29/quant-belief/">Quantifying Beliefs</a></h4>
                    <div class="timeline-post-meta">
                        <span><i class="fa fa-calendar"></i> 2024-09-29</span>
                        <span><i class="fa fa-folder-open"></i> Other</span>
                    </div>
                </div>
            </div>

        

            <!-- Post Entry -->
            <div class="timeline-row timeline-post">
                <span class="timeline-icon"></span>
                <div class="timeline-content">
                    <h4 class="timeline-post-title"><a href="https://shiyisteezin.github.io/nlp-docs/2024/09/17/compositionality/">Compositionality</a></h4>
                    <div class="timeline-post-meta">
                        <span><i class="fa fa-calendar"></i> 2024-09-17</span>
                        <span><i class="fa fa-folder-open"></i> Linguistics</span>
                    </div>
                </div>
            </div>

        

            <!-- Post Entry -->
            <div class="timeline-row timeline-post">
                <span class="timeline-icon"></span>
                <div class="timeline-content">
                    <h4 class="timeline-post-title"><a href="https://shiyisteezin.github.io/nlp-docs/2024/09/02/determinism/">Intro to Determinism</a></h4>
                    <div class="timeline-post-meta">
                        <span><i class="fa fa-calendar"></i> 2024-09-02</span>
                        <span><i class="fa fa-folder-open"></i> Math</span>
                    </div>
                </div>
            </div>

        

            <!-- Post Entry -->
            <div class="timeline-row timeline-post">
                <span class="timeline-icon"></span>
                <div class="timeline-content">
                    <h4 class="timeline-post-title"><a href="https://shiyisteezin.github.io/nlp-docs/2024/09/02/intentionality/">Naturalizing Intentions</a></h4>
                    <div class="timeline-post-meta">
                        <span><i class="fa fa-calendar"></i> 2024-09-02</span>
                        <span><i class="fa fa-folder-open"></i> Other</span>
                    </div>
                </div>
            </div>

        

            <!-- Post Entry -->
            <div class="timeline-row timeline-post">
                <span class="timeline-icon"></span>
                <div class="timeline-content">
                    <h4 class="timeline-post-title"><a href="https://shiyisteezin.github.io/nlp-docs/2024/09/02/qulia/">Understanding Qualia</a></h4>
                    <div class="timeline-post-meta">
                        <span><i class="fa fa-calendar"></i> 2024-09-02</span>
                        <span><i class="fa fa-folder-open"></i> Other</span>
                    </div>
                </div>
            </div>

        
                    </div> <!-- Close previous month's timeline-posts -->
                
                <!-- Month Marker -->
                <div class="timeline-row timeline-month">
                    <span class="timeline-icon"><i class="fa fa-calendar"></i></span>
                    <h3 class="timeline-title">2024-08 (4)</h3>
                </div>
                <div class="timeline-posts"> <!-- Open a new month's timeline-posts -->
        

            <!-- Post Entry -->
            <div class="timeline-row timeline-post">
                <span class="timeline-icon"></span>
                <div class="timeline-content">
                    <h4 class="timeline-post-title"><a href="https://shiyisteezin.github.io/nlp-docs/2024/08/17/transformer/">The GPT Architecture</a></h4>
                    <div class="timeline-post-meta">
                        <span><i class="fa fa-calendar"></i> 2024-08-17</span>
                        <span><i class="fa fa-folder-open"></i> NLP Related</span>
                    </div>
                </div>
            </div>

        

            <!-- Post Entry -->
            <div class="timeline-row timeline-post">
                <span class="timeline-icon"></span>
                <div class="timeline-content">
                    <h4 class="timeline-post-title"><a href="https://shiyisteezin.github.io/nlp-docs/2024/08/13/mdn-nlp/">Contemporary NLP</a></h4>
                    <div class="timeline-post-meta">
                        <span><i class="fa fa-calendar"></i> 2024-08-13</span>
                        <span><i class="fa fa-folder-open"></i> NLP Related</span>
                    </div>
                </div>
            </div>

        

            <!-- Post Entry -->
            <div class="timeline-row timeline-post">
                <span class="timeline-icon"></span>
                <div class="timeline-content">
                    <h4 class="timeline-post-title"><a href="https://shiyisteezin.github.io/nlp-docs/2024/08/13/philo-o-mind/">Philosophy of Mind</a></h4>
                    <div class="timeline-post-meta">
                        <span><i class="fa fa-calendar"></i> 2024-08-13</span>
                        <span><i class="fa fa-folder-open"></i> Linguistics</span>
                    </div>
                </div>
            </div>

        

            <!-- Post Entry -->
            <div class="timeline-row timeline-post">
                <span class="timeline-icon"></span>
                <div class="timeline-content">
                    <h4 class="timeline-post-title"><a href="https://shiyisteezin.github.io/nlp-docs/2024/08/03/mutual-info/">Mutual Information</a></h4>
                    <div class="timeline-post-meta">
                        <span><i class="fa fa-calendar"></i> 2024-08-03</span>
                        <span><i class="fa fa-folder-open"></i> Math</span>
                    </div>
                </div>
            </div>

        
                    </div> <!-- Close previous month's timeline-posts -->
                
                <!-- Month Marker -->
                <div class="timeline-row timeline-month">
                    <span class="timeline-icon"><i class="fa fa-calendar"></i></span>
                    <h3 class="timeline-title">2024-07 (7)</h3>
                </div>
                <div class="timeline-posts"> <!-- Open a new month's timeline-posts -->
        

            <!-- Post Entry -->
            <div class="timeline-row timeline-post">
                <span class="timeline-icon"></span>
                <div class="timeline-content">
                    <h4 class="timeline-post-title"><a href="https://shiyisteezin.github.io/nlp-docs/2024/07/27/vae/">Variational Families</a></h4>
                    <div class="timeline-post-meta">
                        <span><i class="fa fa-calendar"></i> 2024-07-27</span>
                        <span><i class="fa fa-folder-open"></i> Math</span>
                    </div>
                </div>
            </div>

        

            <!-- Post Entry -->
            <div class="timeline-row timeline-post">
                <span class="timeline-icon"></span>
                <div class="timeline-content">
                    <h4 class="timeline-post-title"><a href="https://shiyisteezin.github.io/nlp-docs/2024/07/23/viterbi/">Viterbi Algorithm</a></h4>
                    <div class="timeline-post-meta">
                        <span><i class="fa fa-calendar"></i> 2024-07-23</span>
                        <span><i class="fa fa-folder-open"></i> Math</span>
                    </div>
                </div>
            </div>

        

            <!-- Post Entry -->
            <div class="timeline-row timeline-post">
                <span class="timeline-icon"></span>
                <div class="timeline-content">
                    <h4 class="timeline-post-title"><a href="https://shiyisteezin.github.io/nlp-docs/2024/07/17/jacobian-matrices/">Jacobian Matrices</a></h4>
                    <div class="timeline-post-meta">
                        <span><i class="fa fa-calendar"></i> 2024-07-17</span>
                        <span><i class="fa fa-folder-open"></i> Math</span>
                    </div>
                </div>
            </div>

        

            <!-- Post Entry -->
            <div class="timeline-row timeline-post">
                <span class="timeline-icon"></span>
                <div class="timeline-content">
                    <h4 class="timeline-post-title"><a href="https://shiyisteezin.github.io/nlp-docs/2024/07/17/problem-solving/">Problem Solving</a></h4>
                    <div class="timeline-post-meta">
                        <span><i class="fa fa-calendar"></i> 2024-07-17</span>
                        <span><i class="fa fa-folder-open"></i> Linguistics</span>
                    </div>
                </div>
            </div>

        

            <!-- Post Entry -->
            <div class="timeline-row timeline-post">
                <span class="timeline-icon"></span>
                <div class="timeline-content">
                    <h4 class="timeline-post-title"><a href="https://shiyisteezin.github.io/nlp-docs/2024/07/16/markov-chains/">Markov Processes</a></h4>
                    <div class="timeline-post-meta">
                        <span><i class="fa fa-calendar"></i> 2024-07-16</span>
                        <span><i class="fa fa-folder-open"></i> Math</span>
                    </div>
                </div>
            </div>

        

            <!-- Post Entry -->
            <div class="timeline-row timeline-post">
                <span class="timeline-icon"></span>
                <div class="timeline-content">
                    <h4 class="timeline-post-title"><a href="https://shiyisteezin.github.io/nlp-docs/2024/07/14/audo-diff/">Auto Differentiation</a></h4>
                    <div class="timeline-post-meta">
                        <span><i class="fa fa-calendar"></i> 2024-07-14</span>
                        <span><i class="fa fa-folder-open"></i> Math</span>
                    </div>
                </div>
            </div>

        

            <!-- Post Entry -->
            <div class="timeline-row timeline-post">
                <span class="timeline-icon"></span>
                <div class="timeline-content">
                    <h4 class="timeline-post-title"><a href="https://shiyisteezin.github.io/nlp-docs/2024/07/11/place-holder/">Intro &amp; Overview</a></h4>
                    <div class="timeline-post-meta">
                        <span><i class="fa fa-calendar"></i> 2024-07-11</span>
                        <span><i class="fa fa-folder-open"></i> </span>
                    </div>
                </div>
            </div>

        
        </div> <!-- Close the last month's timeline-posts -->
    </div>
</aside>

            </aside>

        </div>
        <footer id="footer">
    <div class="outer">
        <div id="footer-info" class="inner">
            Shiyi S &copy; 2024 
            <a rel="license noopener" target="_blank" href="http://creativecommons.org/licenses/by-nc-nd/4.0/"><img alt="Creative Commons License" style="border-width:0" src="https://i.creativecommons.org/l/by-nc-nd/4.0/80x15.png" /></a>
            <br> Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>. Theme - <a target="_blank" rel="noopener" href="https://github.com/zthxxx/hexo-theme-Wikitten">wikitten</a>
            
        </div>
    </div>
</footer>

        
    
        
<script src="/nlp-docs/libs/lightgallery/js/lightgallery.min.js"></script>

        
<script src="/nlp-docs/libs/lightgallery/js/lg-thumbnail.min.js"></script>

        
<script src="/nlp-docs/libs/lightgallery/js/lg-pager.min.js"></script>

        
<script src="/nlp-docs/libs/lightgallery/js/lg-autoplay.min.js"></script>

        
<script src="/nlp-docs/libs/lightgallery/js/lg-fullscreen.min.js"></script>

        
<script src="/nlp-docs/libs/lightgallery/js/lg-zoom.min.js"></script>

        
<script src="/nlp-docs/libs/lightgallery/js/lg-hash.min.js"></script>

        
<script src="/nlp-docs/libs/lightgallery/js/lg-share.min.js"></script>

        
<script src="/nlp-docs/libs/lightgallery/js/lg-video.min.js"></script>

    
    
        
<script src="/nlp-docs/libs/justified-gallery/jquery.justifiedGallery.min.js"></script>

    
    
        <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ["$","$"], ["\\(","\\)"] ],
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
            processEscapes: true,
            TeX: {
                equationNumbers: {
                  autoNumber: 'AMS'
                }
            }
        }
    });
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax();
        for (var i = 0; i < all.length; ++i)
            all[i].SourceElement().parentNode.className += ' has-jax';
    });
</script>
<script async src="//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
    
    <script src="/nlp-docs/js/main.js"></script>
    <script src="/nlp-docs/js/dots.js"></script>
    <script src="/nlp-docs/js/yt-bars.js"></script>




<!-- Custom Scripts -->

<script src="/nlp-docs/js/main.js"></script>


    </div>
</html>
