{"meta":{"title":"NLPwShiyi Docs","subtitle":"","description":"","author":"Shiyi S","url":"https://shiyisteezin.github.io/nlp-docs","root":"/nlp-docs/"},"pages":[{"title":"About","date":"2024-12-05T18:39:15.380Z","updated":"2024-12-05T18:39:15.380Z","comments":true,"path":"about/index.html","permalink":"https://shiyisteezin.github.io/nlp-docs/about/index.html","excerpt":"","text":""},{"title":"Tags","date":"2024-12-05T18:39:15.381Z","updated":"2024-12-05T18:39:15.381Z","comments":true,"path":"tags/index.html","permalink":"https://shiyisteezin.github.io/nlp-docs/tags/index.html","excerpt":"","text":""}],"posts":[{"title":"Measuring Subjectivity","slug":"measuring-subjectivity","date":"2024-11-23T13:13:20.000Z","updated":"2024-12-05T18:39:15.375Z","comments":true,"path":"2024/11/23/measuring-subjectivity/","permalink":"https://shiyisteezin.github.io/nlp-docs/2024/11/23/measuring-subjectivity/","excerpt":"","text":"Understanding VAE through a different angleIn a different blog, we have briefly introduced the mechanism behind the VAE algorithm, where it’s essentially taking complex input as numeric values, find a lower-dimensional, structured, and meaningful representation of the input data, and return output that facilitates generative modeling and reconstruction.","categories":[{"name":"Math","slug":"Math","permalink":"https://shiyisteezin.github.io/nlp-docs/categories/Math/"}],"tags":[{"name":"math","slug":"math","permalink":"https://shiyisteezin.github.io/nlp-docs/tags/math/"},{"name":"stochastic","slug":"stochastic","permalink":"https://shiyisteezin.github.io/nlp-docs/tags/stochastic/"}]},{"title":"Understanding SGD","slug":"sgd","date":"2024-11-18T17:48:00.000Z","updated":"2024-12-05T18:39:15.378Z","comments":true,"path":"2024/11/18/sgd/","permalink":"https://shiyisteezin.github.io/nlp-docs/2024/11/18/sgd/","excerpt":"","text":"In this blog, we will comprehensively go through the concepts of stochastic gradient descent","categories":[{"name":"Math","slug":"Math","permalink":"https://shiyisteezin.github.io/nlp-docs/categories/Math/"}],"tags":[{"name":"math","slug":"math","permalink":"https://shiyisteezin.github.io/nlp-docs/tags/math/"},{"name":"stochastic","slug":"stochastic","permalink":"https://shiyisteezin.github.io/nlp-docs/tags/stochastic/"}]},{"title":"Quantifying Beliefs","slug":"quant-belief","date":"2024-09-29T13:47:00.000Z","updated":"2024-12-05T18:39:15.378Z","comments":true,"path":"2024/09/29/quant-belief/","permalink":"https://shiyisteezin.github.io/nlp-docs/2024/09/29/quant-belief/","excerpt":"","text":"This Blog Will Go Through Some Fun Things About Modeling Beliefs with Bayesian MethodsThe relationship between Bayesian conditional probability and theological discussions, particularly the existence or understanding of God, has been a subject of philosophical and theological debate for centuries. I will first explain the Bayesian conditional probability equation in a simple way and then explore how it might relate to theological concepts like belief in God. Bayesian Conditional ProbabilityAt its core, Bayes’ Theorem helps us update our beliefs when we receive new evidence. It tells us how likely something is to be true (the posterior probability) given what we already believe (the prior probability) and the new evidence we observe. Bayes’ Theorem is expressed as: $$P(A | B) &#x3D; \\frac{P(B | A) \\cdot P(A)}{P(B)}$$ Where: $ P(A | B) $ is the posterior probability: the probability of $ A $ being true given that we observe $ B $. $ P(B | A) $ is the likelihood: how likely we would be to observe $ B $ if $ A $ is true. $ P(A) $ is the prior probability: our initial belief about $ A $ before seeing any evidence. $ P(B) $ is the marginal probability: the total probability of observing $ B $, regardless of whether $ A $ is true or not. In essence, Bayes’ Theorem helps us update our beliefs in the light of new evidence. For example, it is often used in medical diagnostics, where we update the likelihood of a patient having a disease based on new test results. Bayesian Reasoning and GodNow, how does this relate to God or theology? Belief in God as a Hypothesis: You can think of belief in God as the hypothesis $ A $, and some kind of evidence as $ B $. This evidence could be anything that people consider relevant, such as personal experiences, miracles, the existence of the universe, or moral values. Prior Probability $ P(A) $: This represents a person’s initial belief about whether God exists before considering the evidence. Different people have different prior probabilities depending on their background, education, culture, and other factors. Some people may have a strong belief in God’s existence (high prior), while others may be more skeptical (low prior). Likelihood $ P(B | A) $: This is the probability of observing the evidence $ B $ if God exists. For example, if the hypothesis is that God exists and the evidence is the fine-tuning of the universe, you would ask: how likely is it that this fine-tuning would exist if God exists? Posterior Probability $ P(A | B) $: After considering the evidence (e.g., personal experiences, philosophical arguments, scientific observations), Bayes’ Theorem helps us calculate the updated probability (posterior) of the hypothesis (i.e., belief in God). This represents how our belief in the existence of God changes after taking the new evidence into account. Using Bayes’ Theorem to Argue for or Against God’s Existence:Bayes’ Theorem has been used in debates both for and against the existence of God. Here’s how different arguments might be framed using Bayesian reasoning: Theistic Argument (Evidence Supporting God): A common theistic argument might use the fine-tuning of the universe as the evidence $ B $. Proponents argue that the existence of such precise conditions for life is much more likely if God exists (high $ P(B | A) $) than if God does not exist (low $ P(B) $). If we assign a reasonable prior probability $ P(A) $ to God’s existence, then the posterior probability $ P(A | B) $ might be higher after considering the evidence. Atheistic Argument (Evidence Against God): On the other hand, someone skeptical of the existence of God might point to the problem of evil as evidence $ B $. They could argue that the existence of so much suffering and evil in the world is more likely if God does not exist than if He does (i.e., $ P(B | A) $ is low if God exists). This would reduce the posterior probability $ P(A | B) $, leading to a lower belief in God’s existence after considering the evidence of evil. Bayesian Belief and Faith:It’s important to note that while Bayesian reasoning is a powerful tool for updating beliefs in light of evidence, religious belief often involves elements of faith that may not be easily reducible to probabilistic reasoning. Faith can involve trust, personal experiences, and relationships that go beyond mere empirical evidence. Faith might be seen as an area where prior beliefs are so strong that evidence $ B $ doesn’t significantly shift the posterior probability. For example, for a devout person, negative evidence might have little effect on belief in God because their prior probability $ P(A) $ is so high. Limitations of Bayesian Reasoning in Theological Contexts: Subjectivity of Priors: The choice of the prior probability $ P(A) $ (belief in God before evidence) is subjective and varies significantly between individuals. What one person considers a strong prior (e.g., due to cultural or personal reasons) might be weak for another. Interpretation of Evidence: Evidence $ B $ can also be interpreted differently. For example, a natural disaster might be seen by some as evidence against God (because of suffering), while others might interpret it as a test of faith or part of a divine plan. Non-empirical Nature of Religious Experience: Many aspects of religious belief, such as spiritual experiences or faith in sacred texts, may not be directly quantifiable in the same way that physical or scientific evidence is. Conclusion:Bayesian conditional probability provides a formal way to update beliefs based on new evidence, and it can certainly be applied to discussions about the existence of God. However, the Bayesian framework relies heavily on subjective priors and how evidence is interpreted, which can vary greatly among individuals. While it can be a useful tool in philosophical debates about God’s existence, belief in God often transcends empirical evidence and enters the realm of faith, which might not fit neatly into a probabilistic model.","categories":[{"name":"Other","slug":"Other","permalink":"https://shiyisteezin.github.io/nlp-docs/categories/Other/"}],"tags":[{"name":"math","slug":"math","permalink":"https://shiyisteezin.github.io/nlp-docs/tags/math/"},{"name":"determinism","slug":"determinism","permalink":"https://shiyisteezin.github.io/nlp-docs/tags/determinism/"}]},{"title":"Compositionality","slug":"compositionality","date":"2024-09-17T13:41:30.000Z","updated":"2024-12-05T18:39:15.372Z","comments":true,"path":"2024/09/17/compositionality/","permalink":"https://shiyisteezin.github.io/nlp-docs/2024/09/17/compositionality/","excerpt":"","text":"Dealing with CompositionalityThis blog will introduce the research done in syntax that addressed compositionality. In one of the connectionist natural language processing papers I have read about, it touches on government binding theory proposed by Chomsky, and the paper tried to model the motion from d-structure to s-structure in GB theory through the non-overlap constraint and chain map combined with NNs. And the demonstration of the non-overlap map is below Non-Overlap Constraint ExplainedThe non-overlap constraint is a rule in cognitive models or neural networks that prevents overlapping activations of units in a chain map. This ensures that no two units representing the same syntactic marker can be active simultaneously, which helps maintain clear and distinct representations. Diagram BreakdownComponents:Chain Map (Green Text): Represents the initial activation of units. Units in this map correspond to elements or tokens that can be active. Non-Overlap Map (Red Text): Corresponds to the chain map and enforces non-overlapping activations. Units in this map prevent other units in the same diagonal from activating. Diagonal, Non-Lateral Inhibitory Links (Black Bold Arrows): These links prevent other units in the corresponding diagonal of the chain map from activating, thereby enforcing the non-overlap constraint. Activation of Corresponding Unit In The Non-Overlap Map (Red Arrow) The red arrow or the lateral link indicates an activation process from the chain map where there is a unit activated. Correspondingly, there is a unit in direct parallel in the non-overlap map being activated. Process: When a unit in the chain map is activated, it activates its corresponding unit in the non-overlap map. The active unit in the non-overlap map then inhibits all other units in the same diagonal in the chain map. This ensures no two units in the chain map, which represent the same syntactic marker, can be active simultaneously. Code Example with ExplanationLet’s look at a pseudo implementation that matches the diagram: 12345678910111213141516171819202122232425262728293031323334353637383940414243444546class Unit: def __init__(self, identifier): self.identifier = identifier self.active = False def __repr__(self): return f&quot;Unit(&#123;self.identifier&#125;, active=&#123;self.active&#125;)&quot;class Map: def __init__(self, name): self.name = name self.units = [[Unit(f&quot;&#123;name&#125;&#123;chr(65+i)&#125;&#123;j+1&#125;&quot;) for j in range(5)] for i in range(5)] def activate_unit(self, row, col): self.units[row][col].active = True print(f&quot;Activating &#123;self.units[row][col]&#125; in &#123;self.name&#125; map.&quot;) self.enforce_non_overlap(row, col) def enforce_non_overlap(self, row, col): for i in range(5): if i != row: self.units[i][col].active = False print(f&quot;Deactivating &#123;self.units[i][col]&#125; in &#123;self.name&#125; map due tonon-overlap constraint.&quot;)# Initialize Chain Map and Non-Overlap Mapchain_map = Map(&quot;ChainMap&quot;)non_overlap_map = Map(&quot;NonOverlapMap&quot;)# Activate unit in Chain Mapchain_map.activate_unit(0, 0)# Corresponding unit in Non-Overlap Map becomes activenon_overlap_map.activate_unit(0, 0)# Output the state of mapsprint(&quot;Chain Map State:&quot;)for row in chain_map.units: print(row)print(&quot;\\nNon-Overlap Map State:&quot;)for row in non_overlap_map.units: print(row) SummaryChain Map Activation: Activating a unit in the chain map triggers the corresponding unit in the non-overlap map. Example: Activating ChainMapA1 will activate NonOverlapMapA1. Non-Overlap Map Enforces Constraint: The activated unit in the non-overlap map inhibits other units in the same diagonal in the chain map. This ensures that other units in the corresponding diagonal of the chain map remain inactive, preserving the non-overlap constraint. ConclusionBy combining the visual diagram with the detailed code example, we’ve illustrated how the non-overlap constraint is implemented and enforced in a cognitive or neural model. The non-overlap map plays a crucial role in ensuring that units representing the same syntactic marker do not overlap in their activation, maintaining a clear and distinct representation of information.","categories":[{"name":"Linguistics","slug":"Linguistics","permalink":"https://shiyisteezin.github.io/nlp-docs/categories/Linguistics/"}],"tags":[]},{"title":"Intro to Determinism","slug":"determinism","date":"2024-09-02T13:15:30.000Z","updated":"2024-12-05T18:39:15.372Z","comments":true,"path":"2024/09/02/determinism/","permalink":"https://shiyisteezin.github.io/nlp-docs/2024/09/02/determinism/","excerpt":"","text":"Introduction to Determinism in Philosophy and MathematicsDeterminism is the philosophical idea that every event or state of affairs, including human decisions, is the consequence of preceding events according to fixed laws of nature. In its purest form, determinism implies that if we had perfect knowledge of the current state of the universe, we could predict all future events with certainty. In contrast, non-determinism implies the possibility of multiple potential outcomes for any given situation, introducing elements of uncertainty, chance, or randomness. In mathematics and computer science, determinism refers to systems or processes where outcomes are strictly determined by initial states and inputs, leading to predictable results. Non-determinism, on the other hand, allows for multiple potential outcomes given the same initial state and inputs, introducing randomness or unpredictability. Philosophical Aspects of DeterminismFrom a philosophical perspective, determinism is often discussed in relation to: Causality: Every effect has a specific cause, and the future is just a sequence of causes and effects that can be predicted. Free Will: One of the central debates is whether free will can exist in a deterministic universe. If everything is predetermined by prior states and natural laws, how do individuals exercise free choice? Compatibilism vs. Incompatibilism: Compatibilists argue that free will and determinism are not mutually exclusive. People can still be held accountable for their actions even if those actions are predetermined by prior causes. Incompatibilists argue that true free will cannot exist in a deterministic framework. Mathematical Aspects of DeterminismIn mathematics, determinism is often studied through computational models and probabilistic systems that describe how inputs relate to outputs in predictable or unpredictable ways. 1. Information Theory: Entropy Entropy is a measure of uncertainty or unpredictability in a system. In a deterministic system, there is little or no entropy because the outcome is always predictable. Example: If you flip a coin that always lands on heads, the entropy is zero because there’s no randomness. Code Example (Python): 12345678910import mathdef entropy(probabilities): return -sum(p * math.log2(p) for p in probabilities if p &gt; 0)# Example with a biased coin (always heads)probabilities = [1.0] # deterministic, no uncertaintyprint(f&#x27;Entropy: &#123;entropy(probabilities)&#125;&#x27;) # Output: 0 2. Markov Chains Markov Chains are used to model systems where the future state depends only on the current state. A Markov process can be deterministic (if there is always one future state) or stochastic (if there are multiple possible future states with different probabilities). Example: A Markov chain can represent a simplified weather model where the weather today (sunny or rainy) depends only on the weather yesterday. Code Example (Python): 1234567891011121314151617import randomdef markov_chain(current_state, transitions): return random.choices(list(transitions[current_state].keys()), list(transitions[current_state].values()))[0]transitions = &#123; &#x27;sunny&#x27;: &#123;&#x27;sunny&#x27;: 0.8, &#x27;rainy&#x27;: 0.2&#125;, &#x27;rainy&#x27;: &#123;&#x27;sunny&#x27;: 0.3, &#x27;rainy&#x27;: 0.7&#125;&#125;# Starting from &#x27;sunny&#x27;current_state = &#x27;sunny&#x27;for _ in range(5): current_state = markov_chain(current_state, transitions) print(f&#x27;Next state: &#123;current_state&#125;&#x27;) This example shows a stochastic process. The current state affects future states, but it’s non-deterministic due to the probabilistic transitions. 3. Finite State Machines (FSMs) Finite State Machines are mathematical models of computation. In linguistics, they are used to model grammar and syntax. Deterministic Finite Automata (DFA): A DFA is a machine where for each state and input, there is a unique next state. It processes strings in a predictable manner. Non-Deterministic Finite Automata (NFA): An NFA allows multiple possible states for a given input, introducing elements of non-determinism. Deterministic Finite Automata (DFA) ExampleA DFA processes input strings deterministically, meaning each state has exactly one possible transition for a given input. Example: Recognizing binary strings that end in “01”. Code Example (Python): 12345678910111213141516171819202122class DFA: def __init__(self, transition_table, start_state, accept_states): self.transition_table = transition_table self.state = start_state self.accept_states = accept_states def process(self, input_string): for symbol in input_string: self.state = self.transition_table[self.state][symbol] return self.state in self.accept_states# DFA that recognizes strings ending in &#x27;01&#x27;transition_table = &#123; 0: &#123;&#x27;0&#x27;: 1, &#x27;1&#x27;: 0&#125;, 1: &#123;&#x27;0&#x27;: 1, &#x27;1&#x27;: 2&#125;, 2: &#123;&#x27;0&#x27;: 1, &#x27;1&#x27;: 0&#125;&#125;dfa = DFA(transition_table, 0, &#123;2&#125;)print(dfa.process(&#x27;10101&#x27;)) # True, as it ends in &#x27;01&#x27;print(dfa.process(&#x27;1001&#x27;)) # False Non-Deterministic Finite Automata (NFA) ExampleIn an NFA, a machine can move to multiple states simultaneously, and if any state reaches an accepting state, the input is accepted. Example: Recognizing binary strings that contain “01” as a substring. Code Example (Python): 123456789101112131415161718192021222324252627class NFA: def __init__(self, transition_table, start_states, accept_states): self.transition_table = transition_table self.start_states = start_states self.accept_states = accept_states def process(self, input_string): current_states = self.start_states for symbol in input_string: next_states = set() for state in current_states: if symbol in self.transition_table[state]: next_states.update(self.transition_table[state][symbol]) current_states = next_states return bool(current_states &amp; self.accept_states)# NFA recognizing strings containing &#x27;01&#x27;transition_table = &#123; 0: &#123;&#x27;0&#x27;: &#123;0, 1&#125;, &#x27;1&#x27;: &#123;0&#125;&#125;, 1: &#123;&#x27;1&#x27;: &#123;2&#125;&#125;, 2: &#123;&#125;&#125;nfa = NFA(transition_table, &#123;0&#125;, &#123;2&#125;)print(nfa.process(&#x27;10101&#x27;)) # True, as &#x27;01&#x27; is a substringprint(nfa.process(&#x27;1111&#x27;)) # False DFA vs NFA in Handling Different Tasks DFAs are faster and easier to implement in practice because of their deterministic nature. They are predictable, and given an input string, there’s only one possible state at any time. NFAs offer more flexibility and simplicity in design. However, they require more complex computation to handle non-determinism, as multiple states may need to be tracked simultaneously. In computational theory, DFAs and NFAs are equivalent in power, meaning any language that can be recognized by an NFA can also be recognized by a DFA (though converting an NFA to a DFA can lead to exponential growth in the number of states). Conclusion: Blending Determinism and Non-DeterminismDeterminism and non-determinism are foundational concepts in both philosophical and mathematical domains. While deterministic systems allow predictability and order, non-determinism introduces flexibility and accounts for uncertainty, which is critical for modeling real-world processes. Whether in the context of linguistic models, automata theory, or information systems, blending deterministic and non-deterministic elements enables the creation of robust, flexible models for solving complex problems.","categories":[{"name":"Math","slug":"Math","permalink":"https://shiyisteezin.github.io/nlp-docs/categories/Math/"}],"tags":[]},{"title":"Naturalizing Intentions","slug":"intentionality","date":"2024-09-02T13:15:30.000Z","updated":"2024-12-05T18:39:15.373Z","comments":true,"path":"2024/09/02/intentionality/","permalink":"https://shiyisteezin.github.io/nlp-docs/2024/09/02/intentionality/","excerpt":"","text":"The Hard Problem in Naturalizing IntentionalityThe Hard Problem of Intentionality mirrors the broader Hard Problem of Consciousness, focusing on a specific aspect of mental life: intentionality—the mind’s ability to represent, think about, or be “about” things. Both problems grapple with the difficulty of explaining subjective mental phenomena in purely naturalistic or physical terms. Let’s break this down into key concepts and challenges in a comprehensive way: 1. What is Intentionality?Intentionality is the capacity of the mind to be directed toward objects, states, or events. When we think, perceive, believe, or desire, our mental states are always about something: Thinking about a tree. Perceiving the sound of a car horn. Believing that it will rain tomorrow. This “aboutness”—the idea that our thoughts are directed at or represent things—is what makes intentionality a unique feature of the mind. The mind doesn’t just generate random activity; it represents things in the world. 2. The Naturalization of IntentionalityThe goal of naturalizing intentionality is to explain this mental “aboutness” in terms of physical processes, such as brain activity, evolutionary biology, or cognitive systems. Naturalism seeks to show how the mind’s ability to represent the world can be explained as part of the natural world, without invoking anything supernatural or mysterious. Many philosophers and cognitive scientists believe that intentionality can be reduced to causal relationships, information processing, or evolutionary functions. For example: Causal Theories suggest that our mental representations are caused by the things they represent. You have a mental image of a tree because the tree caused certain sensory processes (light entering the eyes, neurons firing, etc.). Teleosemantics explains intentionality in terms of biological functions: a mental state represents something because it evolved to fulfill a particular function, such as guiding behavior in response to environmental stimuli. Information-Theoretic Approaches view mental states as information processors, analogous to computers that process inputs and outputs, where the inputs are representations of the external world. 3. The Subjective Nature of Intentionality: The Hard ProblemDespite these approaches, many philosophers argue that these naturalistic explanations leave out something essential—the subjective aspect of intentionality. This is known as the Hard Problem of Intentionality. A. Subjective “Aboutness”At the heart of the hard problem is the issue of subjectivity. When you have a thought about a tree, that thought is experienced from your first-person perspective. It’s not just that brain activity happens; you experience that thought as being about something in the world. This subjective experience of “aboutness” seems qualitatively different from the purely physical processes that naturalistic theories describe. For example: A causal theory might explain that seeing a tree causes certain brain states, but why does that brain state have the subjective character of “being about a tree”? Teleosemantics might explain that certain brain processes evolved to represent environmental features for survival, but how does that give rise to the feeling of “thinking about” something in a first-person, subjective way? This challenge is often compared to the Hard Problem of Consciousness, which philosopher David Chalmers famously articulated. The hard problem of consciousness asks how **subjective experience (qualia)**—the “what it’s like” aspect of experience—arises from physical processes in the brain. Similarly, the hard problem of intentionality asks how the subjective aspect of intentionality arises from natural processes. B. Intentionality as NormativeAnother element that naturalistic explanations struggle with is the normativity of intentionality. Intentional states are not just about things—they can also be correct or incorrect. For example: If you believe that the sky is blue, and it is blue, your belief is correct. If you believe the sky is green, your belief is incorrect. This capacity for mental states to be evaluated (correct vs. incorrect) makes intentionality normative—mental states can succeed or fail at representing the world. The hard problem of intentionality asks how purely physical processes (which are generally described in descriptive, not normative, terms) can give rise to normative mental properties. 4. How is This Like the Hard Problem of Consciousness?The hard problem of intentionality is closely related to the hard problem of consciousness because both deal with subjective experience and its relationship to physical processes. In both cases, naturalistic explanations tend to focus on the objective, third-person description of processes, but they struggle to account for the first-person experience of those processes. The hard problem of consciousness asks: “How do subjective experiences (what it’s like to feel pain, see red, etc.) arise from the firing of neurons in the brain?” The hard problem of intentionality asks: “How does the mind’s ability to be about things (representing a tree, desiring an outcome, etc.) arise from brain activity and causal relationships?” In both cases, the difficulty is explaining the qualitative, subjective aspect of mental life, which seems to resist explanation in terms of objective physical processes. 5. Philosophical ChallengesThere are several reasons why naturalizing intentionality faces deep philosophical challenges: A. The Explanatory GapJust as with consciousness, there seems to be an explanatory gap between physical processes and the subjective “aboutness” of intentionality. Even if we can fully map the brain processes involved in thinking about a tree, it’s unclear how or why those processes lead to the experience of “thinking about the tree.” This gap is sometimes described as a difference in kinds: physical processes and subjective intentional states are so different in their nature that it’s hard to see how one could reduce to or explain the other. B. Non-reductive ViewsSome philosophers argue that intentionality, like consciousness, might not be reducible to physical processes. Non-reductive physicalism or property dualism are philosophical views suggesting that while mental states are grounded in the physical, they involve emergent properties that cannot be fully explained by physical processes alone. For instance, John Searle argues that while intentionality is a biological feature of the brain, it cannot be reduced to mere causal or functional processes—it’s a higher-level feature that arises from, but isn’t identical to, brain processes. C. Phenomenology and First-Person PerspectivePhenomenologists, like Edmund Husserl and Maurice Merleau-Ponty, argue that intentionality can only be fully understood by focusing on the first-person experience of being directed toward the world. They believe that trying to reduce intentionality to objective, third-person descriptions (such as causal processes or information flow) misses its core feature: the lived, subjective experience of being directed toward the world. 6. Responses to the Hard Problem of IntentionalitySome responses to the hard problem of intentionality propose that the subjective aspect of intentionality might not be as intractable as it seems: Cognitive Science: Some cognitive scientists believe that advances in understanding the brain’s computational processes will eventually bridge the gap. By explaining how the brain processes information, stores representations, and learns from the environment, they hope to explain the emergence of intentionality in a way that accounts for both its subjective and objective aspects. Emergentism: Some philosophers suggest that intentionality is an emergent property of complex brain systems. While the individual parts of the brain may not be intentional, intentionality may emerge from the complex interaction of those parts. However, explaining how emergence happens in detail remains a challenge. Panpsychism: In a more radical response, some philosophers (like David Chalmers) propose panpsychism, the view that consciousness (and perhaps intentionality) is a fundamental feature of the universe, much like space, time, or matter. In this view, intentionality wouldn’t need to be “reduced” to physical processes because it would be a basic feature of reality. 7. ConclusionThe Hard Problem of Intentionality raises profound questions about how the mind represents the world and how subjective mental states can arise from physical processes. While naturalistic theories of intentionality (like causal, teleosemantic, and information-theoretic approaches) have made progress in explaining aspects of mental representation, the subjective aspect of intentionality—the first-person experience of “aboutness”—remains elusive. This challenge parallels the Hard Problem of Consciousness, leaving open fundamental questions about the nature of the mind and its relationship to the physical world.","categories":[{"name":"Other","slug":"Other","permalink":"https://shiyisteezin.github.io/nlp-docs/categories/Other/"}],"tags":[{"name":"nlp-theories","slug":"nlp-theories","permalink":"https://shiyisteezin.github.io/nlp-docs/tags/nlp-theories/"}]},{"title":"Understanding Qualia","slug":"qulia","date":"2024-09-02T13:15:30.000Z","updated":"2024-12-05T18:39:15.378Z","comments":true,"path":"2024/09/02/qulia/","permalink":"https://shiyisteezin.github.io/nlp-docs/2024/09/02/qulia/","excerpt":"","text":"This blog will try to understand Qualia comprehensively.Qualia (pronounced KWAL-ee-uh) are the subjective, internal experiences that each individual perceives when interacting with the world. These experiences refer to the way things feel to an individual, rather than their physical or scientific properties. Examples include: The specific redness of a rose as perceived by an individual. The feeling of pain from a stubbed toe. The taste of chocolate or the sound of a piano. Qualia are the building blocks of conscious experience and are central to discussions in philosophy of mind, consciousness studies, and cognitive science. Key Characteristics of Qualia Subjectivity: Qualia are unique to each person. While two people might look at the same object, such as a red apple, they might experience the color “red” differently, though both refer to it as “red.” No one can truly know what another person’s subjective experience of “redness” is like. Inaccessibility: Qualia cannot be measured or observed by others. You cannot fully explain what chocolate tastes like to someone who has never tasted it. This private aspect of qualia makes them hard to study scientifically. Qualitative Nature: Qualia focus on the quality of experience. It is the “what it feels like” to perceive something, rather than objective measurements like wavelength of light or sound frequencies. Examples of Qualia Visual Qualia: How an individual experiences colors, shapes, and light. For example, the blue of the sky or the green of a forest. Auditory Qualia: The perception of sounds, like the particular tone of a violin or the loudness of a car horn. Tactile Qualia: The sensation of touch, such as the softness of a blanket or the sharpness of a needle. Emotional Qualia: The subjective feeling of emotions, such as joy, sadness, or anger. Philosophical Importance The Hard Problem of Consciousness: Philosopher David Chalmers highlighted the difficulty of explaining qualia as part of what he called the “hard problem of consciousness.” While scientists can explain how the brain processes sensory data, they struggle to explain why these processes result in subjective experiences. The Inverted Spectrum Problem: Imagine two people both looking at a red apple. One person sees the apple as we traditionally understand red, but the other person’s brain might perceive red as what the first person would call blue. However, both use the word “red” to describe the apple. This thought experiment illustrates the subjectivity and mystery surrounding qualia. Philosophical Zombies: This is a hypothetical scenario where beings exist that act exactly like humans but have no conscious experience (i.e., they have no qualia). This idea is used in philosophy to discuss whether consciousness and qualia are necessary for intelligent behavior. Scientific Challenges Measuring Qualia: One of the biggest challenges in studying qualia is their subjective nature. Since they can’t be directly observed or quantified, scientists find it hard to fit them into objective frameworks. Relationship to the Physical Brain: Neuroscientists attempt to map brain activity to conscious experiences, but no clear pathway explains how specific brain processes produce qualia. For example, how does the firing of neurons in the visual cortex lead to the experience of seeing blue? Theories on Qualia Dualism: This theory, most famously supported by philosopher René Descartes, argues that mind and matter are two distinct substances. According to dualists, qualia exist in the mental realm and cannot be fully explained by physical processes alone. Physicalism: In contrast, physicalists believe that everything, including qualia, is ultimately physical in nature. They argue that qualia are simply the result of brain processes, though the exact mechanisms are not yet fully understood. Panpsychism: A more radical view, panpsychism suggests that all matter, even down to the level of particles, has some form of conscious experience. Under this theory, qualia might be seen as a fundamental property of the universe. Why Qualia MatterThe study of qualia isn’t just an academic exercise; it challenges our understanding of the nature of consciousness, self-awareness, and human experience. The debate over qualia touches on: Artificial Intelligence: Can machines experience qualia? If not, will they ever truly understand human experiences? Ethics and Morality: If we can’t fully understand someone else’s qualia, how can we judge their experiences of pleasure, pain, or emotion? Neuroscience: What part of the brain is responsible for creating these subjective experiences? In summary, qualia are a crucial concept for understanding how we experience the world, presenting a philosophical and scientific puzzle that remains unsolved. While we can measure physical processes, the “why” and “how” of personal experience continue to elude a definitive explanation.","categories":[{"name":"Other","slug":"Other","permalink":"https://shiyisteezin.github.io/nlp-docs/categories/Other/"}],"tags":[]},{"title":"The GPT Architecture","slug":"transformer","date":"2024-08-17T13:16:00.000Z","updated":"2024-12-05T18:39:15.379Z","comments":true,"path":"2024/08/17/transformer/","permalink":"https://shiyisteezin.github.io/nlp-docs/2024/08/17/transformer/","excerpt":"","text":"Summary and breakdown of the code that form the Generative Pre-trained Transformer architecture continuedLet’s break down the code snippet line by line to understand what each step does in the context of creating positional encodings for a Transformer model using PyTorch. Code Snippet12345div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))pe[:, 0::2] = torch.sin(position * div_term)pe[:, 1::2] = torch.cos(position * div_term)pe = pe.unsqueeze(0).transpose(0, 1) Explanation1. div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model)) Purpose: Calculate the denominator for the sine and cosine functions in the positional encoding formula. Breakdown: torch.arange(0, d_model, 2): Creates a tensor with values starting from 0 to d_model - 1 with a step size of 2. This gives us indices like [0, 2, 4, ..., d_model-2]. If d_model is 512, this tensor will have 256 values: [0, 2, 4, ..., 510]. .float(): Converts the tensor to a floating-point type. (-math.log(10000.0) / d_model): Computes a scaling factor for the positional encoding formula. The value 10000.0 is a hyperparameter that determines the rate of change of the sine and cosine functions. *: Multiplies each value in the tensor by the scaling factor. torch.exp(): Applies the exponential function to each element in the tensor, resulting in the div_term tensor which will be used to scale the positions. 2. pe[:, 0::2] = torch.sin(position * div_term) Purpose: Compute the sine values for even-indexed dimensions in the positional encoding matrix. Breakdown: position: A tensor representing the positions of the words in the sequence. This could be something like torch.arange(0, max_len).unsqueeze(1), where max_len is the maximum sequence length. position * div_term: Element-wise multiplication of the position tensor with the div_term tensor calculated earlier. This scales the positions appropriately. torch.sin(): Applies the sine function to each element in the resulting tensor. pe[:, 0::2]: Selects all rows (:) and every second column starting from 0 (0::2). This targets the even-indexed dimensions of the positional encoding matrix. =: Assigns the computed sine values to these selected positions in the positional encoding matrix pe. 3. pe[:, 1::2] = torch.cos(position * div_term) Purpose: Compute the cosine values for odd-indexed dimensions in the positional encoding matrix. Breakdown: position * div_term: Same as above, scales the positions appropriately. torch.cos(): Applies the cosine function to each element in the resulting tensor. pe[:, 1::2]: Selects all rows (:) and every second column starting from 1 (1::2). This targets the odd-indexed dimensions of the positional encoding matrix. =: Assigns the computed cosine values to these selected positions in the positional encoding matrix pe. 4. pe = pe.unsqueeze(0).transpose(0, 1) Purpose: Reshape the positional encoding matrix to match the expected input shape for the Transformer model. Breakdown: pe.unsqueeze(0): Adds an extra dimension at the 0-th position. If pe originally has shape (max_len, d_model), it will now have shape (1, max_len, d_model). This extra dimension is often used to represent the batch size, which is 1 in this case. transpose(0, 1): Swaps the 0-th and 1-st dimensions. After this operation, the shape will be (max_len, 1, d_model). This step ensures that the positional encoding matrix can be correctly broadcasted and added to the input embeddings in the Transformer model. The division by d_model in the expression (-math.log(10000.0) / d_model) is a critical part of the positional encoding design in the Transformer model. This design ensures that different dimensions of the positional encoding vary at different frequencies. Here’s a more detailed explanation: Positional Encoding in TransformersThe idea behind positional encoding is to inject information about the position of each token in the sequence into the token’s embedding. This is necessary because the Transformer model, unlike RNNs or CNNs, does not inherently capture the order of tokens. Frequency Scaling Frequency Spectrum: By dividing by d_model, we spread the frequencies of the sine and cosine functions across the dimensions of the embedding vector. The lower dimensions correspond to lower frequencies, and the higher dimensions correspond to higher frequencies. This spread allows the model to capture a wide range of positional dependencies. Mathematical Justification: The formula for positional encoding in the Transformer is designed such that for a given position $ pos $ and dimension $ i $: $ PE_{(pos, 2i)} &#x3D; \\sin\\left(\\frac{pos}{10000^{\\frac{2i}{d_{\\text{model}}}}}\\right) $ $ PE_{(pos, 2i+1)} &#x3D; \\cos\\left(\\frac{pos}{10000^{\\frac{2i}{d_{\\text{model}}}}}\\right) $ The term $\\frac{1}{10000^{\\frac{2i}{d_{\\text{model}}}}}$ ensures that the positions are scaled appropriately across different dimensions. Implementation: The division by d_model normalizes the range of exponents to ensure they vary smoothly between 0 and 1, creating a geometric progression of frequencies. Detailed Steps Let’s rewrite the specific part of the code to understand its purpose: 12div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model)) Let’s break down the specific line of code div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model)) and explain its purpose in the context of positional encoding in the Transformer model. Purpose of the CodeThis line of code is part of the positional encoding generation process in the Transformer model, as described in the paper “Attention is All You Need”. The positional encodings allow the model to utilize the order of the sequence since the Transformer itself is position-agnostic. Breaking Down the Code1. torch.arange(0, d_model, 2) Purpose: Creates a sequence of even integers from 0 to d_model - 2. Example: If d_model is 512, torch.arange(0, d_model, 2) generates a tensor containing [0, 2, 4, ..., 510]. 1indices = torch.arange(0, d_model, 2) Output: A tensor of shape (d_model/2,) containing even integers up to d_model - 2. 2. .float() Purpose: Converts the integer tensor to a tensor of floats. This is necessary because we will perform mathematical operations that require floating-point precision. Example: Continuing from the previous step, .float() converts the integer tensor to floating-point numbers. 1indices = indices.float() Output: A tensor of shape (d_model/2,) containing floating-point numbers [0.0, 2.0, 4.0, ..., 510.0]. 3. (-math.log(10000.0) / d_model) Purpose: Computes a scaling factor for the positional encodings. The value -math.log(10000.0) / d_model ensures the positional encodings have values that decay exponentially. Value: If d_model is 512, this term calculates to -math.log(10000.0) / 512 ≈ -0.02302585. 1scale_factor = -math.log(10000.0) / d_model 4. * scale_factor Purpose: Multiplies each element in the tensor of indices by the scale factor. This operation scales the indices to a range suitable for the exponential function, ensuring the positional encodings vary smoothly. Example: Continuing from the previous steps, indices * scale_factor scales each index. 1scaled_indices = indices * scale_factor Output: A tensor of shape (d_model/2,) with scaled values. 5. torch.exp(scaled_indices) Purpose: Applies the exponential function to each element in the scaled tensor. The exponential function is used to create a set of frequencies for the positional encodings. Example: Applying the exponential function to the scaled indices. 1div_term = torch.exp(scaled_indices) Output: A tensor of shape (d_model/2,) containing the calculated frequencies for the positional encodings. Final OutputThe variable div_term now contains a series of exponentially scaled values. These values are used to create the positional encodings, which alternate between sine and cosine functions at different frequencies. 12345678910import torchimport mathd_model = 512 # Example valueindices = torch.arange(0, d_model, 2).float()scale_factor = -math.log(10000.0) / d_modelscaled_indices = indices * scale_factordiv_term = torch.exp(scaled_indices)print(div_term) Summary torch.arange(0, d_model, 2).float(): Creates a tensor of even indices from 0 to d_model - 2 and converts them to floats. (-math.log(10000.0) / d_model): Computes a scaling factor. * scale_factor: Scales the indices by the computed factor. torch.exp(scaled_indices): Applies the exponential function to get the final div_term. Purpose in Positional EncodingThe div_term tensor represents the denominators for the positional encodings’ sine and cosine functions. These frequencies ensure that different positions in the input sequence have unique encodings, allowing the Transformer model to infer the position of each token. The overall goal is to introduce a form of positional information that helps the model understand the order of the sequence. Intuitive Understanding Varying Frequencies: Lower dimensions of the embedding vector (e.g., dimensions 0, 2, 4) will vary more slowly (lower frequency). Higher dimensions (e.g., dimensions 508, 510) will vary more quickly (higher frequency). Why Divide by d_model: To ensure that the entire range of positional encodings uses a range of frequencies from very slow to very fast. This allows the Transformer to distinguish between different positions effectively. Example CalculationLet’s assume d_model = 512: For dimension i = 0: The exponent would be $\\frac{0}{512} &#x3D; 0$. So, the term would be $10000^{0} &#x3D; 1$. For dimension i = 256: The exponent would be $\\frac{256}{512} &#x3D; 0.5$. So, the term would be $10000^{0.5} &#x3D; 100$. The above steps ensure that the positional encoding matrix has a smooth and gradual change in frequencies across the dimensions, which helps the model to capture the positional information effectively. Summary Dividing by d_model ensures the frequencies of sine and cosine functions used in positional encodings are spread across a wide range. This design allows the Transformer model to learn and utilize positional information effectively, enhancing its ability to understand the order and relative position of tokens in a sequence.","categories":[{"name":"NLP Related","slug":"NLP-Related","permalink":"https://shiyisteezin.github.io/nlp-docs/categories/NLP-Related/"}],"tags":[{"name":"nlp-theories","slug":"nlp-theories","permalink":"https://shiyisteezin.github.io/nlp-docs/tags/nlp-theories/"}]},{"title":"Contemporary NLP","slug":"mdn-nlp","date":"2024-08-13T22:27:14.000Z","updated":"2024-12-05T18:39:15.375Z","comments":true,"path":"2024/08/13/mdn-nlp/","permalink":"https://shiyisteezin.github.io/nlp-docs/2024/08/13/mdn-nlp/","excerpt":"","text":"Introduction to Contemporary NLPQ What is the importance of psychological concepts in NLP? A To understand modern natural language processing (NLP), it’s essential to draw inferences from crucial psychological concepts like the Language of Thought Hypothesis and the Representational Theory of Mind. These concepts help explain how our brain processes and produces language and mental representations, which are foundational for NLP. Language of Thought Hypothesis (LOTH)Q What does the Language of Thought Hypothesis (LOTH) propose? A LOTH proposes that our brain has a schema for producing language of thought, known as Mentalese. It suggests that mental states and thoughts have a structured, language-like format, which facilitates reasoning, problem-solving, and decision-making. Q What are propositional attitudes in LOTH? A Propositional attitudes in LOTH refer to the mental states that involve a relationship between a person and a proposition, such as beliefs, desires, and intentions. These attitudes are expressed through mental representations and are essential for inferential reasoning. Representational Theory of MindQ How does the Representational Theory of Mind relate to LOTH? A The Representational Theory of Mind (RTM) supports LOTH by emphasizing that our cognitive abilities, such as conscious decision-making and problem-solving, are based on mental representations. These representations can be analyzed through the semantics of natural language and are crucial for understanding mental processes. Compositionality of Mental Processes (COMP)Q What is the Compositionality of Mental Processes (COMP)? A COMP suggests that mental states have constituent structures similar to natural language. This means that complex thoughts are composed of simpler mental representations, just as complex sentences are formed from simpler linguistic expressions. Q How do ancient and modern researchers differ in their approach to COMP? A Ancient proponents of LOTH used syllogism and propositional logic to analyze the semantics of Mentalese, while modern researchers use predicate calculus and other formal systems to study the compositional nature of mental representations. Concept Acquisition in Language LearningQ How do infants acquire concepts according to hypothesis formulation? A Infants form hypotheses about the world based on their observations and experiences. They test these hypotheses through interactions with their environment, updating their understanding of concepts like gravity through a process of hypothesis testing and model refinement. Type-Token Relation of Mental RepresentationsQ What is the type-token relation in mental representations? A The type-token relation distinguishes between different instances (tokens) of the same mental representation (type). For example, two instances of the word “cat” in different contexts are tokens of the same type in Mentalese. More on Type-Token Identity TheoryQ What is the Token-Type Identity Theory? A The Token-Type Identity Theory is a perspective in philosophy of mind that suggests that mental states and processes are identical to specific physical states and processes in the brain. According to this theory, each mental state (a token) is a unique instance of a physical state (a type) in the brain. Type and TokenQ What is the difference between a type and a token in this theory? A In the context of Token-Type Identity Theory: A type refers to a general category or class of mental states, such as the concept of “pain” or “belief.” A token is a specific instance of a type, such as a particular feeling of pain or a specific belief held by an individual at a given moment. Relation to Mental StatesQ How does Token-Type Identity Theory relate to mental states? A The theory posits that every mental state is a token of a specific type of physical state in the brain. For example, the mental state of feeling happy is identical to a particular pattern of neural activity in the brain, which is a token of the broader type of neural patterns associated with happiness. Advantages of the TheoryQ What are the advantages of Token-Type Identity Theory? A Some advantages of Token-Type Identity Theory include: Scientific Alignment: It aligns with scientific research in neuroscience that links mental processes to brain activity. Simplicity: It offers a straightforward explanation of the mind-body relationship by equating mental states with physical states. Reductionism: It supports a reductionist approach, which aims to explain complex phenomena in terms of simpler physical processes. Multiple Realizability ChallengeQ What is the challenge of multiple realizability in Token-Type Identity Theory? A The challenge of multiple realizability refers to the idea that the same mental state (type) can be realized by different physical states (tokens) in different individuals or species. For example, the mental state of pain might correspond to different neural configurations in humans, animals, or artificial intelligence, challenging the one-to-one correspondence proposed by Token-Type Identity Theory. Functionalism as an AlternativeQ How does functionalism address the challenge of multiple realizability? A Functionalism offers an alternative to Token-Type Identity Theory by defining mental states in terms of their functional roles rather than their physical substrates. According to functionalism, a mental state is identified by what it does rather than what it is made of, allowing for multiple realizations of the same mental state across different physical systems. Historical ContextQ What is the historical context of Token-Type Identity Theory? A Token-Type Identity Theory emerged in the mid-20th century as part of the broader identity theory movement in philosophy of mind. It was developed in response to the limitations of dualism and behaviorism, offering a more scientifically grounded approach to understanding the mind-body relationship. Examples and ApplicationsQ Can you provide examples of Token-Type Identity Theory in practice? A Examples of Token-Type Identity Theory include: Pain: A specific neural pattern in the brain that corresponds to the feeling of pain is a token of the type “pain.” Belief: A particular neural configuration associated with the belief that “the sky is blue” is a token of the type “belief.” CriticismsQ What are some criticisms of Token-Type Identity Theory? A Criticisms of Token-Type Identity Theory include: Multiple Realizability: The theory struggles to account for the fact that the same mental state can be realized by different physical states. Subjectivity: It may overlook the subjective, qualitative aspects of mental experiences (qualia) that are difficult to reduce to physical states. Reductionism: Some argue that reducing mental states to physical states oversimplifies the complexity of human cognition and consciousness. Modern DevelopmentsQ How has Token-Type Identity Theory evolved in modern philosophy? A In modern philosophy, Token-Type Identity Theory has evolved to incorporate insights from neuroscience and cognitive science. While some philosophers continue to defend the theory, others have developed more nuanced approaches that address its limitations, such as non-reductive physicalism and emergentism. Connectionism in NLPQ What is connectionism and how does it differ from traditional computational models? A Connectionism is an approach that models cognitive processes through networks of interconnected units, similar to neurons in the brain. Unlike traditional symbolic models, connectionist models use distributed representations and learn from experience, providing a more biologically plausible way to emulate brain activity. COMP and Syntactic StructuresQ How does Chomsky’s Transformational Grammar Theory relate to COMP? A Chomsky’s Transformational Grammar Theory demonstrates how complex syntactic structures in natural language can be generated through transformations applied to underlying structures. This theory aligns with COMP by showing how simple linguistic expressions combine to form complex sentences. Statistical Semantics in NLPQ How did statistical NLP change the field of natural language processing? A Statistical NLP introduced probabilistic models and corpus-based approaches, allowing researchers to systematically exploit the distributional properties of language. This shift made it possible to develop more scalable and accurate models for tasks like speech recognition, part-of-speech tagging, and machine translation. Techniques in Statistical NLPQ What are some key techniques used in statistical NLP? A Key techniques in statistical NLP include: TF-IDF Normalization: Assigning weights to words based on their frequency in the document and rarity across the corpus. Bayesian Approach: Using probabilistic models to classify text. Sequence Models and HMMs: Capturing dependencies in text sequences. kNN Method and Decision Trees: Classifying text based on nearest neighbors or decision rules. MaxEnt (Logistic Regression) and SVM: Using advanced statistical models for classification. Connectionism and Deep Neural NetworksQ How have neural networks evolved in NLP? A Neural networks have evolved from simple recurrent neural networks (RNNs) to more advanced models like transformers. RNNs, while powerful, have limitations such as long training times and gradient issues. Transformers, with their attention mechanisms, have surpassed RNNs by enabling parallel processing and capturing long-range dependencies more effectively. In A NutshellQ What is the philosophical significance of the shift to statistical NLP? A The shift to statistical NLP highlights the limitations of introspection and suggests that language and thought are not only symbolic but also deeply quantitative and probabilistic. This perspective has driven the integration of formal logical approaches with statistical methods to achieve deeper understanding and more intelligent behavior in language comprehension and dialogue systems. ReferencesQ Where can I find the resources to understand these concepts? A Here are some key references: Rescorla, Michael. “The Computational Theory of Mind.” The Stanford Encyclopedia of Philosophy (Fall 2020 Edition), edited by Edward N. Zalta. Rumelhart, D. E., McClelland, J. L., & the PDP Research Group. (1986). Parallel Distributed Processing: Explorations in the Microstructure of Cognition. Clark, A. (1993). Connectionism and Cognitive Architecture: A Critical Analysis. Bechtel, W., & Graham, G. (Eds.). (1998). Connectionism and Cognitive Science. Horgan, T., & Tienson, J. (1996). Foundations of Connectionism: A Reassessment. Clark, A. (2001). Mindware: An Introduction to the Philosophy of Cognitive Science. By structuring the article in this Q&amp;A format, it becomes easier to understand the key points and the relationships between different concepts in contemporary NLP.","categories":[{"name":"NLP Related","slug":"NLP-Related","permalink":"https://shiyisteezin.github.io/nlp-docs/categories/NLP-Related/"}],"tags":[{"name":"nlp-theories","slug":"nlp-theories","permalink":"https://shiyisteezin.github.io/nlp-docs/tags/nlp-theories/"}]},{"title":"Philosophy of Mind","slug":"philo-o-mind","date":"2024-08-13T22:27:10.000Z","updated":"2024-12-05T18:39:15.376Z","comments":true,"path":"2024/08/13/philo-o-mind/","permalink":"https://shiyisteezin.github.io/nlp-docs/2024/08/13/philo-o-mind/","excerpt":"","text":"Cognitive Science and The Philosophy of MindQ What is the focus of this blog? A This blog will summarize articles, papers, and materials I have gone through that touch on the subject of Philosophy of Mind and how its presence lays important foundation for the development of general artificial intelligence. The blog covers the following topics: What Constitutes The Philosophy of Mind The Implications of Human Beings As Conscious Automata Consciousness As The Fundamental Property of Nature Consciousness As A Weak, Strong, or Normal Emergence The Primal Instincts vs. The Unknown Theories That Address The Mind-Body Problem Computationalism and The Computational Theory of Mind A Turing Style Computational System The Computational vs The Representational Theory of Mind Computationalism vs Functionalism The Emergence of The Representational Theory of Mind What Are Syntactic Underpinnings? Tying Everything Together and Connecting The Dots In A Nutshell Q What are “easy” and “hard” problems of consciousness? A The easy problems involve understanding mechanisms of perception, attention, and behavior. The hard problem concerns subjective experience or qualia, which are deeply subjective and cannot be directly observed or measured. Q What is fundamental property dualism? A Fundamental property dualism regards conscious mental properties as basic constituents of reality, on a par with fundamental physical properties. This view is also referred to as panpsychism. Q What are the hypotheses over the emergence and origin of consciousness? A The hypotheses include strong emergence, weak emergence, and normal emergence. Each hypothesis offers a different perspective on how consciousness arises from physical processes: Strong Emergence: Higher-level properties that are fundamentally new and cannot be reduced to lower-level explanations. For example, consciousness itself might be considered strongly emergent, involving subjective experiences that cannot be directly deduced from neural activity alone. Weak Emergence: Higher-level properties that are unexpected but fully explainable by lower-level processes. For example, the behavior of a flock of birds can be explained by simple rules followed by individual birds, leading to complex patterns. Normal Emergence: Properties that arise predictably from underlying processes. For example, the temperature of a gas results from the average kinetic energy of its molecules, and this relationship is well-understood and predictable. Q What is the Primal Instincts vs. The Unknown theory? A This theory suggests that humans could perform tasks as automata without being aware of it, citing examples such as driving while talking and fight-or-flight responses. Q What is the significance of consciousness according to Thomas Henry Huxley? A Huxley believed that sensations and feelings are mere byproducts of the brain’s mechanics, and do not cause any behavior. Q What is the &quot;Nomological dangler&quot; according to J.J.C. Smart? A Smart argued that seeing consciousness as a purely physical process eliminates the need to explain the grey area of brain processes in a more scientific and established system. Q What are the theories that address the mind-body problem? A Theories include Type vs. Token Identity Theory, Eliminative Materialism, Functionalism, Neutral Monism, and Mind-Body Dualism. These theories offer different perspectives on the relationship between consciousness and the physical world: Type vs. Token Identity Theory: Proposes that mental states are identical to specific physical states or processes in the brain. Type identity theory suggests each mental state type corresponds to a specific physical state type, while token identity theory allows for different physical states across different instances. Eliminative Materialism: Suggests that current folk psychology and common-sense understandings of mental states, including consciousness, are fundamentally flawed and may be eliminated or revised in light of future scientific understanding. Functionalism: Defines consciousness in terms of functional roles within a system, emphasizing the causal relations between inputs, outputs, and other mental states. Consciousness arises from the functional organization of the brain. Neutral Monism: Proposes that consciousness and physical phenomena are different manifestations of a neutral substance or property underlying reality. Consciousness is neither purely mental nor purely physical but emerges from a more fundamental neutral substrate. Mind-Body Dualism: Posits that consciousness is a non-physical or immaterial aspect of reality. It suggests that consciousness exists independently of physical processes and may have properties that cannot be fully explained in terms of material phenomena. Q What is Computationalism? A Computationalism holds that the mind is a computational system similar to a Turing machine, and core mental processes are computations similar to those executed by a Turing machine. Q What is a Turing-style computational system? A A Turing-style computational system includes memory locations, a central processor, and a machine table that determines the processor’s actions based on its current state and the symbol it is accessing. Q How does the Computational Theory of Mind (CTM) compare with the Representational Theory of Mind (RTM)? A CTM focuses on computational processes, while RTM emphasizes mental representations and their connections to the external world. RTM addresses limitations of CTM by incorporating qualitative aspects of consciousness and flexible cognitive processing. Q What are productivity and systematicity in CTM and RTM? A1 CTM: Productivity: CTM explains the productivity of thoughts by assuming that the mind, as a computational system, can generate an infinite number of thoughts from a finite set of symbols and rules. Systematicity: CTM assumes systematicity by subscribing to the structural organization of thoughts and the systematic rules of inference that govern them. A2 RTM: Productivity: RTM posits that a finite set of symbols in natural language can entertain an infinite number of logical propositions using a finite set of concepts and ideas. Systematicity: RTM highlights the inherent systematic relationships between basic cognitive constituents, facilitating coherent and structured thought processes. Q What are the limitations of CTM? A Limitations include the Symbol Grounding Problem, difficulty in explaining qualia and consciousness, and rigid rule-based processing that may not capture the flexible nature of human cognition.Q What is Connectionism? A Connectionism is an approach within cognitive science that emphasizes distributed processing and learning from experience, using interconnected units similar to neurons in the brain. Q What is the significance of hybrid models in AI? A Hybrid models integrate connectionist ideas with representational theories, combining symbolic manipulation capabilities with the learning and adaptability features of connectionism. Q What are syntactic underpinnings? A Syntactic underpinnings refer to the foundational principles and structures that dictate the formation of sentences and phrases in a language, including rules for word order, phrase structure, and grammatical categories. Q What strategies can address syntactic underpinnings? A Strategies include using grammar formalisms, developing parsing techniques, building rule-based systems, leveraging linguistic resources, and employing machine and deep learning approaches to learn syntactic patterns. Q How does Connectionism address the limitations of CTM? A Connectionism offers a dynamic, continuous representation of cognitive processes through interconnected units, addressing the rigidity and symbolic limitations of CTM by using distributed representations and learning from experience. Q What is the main takeaway from this blog? A The blog explores the Computational Theory of Mind (CTM) and its implications, addressing various theories on consciousness, the mind-body problem, and cognitive processes. It highlights the limitations of CTM and introduces Connectionism and the Representational Theory of Mind (RTM) as alternative approaches. Q Where could I find the resources that help me understand these concepts? A Here are some key references: Rescorla, Michael. \"The Computational Theory of Mind.\" The Stanford Encyclopedia of Philosophy, edited by Edward N. Zalta, Fall 2020 Edition. Rumelhart, David E., James L. McClelland, and the PDP Research Group. Parallel Distributed Processing: Explorations in the Microstructure of Cognition, 1986.","categories":[{"name":"Linguistics","slug":"Linguistics","permalink":"https://shiyisteezin.github.io/nlp-docs/categories/Linguistics/"}],"tags":[{"name":"nlp-theories","slug":"nlp-theories","permalink":"https://shiyisteezin.github.io/nlp-docs/tags/nlp-theories/"}]},{"title":"Mutual Information","slug":"mutual-info","date":"2024-08-03T12:45:14.000Z","updated":"2024-12-05T18:39:15.376Z","comments":true,"path":"2024/08/03/mutual-info/","permalink":"https://shiyisteezin.github.io/nlp-docs/2024/08/03/mutual-info/","excerpt":"","text":"More on Mutual InformationBelow is the code for WAPMI or the Weighted Average Point-wise Mutual Information. And this code measures the distance between two probabilities distributions. It is a method used in computational linguistics to measure the strength of association between words in a given context, typically in the analysis of text data. Imagine you have a box of different colored marbles, and you want to know which colors tend to appear together. WAPMI helps you figure out how often certain words (or colors) appear together more often than by random chance. It’s like a smart way to understand word relationships in sentences! 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859import mathimport collectionsdef calculate_pmi(joint_prob, marginal_prob1, marginal_prob2): &quot;&quot;&quot; Calculate the pointwise mutual information (PMI) between two words. :param joint_prob: The joint probability of the two words :param marginal_prob1: The marginal probability of the first word :param marginal_prob2: The marginal probability of the second word :return: The PMI score &quot;&quot;&quot; if joint_prob == 0 or marginal_prob1 == 0 or marginal_prob2 == 0: return 0 # Avoid division by zero return math.log(joint_prob / (marginal_prob1 * marginal_prob2), 2)def calculate_pmi_corpus_optimized(corpus): &quot;&quot;&quot; Calculate the PMI scores for all pairs of words in a corpus. :param corpus: The corpus of text :return: A dictionary of PMI scores &quot;&quot;&quot; word_counts = collections.defaultdict(int) cooccurrence_counts = collections.defaultdict(int) total_sentences = len(corpus) # Precompute word counts and co-occurrence counts for sentence in corpus: unique_words = set(sentence) # Avoid counting duplicates within the same sentence for word in unique_words: word_counts[word] += 1 for word1 in unique_words: for word2 in unique_words: if word1 != word2: cooccurrence_counts[(word1, word2)] += 1 # Calculate PMI scores pmi_scores = &#123;&#125; for (word1, word2), joint_count in cooccurrence_counts.items(): joint_prob = joint_count / total_sentences marginal_prob1 = word_counts[word1] / total_sentences marginal_prob2 = word_counts[word2] / total_sentences pmi = calculate_pmi(joint_prob, marginal_prob1, marginal_prob2) pmi_scores[(word1, word2)] = pmi return pmi_scores# Example usagecorpus = [ [&quot;this&quot;, &quot;is&quot;, &quot;a&quot;, &quot;foo&quot;, &quot;bar&quot;], [&quot;bar&quot;, &quot;black&quot;, &quot;sheep&quot;], [&quot;foo&quot;, &quot;bar&quot;, &quot;black&quot;, &quot;sheep&quot;], [&quot;sheep&quot;, &quot;bar&quot;, &quot;black&quot;]]pmi_scores = calculate_pmi_corpus_optimized(corpus)print(pmi_scores) A walkthrough of the code part by part. Imports12import mathimport collections import math: Imports the math module, which provides mathematical functions such as logarithms. import collections: Imports the collections module, which provides specialized container datatypes, like defaultdict. Function Definitions1. calculate_pmi Function12345678910111213def calculate_pmi(word1, word2, joint_prob, marginal_prob1, marginal_prob2): &quot;&quot;&quot; Calculate the point-wise mutual information (PMI) between two words. :param word1: The first word :param word2: The second word :param joint_prob: The joint probability of the two words :param marginal_prob1: The marginal probability of the first word :param marginal_prob2: The marginal probability of the second word :return: The PMI score &quot;&quot;&quot; pmi = math.log(joint_prob / (marginal_prob1 * marginal_prob2), 2) return pmi Purpose: Calculates the Pointwise Mutual Information (PMI) score between two words. Parameters: word1 and word2: Words for which PMI is calculated. joint_prob: Probability of both words appearing together. marginal_prob1: Probability of word1 appearing. marginal_prob2: Probability of word2 appearing. pmi Calculation: math.log(joint_prob / (marginal_prob1 * marginal_prob2), 2): Computes the logarithm (base 2) of the ratio of the joint probability to the product of the marginal probabilities. Returns: PMI score. 2. calculate_joint_prob Function123456789101112131415def calculate_joint_prob(word1, word2, corpus): &quot;&quot;&quot; Calculate the joint probability of two words in a corpus. :param word1: The first word :param word2: The second word :param corpus: The corpus of text :return: The joint probability &quot;&quot;&quot; joint_count = 0 for sentence in corpus: if word1 in sentence and word2 in sentence: joint_count += 1 joint_prob = joint_count / len(corpus) return joint_prob Purpose: Calculates the joint probability of two words appearing together in the same sentence. Parameters: word1 and word2: Words to check. corpus: List of sentences (each sentence is a list of words). joint_count: Counts how many sentences contain both word1 and word2. joint_prob Calculation: Divides joint_count by the total number of sentences to get the joint probability. Returns: Joint probability of the two words. 3. calculate_marginal_prob Function1234567891011121314def calculate_marginal_prob(word, corpus): &quot;&quot;&quot; Calculate the marginal probability of a word in a corpus. :param word: The word :param corpus: The corpus of text :return: The marginal probability &quot;&quot;&quot; word_count = 0 for sentence in corpus: if word in sentence: word_count += 1 marginal_prob = word_count / len(corpus) return marginal_prob Purpose: Calculates the marginal probability of a single word. Parameters: word: The word for which probability is calculated. corpus: List of sentences. word_count: Counts how many sentences contain word. marginal_prob Calculation: Divides word_count by the total number of sentences to get the marginal probability. Returns: Marginal probability of the word. 4. calculate_pmi_corpus Function12345678910111213141516171819202122232425262728293031def calculate_pmi_corpus_optimized(corpus): &quot;&quot;&quot; Calculate the PMI scores for all pairs of words in a corpus. :param corpus: The corpus of text :return: A dictionary of PMI scores &quot;&quot;&quot; word_counts = collections.defaultdict(int) cooccurrence_counts = collections.defaultdict(int) total_sentences = len(corpus) # Precompute word counts and co-occurrence counts for sentence in corpus: unique_words = set(sentence) # Avoid counting duplicates within the same sentence for word in unique_words: word_counts[word] += 1 for word1 in unique_words: for word2 in unique_words: if word1 != word2: cooccurrence_counts[(word1, word2)] += 1 # Calculate PMI scores pmi_scores = &#123;&#125; for (word1, word2), joint_count in cooccurrence_counts.items(): joint_prob = joint_count / total_sentences marginal_prob1 = word_counts[word1] / total_sentences marginal_prob2 = word_counts[word2] / total_sentences pmi = calculate_pmi(joint_prob, marginal_prob1, marginal_prob2) pmi_scores[(word1, word2)] = pmi return pmi_scores Purpose: Calculates PMI scores for all pairs of words in the corpus. Parameters: corpus: List of sentences. word_counts: A defaultdict to count occurrences of each word. Count Words: Iterates over each sentence to count occurrences of each word. Compute PMI: Iterates over all pairs of words (excluding pairs where word1 is the same as word2). Calculates joint and marginal probabilities, then computes PMI for each pair. Returns: A dictionary of PMI scores for all word pairs. Example Usage123456789corpus = [ [&quot;this&quot;, &quot;is&quot;, &quot;a&quot;, &quot;foo&quot;, &quot;bar&quot;], [&quot;bar&quot;, &quot;black&quot;, &quot;sheep&quot;], [&quot;foo&quot;, &quot;bar&quot;, &quot;black&quot;, &quot;sheep&quot;], [&quot;sheep&quot;, &quot;bar&quot;, &quot;black&quot;]]pmi_scores = calculate_pmi_corpus(corpus)print(pmi_scores) Purpose: Runs the PMI calculations on a sample corpus and prints the PMI scores for all word pairs. Summary calculate_pmi computes PMI given probabilities. calculate_joint_prob finds how often two words appear together. calculate_marginal_prob finds how often one word appears. calculate_pmi_corpus calculates PMI for all word pairs in a corpus. This code helps measure how strongly two words are associated compared to what you would expect by chance. Comparing Mutural Informaiton, WAPMI, VAE, and KL Divergence.Let’s break down Wappmi, mutual information, and how they relate to Variational Autoencoders (VAEs) and KL divergence in simple terms, like you’re five. Mutual InformationImagine you have two sets of toys, and you want to know how much one set tells you about the other. Mutual information is a way to measure how much knowing about one set of toys helps you predict what’s in the other set. If you always find a red truck when you find a blue car, that’s high mutual information. Wappmi (Weighted Average Prediction Pointwise Mutual Information)Now, Wappmi takes this idea of mutual information and looks at words in sentences. It asks, “How often do these words appear together, and is it more than just by chance?” It’s like seeing if certain toys always end up next to each other more than they would randomly. Variational Autoencoders (VAEs) and KL DivergenceImagine you have a machine that tries to guess which toys you might pull out of a box based on previous toys. VAEs are like that machine—they learn patterns to predict what comes next. The KL Divergence (Kullback-Leibler Divergence) is a way for the machine to measure how different its guesses (predictions) are from what actually happens. It helps the machine get better by adjusting its guesses to be more like what it observes. How They Relate Mutual Information: Helps understand how related different pieces of data are, like words in a sentence. Wappmi: Uses the idea of mutual information to find strong word pairings in text. VAE: Tries to model data (like sentences) to understand it better. KL Divergence: Helps the VAE improve its understanding by comparing its guesses to reality and adjusting accordingly. Example Code WalkthroughLet’s imagine you have a simple code where you’re trying to guess if two words often appear together: 1234567891011121314import collectionsdef get_stats(vocab): pairs = collections.defaultdict(int) for word, freq in vocab.items(): symbols = word.split() for i in range(len(symbols) - 1): pairs[symbols[i], symbols[i + 1]] += freq return pairsvocab = &#123;&#x27;new&#x27;: 2, &#x27;news&#x27;: 6, &#x27;newer&#x27;: 3&#125;pairs = get_stats(vocab)print(pairs) This code checks how often pairs of words (or symbols) appear together. If “new” and “news” show up together a lot, the program will tell you. This is similar to how Wappmi works. In a VAE, you might use something like KL divergence to measure how far off your model’s guesses are from reality, then adjust it to improve. The model might then get better at understanding the relationships between words (like mutual information). This approach helps in building smarter models that can understand and predict data more accurately. Simple code demonstration of each one of them1. Mutual InformationConcept:Mutual Information (MI) measures how much knowing one random variable reduces uncertainty about another. It quantifies the “shared information” between two variables. Code Example: 123456789from sklearn.metrics import mutual_info_score# Example dataX = [0, 0, 1, 1]Y = [0, 1, 0, 1]# Calculate mutual informationmi = mutual_info_score(X, Y)print(f&#x27;Mutual Information: &#123;mi&#125;&#x27;) Explanation:In this example, mutual_info_score from sklearn calculates the MI between two lists X and Y. MI quantifies how much knowing X helps predict Y. 2. WAPPMI (Weighted Average Pointwise Mutual Information)Concept:WAPPMI is used in NLP to find out how often words co-occur in a text corpus beyond what would be expected by chance. It gives more weight to frequently co-occurring pairs. Code Example: 12345678910111213141516171819import collectionsimport mathdef pmi(x, y, corpus): px = corpus.count(x) / len(corpus) py = corpus.count(y) / len(corpus) pxy = corpus.count(x + &#x27; &#x27; + y) / len(corpus) return math.log(pxy / (px * py), 2)corpus = &quot;this is a simple corpus with some simple words in this simple text&quot;pairs = collections.defaultdict(int)# Example calculationwords = corpus.split()for i in range(len(words) - 1): pairs[(words[i], words[i + 1])] += 1for pair, freq in pairs.items(): print(f&#x27;PMI(&#123;pair&#125;): &#123;pmi(pair[0], pair[1], corpus)&#125;&#x27;) Explanation:The PMI function calculates the pointwise mutual information between word pairs in the corpus. WAPPMI extends this by weighting these values. 3. Variational Autoencoder (VAE)Concept:VAEs are used to generate data that’s similar to a given dataset. They work by learning a probabilistic model of the data and then sampling from it. Code Example: 1234567891011121314151617181920212223242526272829import torchimport torch.nn as nnclass VAE(nn.Module): def __init__(self, input_dim, hidden_dim, z_dim): super(VAE, self).__init__() self.encoder = nn.Sequential( nn.Linear(input_dim, hidden_dim), nn.ReLU(), nn.Linear(hidden_dim, z_dim * 2) ) self.decoder = nn.Sequential( nn.Linear(z_dim, hidden_dim), nn.ReLU(), nn.Linear(hidden_dim, input_dim), nn.Sigmoid() ) def forward(self, x): mu_logvar = self.encoder(x) mu, logvar = mu_logvar.chunk(2, dim=-1) std = torch.exp(0.5 * logvar) z = mu + std * torch.randn_like(std) return self.decoder(z), mu, logvar# Instantiate and run the modelvae = VAE(input_dim=784, hidden_dim=400, z_dim=20)sample_input = torch.randn((64, 784))output, mu, logvar = vae(sample_input) Explanation:The VAE class in this example defines an encoder and decoder. The encoder outputs a mean (mu) and log-variance (logvar) for a latent variable z. The decoder reconstructs the input data from z. The KL divergence between the latent distribution and a normal distribution helps regularize the model. 4. KL DivergenceConcept:KL Divergence measures how one probability distribution diverges from a second, expected probability distribution. It’s used in VAEs to ensure that the latent variables follow a desired distribution (usually a Gaussian). Code Example: 1234567891011import torchimport torch.nn.functional as F# KL Divergence between the prior (standard normal) and the approximate posterior (q(z|x))def kl_divergence(mu, logvar): return -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())mu = torch.zeros(64, 20)logvar = torch.zeros(64, 20)kl = kl_divergence(mu, logvar)print(f&#x27;KL Divergence: &#123;kl&#125;&#x27;) Explanation:This code calculates the KL Divergence for a batch of latent variables with mu and logvar as parameters. It shows how much the approximate posterior (the learned distribution) diverges from the prior (standard normal distribution). Comparison: Mutual Information tells us how much knowing one variable helps us know another. WAPPMI applies this idea to words in text, showing how often they appear together. VAEs are generative models that learn to create data similar to a training set. They use KL Divergence to ensure the generated data follows a specific distribution. Overall, these concepts are connected through their use in understanding and modeling the relationships within data, whether through measuring information (MI, WAPPMI) or generating and adjusting data distributions (VAEs, KL Divergence).","categories":[{"name":"Math","slug":"Math","permalink":"https://shiyisteezin.github.io/nlp-docs/categories/Math/"}],"tags":[{"name":"nlp-theories","slug":"nlp-theories","permalink":"https://shiyisteezin.github.io/nlp-docs/tags/nlp-theories/"}]},{"title":"Variational Families","slug":"vae","date":"2024-07-27T13:16:00.000Z","updated":"2024-12-05T18:39:15.380Z","comments":true,"path":"2024/07/27/vae/","permalink":"https://shiyisteezin.github.io/nlp-docs/2024/07/27/vae/","excerpt":"","text":"Introducing The Gists of Variational Autoencoders (VAEs)What is a Variational Autoencoder (VAE)?Imagine you have a magical machine that can take a picture of your favorite toy, turn it into a secret code, and then use that code to recreate the toy’s picture. This is kind of what a Variational Autoencoder (VAE) does, but instead of toys, it works with things like images, sounds, or even words. A VAE is a type of artificial brain (or neural network) that learns to compress data (like a picture) into a simpler form and then uses that simple form to recreate the original data. The “variational” part means that it doesn’t just learn one way to represent the data, but many possible ways, which helps it be more flexible and creative. Math Foundations: Breaking it DownLet’s keep things simple. Imagine you have a bunch of colored balls, and you want to sort them by color. First, you need to pick a way to describe each color with a number. This number is like the “code” that the VAE creates. Then, once you have the code, you need to figure out how to recreate the exact color from the number. Encoder: This part of the VAE is like a friend who looks at the color of the ball and writes down a secret code (a number) that represents that color. Latent Space: This is like a secret storage area where all the codes are kept. It’s like a map where similar colors are stored close to each other. Decoder: This is like another friend who can look at the code and recreate the exact color of the ball. The tricky part is that the VAE learns to write codes in a way that they can be easily decoded back into the original color. Simple Math Example: If your color is described by a number, say $ x $, the VAE learns a new number, $ z $, that is simpler and can still be used to recreate $ x $. The VAE tries to make sure that: $$\\text{Original Data} \\approx \\text{Decoder}(\\text{Encoder}(\\text{Original Data}))$$ This means the data recreated by the VAE should be very close to the original data. Coding Example: A Simple VAE in PythonLet’s see a simple example using Python. We’ll use a small dataset of handwritten digits called MNIST. We want to teach our VAE to encode these digits into a simpler form and then recreate them. 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475import torchimport torch.nn as nnimport torch.optim as optimfrom torchvision import datasets, transforms# Define the encoderclass Encoder(nn.Module): def __init__(self): super(Encoder, self).__init__() self.fc1 = nn.Linear(784, 400) self.fc2_mu = nn.Linear(400, 20) self.fc2_logvar = nn.Linear(400, 20) def forward(self, x): h = torch.relu(self.fc1(x)) return self.fc2_mu(h), self.fc2_logvar(h)# Define the decoderclass Decoder(nn.Module): def __init__(self): super(Decoder, self).__init__() self.fc3 = nn.Linear(20, 400) self.fc4 = nn.Linear(400, 784) def forward(self, z): h = torch.relu(self.fc3(z)) return torch.sigmoid(self.fc4(h))# VAE Modelclass VAE(nn.Module): def __init__(self): super(VAE, self).__init__() self.encoder = Encoder() self.decoder = Decoder() def reparameterize(self, mu, logvar): std = torch.exp(0.5 * logvar) eps = torch.randn_like(std) return mu + eps * std def forward(self, x): mu, logvar = self.encoder(x.view(-1, 784)) z = self.reparameterize(mu, logvar) return self.decoder(z), mu, logvar# Loss function (how far off we are from the original)def loss_function(recon_x, x, mu, logvar): BCE = nn.functional.binary_cross_entropy(recon_x, x.view(-1, 784), reduction=&#x27;sum&#x27;) KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp()) return BCE + KLD# Training the VAEdef train(model, optimizer, train_loader): model.train() train_loss = 0 for batch_idx, (data, _) in enumerate(train_loader): optimizer.zero_grad() recon_batch, mu, logvar = model(data) loss = loss_function(recon_batch, data, mu, logvar) loss.backward() train_loss += loss.item() optimizer.step() return train_loss / len(train_loader.dataset)# Load datasettrain_loader = torch.utils.data.DataLoader( datasets.MNIST(&#x27;.&#x27;, train=True, download=True, transform=transforms.ToTensor()), batch_size=128, shuffle=True)vae = VAE()optimizer = optim.Adam(vae.parameters(), lr=1e-3)for epoch in range(1, 11): train_loss = train(vae, optimizer, train_loader) print(f&#x27;Epoch &#123;epoch&#125;, Loss: &#123;train_loss:.4f&#125;&#x27;) In this code: Encoder: Learns to compress the digit images into a simpler form. Decoder: Learns to recreate the digit images from this simpler form. Reparameterization: Adds a bit of randomness, helping the model learn multiple ways to recreate the images. Practical Example: How VAEs Can HelpImagine you’re building a tool to generate new images of cartoon characters. You don’t want just one version of a character, but many different versions. The VAE can learn from existing images and then create new, similar images by tweaking the codes stored in the latent space. For example, if you’ve trained a VAE on images of cats, you can generate new, never-before-seen images of cats by sampling from the latent space. More on The Stochasticity of VAE MethodsVariational Autoencoders (VAEs) are inherently stochastic in nature. This stochasticity is a key feature that differentiates VAEs from traditional deterministic autoencoders. Here’s how the stochastic nature of VAEs works: 1. Latent Space Representation:In a traditional autoencoder, the encoder deterministically maps an input $ x $ to a single point in a latent space $ z $. The decoder then maps this point $ z $ back to reconstruct the input $ x $. In contrast, in a VAE, the encoder doesn’t map $ x $ to a single point. Instead, it maps $ x $ to a probability distribution over the latent space. Typically, this is modeled as a multivariate Gaussian distribution with a mean vector $ \\mu $ and a variance (or log-variance) vector $ \\sigma^2 $. 2. Sampling from the Latent Space:To generate a latent variable $ z $ that will be passed to the decoder, VAEs sample from the distribution defined by $ \\mu $ and $ \\sigma^2 $. This sampling process introduces stochasticity into the VAE because even for the same input $ x $, different samples from the distribution will produce slightly different $ z $ values, leading to variations in the output. 3. Stochasticity and Learning:The stochastic nature of VAEs is crucial for the model’s ability to generate diverse outputs and to learn a well-distributed latent space. During training, the VAE learns to shape these distributions in the latent space so that they can generate realistic outputs even when new $ z $ values are sampled. 4. Reparameterization Trick:The reparameterization trick is a method used in VAEs to make the stochastic sampling differentiable, which is necessary for backpropagation. It involves expressing the sampling process as $ z &#x3D; \\mu + \\sigma \\times \\epsilon $, where $ \\epsilon $ is a random variable drawn from a standard normal distribution. This trick enables the VAE to learn $ \\mu $ and $ \\sigma $ while still incorporating stochasticity. In Summary:VAEs are stochastic because they incorporate random sampling within their latent space representation. This stochasticity is essential for the model’s ability to generate diverse and meaningful data from the learned latent space. The randomness allows the VAE to explore different potential reconstructions, making it a powerful generative model. A Variational Autoencoder is like a creative artist that learns to simplify complex things into a simpler form (like codes) and then use those codes to recreate or even generate new things. It’s powerful because it can learn many ways to represent the data, making it flexible and creative. By understanding the basic math, seeing some code, and applying it practically, you get a glimpse of how VAEs are helping machines to learn and create in innovative ways!","categories":[{"name":"Math","slug":"Math","permalink":"https://shiyisteezin.github.io/nlp-docs/categories/Math/"}],"tags":[{"name":"math","slug":"math","permalink":"https://shiyisteezin.github.io/nlp-docs/tags/math/"},{"name":"stochastic","slug":"stochastic","permalink":"https://shiyisteezin.github.io/nlp-docs/tags/stochastic/"}]},{"title":"Viterbi Algorithm","slug":"viterbi","date":"2024-07-23T21:43:00.000Z","updated":"2024-12-05T18:39:15.380Z","comments":true,"path":"2024/07/23/viterbi/","permalink":"https://shiyisteezin.github.io/nlp-docs/2024/07/23/viterbi/","excerpt":"","text":"This Blog Will Explain The Mechanism of The Viterbi AlgorithmIn this blog, we will introduce the Viterbi Algorithm explanation along with a Python code demonstration for a sequence prediction task. Viterbi Algorithm: Explanation and Code DemonstrationThe Viterbi Algorithm is a dynamic programming technique used to find the most probable sequence of hidden states in a Hidden Markov Model (HMM). It’s widely applied in sequence prediction tasks like speech recognition, natural language processing, and bioinformatics. In this post, we’ll not only break down the algorithm’s mechanism but also provide a practical Python code demonstration to predict hidden states based on observed data. Components of the Viterbi AlgorithmBefore diving into the algorithm, let’s review the core components of a Hidden Markov Model: States: These are the possible hidden states of the system, denoted as:$$S &#x3D; { S_1, S_2, \\dots, S_N }$$ Observations: The visible outputs from the system, denoted as:$$O &#x3D; { O_1, O_2, \\dots, O_T }$$ Transition Probabilities: The probability of transitioning from one state to another, represented as:$$a_{ij} &#x3D; P(S_j | S_i)$$ Emission Probabilities: The probability of observing a particular output given a state, denoted as:$$b_j(O_t) &#x3D; P(O_t | S_j)$$ Initial Probabilities: The probability of starting in a particular state:$$\\pi_i &#x3D; P(S_i | \\text{start})$$ Problem DefinitionThe goal of the Viterbi Algorithm is to find the most probable sequence of hidden states:$$S_1, S_2, \\dots, S_T$$given a sequence of observations:$$O_1, O_2, \\dots, O_T$$ Viterbi Algorithm Steps Initialization: Initialize the probabilities of the first observation for each state:$$\\delta_1(i) &#x3D; \\pi_i \\cdot b_i(O_1)$$ Recursion: For each time step $t &#x3D; 2, 3, \\dots, T$, compute the maximum probability for each state $S_j$:$$\\delta_t(j) &#x3D; \\max_i \\left( \\delta_{t-1}(i) \\cdot a_{ij} \\right) \\cdot b_j(O_t)$$Track the backpointers to reconstruct the path:$$\\psi_t(j) &#x3D; \\arg \\max_i \\left( \\delta_{t-1}(i) \\cdot a_{ij} \\right)$$ Termination: At the final time step $T$, select the state with the highest probability:$$S_T &#x3D; \\arg \\max_i \\delta_T(i)$$ Path Backtracking: Using the backpointers, trace back through the most probable path to recover the sequence of hidden states. Code Demonstration: Predicting Weather StatesLet’s consider an example where we predict the most likely weather conditions given observations of “Dry”, “Dryish”, and “Wet”. Step 1: Define the ProblemWe’ll use two hidden states: Sunny and Rainy, and three possible observations: Dry, Dryish, and Wet. States: Sunny, Rainy Observations: Dry, Dryish, Wet Transition Probabilities:$$a_{\\text{Sunny, Sunny}} &#x3D; 0.8, \\quad a_{\\text{Sunny, Rainy}} &#x3D; 0.2$$$$a_{\\text{Rainy, Sunny}} &#x3D; 0.4, \\quad a_{\\text{Rainy, Rainy}} &#x3D; 0.6$$ Emission Probabilities:$$b_{\\text{Sunny}}(\\text{Dry}) &#x3D; 0.6, \\quad b_{\\text{Sunny}}(\\text{Dryish}) &#x3D; 0.3, \\quad b_{\\text{Sunny}}(\\text{Wet}) &#x3D; 0.1$$$$b_{\\text{Rainy}}(\\text{Dry}) &#x3D; 0.1, \\quad b_{\\text{Rainy}}(\\text{Dryish}) &#x3D; 0.4, \\quad b_{\\text{Rainy}}(\\text{Wet}) &#x3D; 0.5$$ Initial Probabilities:$$\\pi_{\\text{Sunny}} &#x3D; 0.5, \\quad \\pi_{\\text{Rainy}} &#x3D; 0.5$$ Step 2: Python Implementation12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970import numpy as np# Define the states, observations, and sequencesstates = [&#x27;Sunny&#x27;, &#x27;Rainy&#x27;]observations = [&#x27;Dry&#x27;, &#x27;Dryish&#x27;, &#x27;Wet&#x27;] # the two lists don&#x27;t have to be the exact matchobs_sequence = [&#x27;Dry&#x27;, &#x27;Dryish&#x27;, &#x27;Wet&#x27;] # this is the sequence of results we want to use the algorithm to explain.# the likely outcome might be [&#x27;Sunny&#x27;, &#x27;Sunny&#x27; , &#x27;Rainy&#x27;].# Transition probabilitiestransition_probs = &#123; &#x27;Sunny&#x27;: &#123;&#x27;Sunny&#x27;: 0.8, &#x27;Rainy&#x27;: 0.2&#125;, &#x27;Rainy&#x27;: &#123;&#x27;Sunny&#x27;: 0.4, &#x27;Rainy&#x27;: 0.6&#125;&#125;# Emission probabilitiesemission_probs = &#123; &#x27;Sunny&#x27;: &#123;&#x27;Dry&#x27;: 0.6, &#x27;Dryish&#x27;: 0.3, &#x27;Wet&#x27;: 0.1&#125;, &#x27;Rainy&#x27;: &#123;&#x27;Dry&#x27;: 0.1, &#x27;Dryish&#x27;: 0.4, &#x27;Wet&#x27;: 0.5&#125;&#125;# Initial probabilitiesstart_probs = &#123;&#x27;Sunny&#x27;: 0.5, &#x27;Rainy&#x27;: 0.5&#125;# Viterbi Algorithm implementationdef viterbi(obs_sequence, states, start_probs, transition_probs, emission_probs): T = len(obs_sequence) N = len(states) # Initialize variables viterbi_matrix = np.zeros((N, T)) # Store probabilities backpointer = np.zeros((N, T), dtype=int) # Store backtracking paths # Mapping state names to indices state_index = &#123;states[i]: i for i in range(N)&#125; # Initialization step for s in range(N): viterbi_matrix[s, 0] = start_probs[states[s]] * emission_probs[states[s]][obs_sequence[0]] backpointer[s, 0] = 0 # No backpointer in the first column # Recursion step for t in range(1, T): for s in range(N): max_prob, max_state = max( (viterbi_matrix[prev_s, t-1] * transition_probs[states[prev_s]][states[s]], prev_s) for prev_s in range(N) ) viterbi_matrix[s, t] = max_prob * emission_probs[states[s]][obs_sequence[t]] backpointer[s, t] = max_state # Termination step best_path_prob = np.max(viterbi_matrix[:, T-1]) best_last_state = np.argmax(viterbi_matrix[:, T-1]) # Backtracking best_path = [best_last_state] for t in range(T-1, 0, -1): best_last_state = backpointer[best_last_state, t] best_path.insert(0, best_last_state) best_path_states = [states[state] for state in best_path] return best_path_states, best_path_prob# Run the Viterbi algorithmbest_path, best_prob = viterbi(obs_sequence, states, start_probs, transition_probs, emission_probs)print(&quot;Most likely hidden state sequence:&quot;, best_path)print(&quot;Probability of this sequence:&quot;, best_prob) Step 3: Output InterpretationRunning this code will yield: 12Most likely hidden state sequence: [&#x27;Sunny&#x27;, &#x27;Sunny&#x27;, &#x27;Rainy&#x27;]Probability of this sequence: 0.0144 This result indicates that, given the observations [&#39;Dry&#39;, &#39;Dryish&#39;, &#39;Wet&#39;], the most likely sequence of weather conditions is that it was Sunny, then Sunny again, and finally Rainy. Time ComplexityThe time complexity of the Viterbi Algorithm is $O(N^2 \\cdot T)$, where: $N$ is the number of states. $T$ is the number of observations. This efficiency makes it ideal for sequence prediction tasks, such as speech recognition, part-of-speech tagging, and bioinformatics. ConclusionThe Viterbi Algorithm efficiently finds the most probable sequence of hidden states in Hidden Markov Models. This combined explanation and code demonstration should help you understand its use in solving sequence prediction problems. Whether in speech recognition, natural language processing, or bioinformatics, Viterbi is a key algorithm in decoding hidden states based on observed data. Importance Q &amp; A Why do we need both the observations and observed sequence? In the Viterbi Algorithm, we need both observations (the set of all possible outcomes) and obs_sequence (the specific sequence of observations) for a few key reasons. Let’s break it down simply: Why We Need observations (All Possible Outcomes): Defining the Model: The list of possible observations (observations) helps to define the entire problem space. We need to know what could potentially happen in the system (e.g., “Dry,” “Dryish,” “Wet”) because the Viterbi Algorithm needs this information to compute probabilities and handle each possible situation. When you calculate emission probabilities (the likelihood of seeing an observation given a hidden state), you need to refer to all the possible observations to assign these probabilities. Mapping Probabilities: The emission probabilities are assigned based on the observations. For example, in a weather prediction model, we need to know the probability of observing “Dry” weather when it’s sunny or rainy. Without defining all possible observations, we wouldn’t be able to assign and calculate these probabilities for the algorithm. Why We Need obs_sequence (The Actual Observed Sequence): What We’re Trying to Solve: The obs_sequence represents the actual sequence of observations we’re trying to explain with the Viterbi Algorithm. This sequence is the input to the algorithm, and the goal is to find the most likely sequence of hidden states (like “Sunny,” “Rainy”) that could explain these observations. For example, if you see the sequence [“Dry”, “Dryish”, “Wet”], the algorithm will work to find the most probable hidden state sequence (like “Sunny, Sunny, Rainy”) that led to these observed outcomes. Step-by-Step Processing: The Viterbi Algorithm works step by step through this specific obs_sequence to calculate the probabilities of moving through different hidden states. Without this actual sequence, the algorithm wouldn’t know what to process or what it’s trying to predict. Summary: observations: Lists all the possible things you could observe, which is important for defining the model and assigning probabilities. It helps create the rules (emission probabilities) for how likely certain observations are for different states. obs_sequence: This is the specific sequence of observations you’ve seen, and the Viterbi Algorithm uses it to calculate and find the hidden states that best explain what happened. In short, observations set the stage (the possible things that could happen), and obs_sequence gives the actual events (what did happen) that the Viterbi Algorithm needs to explain. Both are essential to run the algorithm and solve the problem correctly!","categories":[{"name":"Math","slug":"Math","permalink":"https://shiyisteezin.github.io/nlp-docs/categories/Math/"}],"tags":[{"name":"math","slug":"math","permalink":"https://shiyisteezin.github.io/nlp-docs/tags/math/"}]},{"title":"Jacobian Matrices","slug":"jacobian-matrices","date":"2024-07-17T17:47:00.000Z","updated":"2024-12-05T18:39:15.373Z","comments":true,"path":"2024/07/17/jacobian-matrices/","permalink":"https://shiyisteezin.github.io/nlp-docs/2024/07/17/jacobian-matrices/","excerpt":"","text":"Discussions on Jacobian Matrices ContinuedThis blog will break down and continue explaining Jacobian matrices and Taylor expansions in plain language and explore how they are connected. Jacobian MatrixWhat it is: Imagine you have a function that takes multiple inputs and gives multiple outputs. For example, you might have a function that takes two numbers (like coordinates $x$ and $y$) and gives back two other numbers. The Jacobian matrix is a way to capture how small changes in each input affect each output. How it works: Suppose you have a function $f(x, y)$ that gives outputs $u$ and $v$. The Jacobian matrix for this function is like a grid that shows how $u$ and $v$ change when $x$ and $y$ change. Mathematically, it’s a 2x2 matrix (in this case) where each entry is a partial derivative. It looks like this: $$ \\text{Jacobian} = \\begin{pmatrix} \\frac{\\partial u}{\\partial x} & \\frac{\\partial u}{\\partial y} \\\\ \\frac{\\partial v}{\\partial x} & \\frac{\\partial v}{\\partial y} \\end{pmatrix} $$ What it tells you: Each entry in the Jacobian matrix tells you how one output changes with respect to one input. For instance, $\\frac{\\partial u}{\\partial x}$ tells you how $u$ changes when you make a tiny change in $x$. Taylor ExpansionWhat it is: The Taylor expansion is a way to approximate a complex function using simpler polynomial terms. Think of it as breaking down a complicated function into a sum of easy-to-handle pieces. How it works: Suppose you have a function $f(x)$ and you want to approximate it near a point $a$ . The Taylor expansion uses the value of the function at $a$ and its derivatives (rates of change) at $a$ to build this approximation. The formula for the Taylor expansion up to the first few terms looks like this: $$ f(x) \\approx f(a) + f’(a)(x-a) + \\frac{f’’(a)}{2!}(x-a)^2 + \\cdots $$ What it tells you: The first term $f(a)$ is the function’s value at $a$ . The second term $f’(a)(x-a)$ shows how the function changes linearly around $a$ . The higher-order terms $\\frac{f’’(a)}{2!}(x-a)^2$ , etc., show more complex changes (like curvature). Connection Between Jacobian Matrix and Taylor ExpansionHow they are connected: When you use the Taylor expansion for functions with multiple inputs and outputs, the Jacobian matrix comes into play. For a function with multiple variables, the first-order Taylor expansion looks like this: $$ f(\\mathbf{x}) \\approx f(\\mathbf{a}) + J(\\mathbf{a})(\\mathbf{x} - \\mathbf{a}) $$ where $\\mathbf{x}$ and $\\mathbf{a}$ are vectors (like coordinates), and $J(\\mathbf{a})$ is the Jacobian matrix at $\\mathbf{a}$. What this means: The Jacobian matrix $J(\\mathbf{a})$ captures how the function changes in all directions from the point $\\mathbf{a}$. The term $J(\\mathbf{a})(\\mathbf{x} - \\mathbf{a})$ is like a multi-dimensional linear approximation, showing how small changes in inputs affect the outputs. In summary, the Jacobian matrix gives you a snapshot of how changes in inputs affect outputs for functions with multiple variables. The Taylor expansion uses this information (and higher-order derivatives) to build an approximation of the function near a specific point.","categories":[{"name":"Math","slug":"Math","permalink":"https://shiyisteezin.github.io/nlp-docs/categories/Math/"}],"tags":[{"name":"math","slug":"math","permalink":"https://shiyisteezin.github.io/nlp-docs/tags/math/"},{"name":"determinism","slug":"determinism","permalink":"https://shiyisteezin.github.io/nlp-docs/tags/determinism/"}]},{"title":"Problem Solving","slug":"problem-solving","date":"2024-07-17T13:27:00.000Z","updated":"2024-12-05T18:39:15.377Z","comments":true,"path":"2024/07/17/problem-solving/","permalink":"https://shiyisteezin.github.io/nlp-docs/2024/07/17/problem-solving/","excerpt":"","text":"More on Logic And Problem SolvingIn a different blog, I have briefly introduced some of the most important concepts of logic and problem solving, including but not limited to predicate calculus, propositional logic, and lambda calculus. In this blog, the notes will be more in detail and introduce relevant ideas. Defining Entailment, Implicatures, and PresuppositionsImplicatures: What’s suggested in an utterance, even though it is not explicitly stated or entailed by the utterance. Entailment: Entailment is a relationship between statements where one statement necessarily follows from another. If statement A entails statement B, then if A is true, B must also be true. Presuppositions: A presupposition is an assumption that as speaker makes about what the listener already knows or believes to be true. It’s information taken for granted in the utterance. Differences Between The Above ThreeLet’s define implicatures, entailments, and presuppositions and compare their differences in simple terms Nature of Meaning: Implicature: Implied meaning that relies on context and shared understanding. It is not directly stated but inferred. Entailment: Logical meaning that follows necessarily from the truth of another statement. It is a strict logical relationship. Presupposition: Assumed background information or beliefs. It is taken for granted by the speaker as known to the listener. Dependence on Context: Implicature: Highly dependent on context. The same statement can imply different things in different situations. Entailment: Not dependent on context. If the antecedent is true, the consequent must be true regardless of context. Presupposition: Partially dependent on context. It relies on shared knowledge but is less flexible than implicature. Truth Conditions: Implicature: Can be canceled or denied without contradiction. For example, “It’s cold in here” doesn’t necessarily mean the speaker wants the window closed if they follow up with, “But I like it that way.” Entailment: Cannot be canceled without contradiction. If “All cats are animals” is true, saying “No cats are animals” would be a direct contradiction. Presupposition: Remains even if the statement is negated. For example, “John’s brother is not tall” still presupposes that John has a brother. Example Comparisons: Implicature: Statement: “Can you pass the salt?” Implicature: The speaker is asking you to pass the salt, not questioning your ability to do so. Entailment: Statement: “She is a mother.” Entailment: She has a child. If “She is a mother” is true, it logically follows that “She has a child” must be true. Presupposition: Statement: “The king of France is bald.” Presupposition: There is a king of France. This assumption is taken for granted by the speaker. SummaryImplicatures are implied meanings that depend on context and can be canceled without contradiction.Entailments are logical consequences that must be true if the initial statement is true, and they cannot be canceled without contradiction.Presuppositions are background assumptions that remain true even if the statement is negated and depend on shared knowledge between the speaker and listener.By understanding these differences, we can better analyze and interpret the subtleties of communication and language use. The Gists of Predicate Calculus and Propositional LogicBasic Concepts of Lambda CalculusLambda calculus is a formal system in mathematical logic and computer science for expressing computation based on function abstraction and application. It was introduced by Alonzo Church in the 1930s as part of his work on the foundations of mathematics. 1. Lambda Abstraction: Syntax: λx. E Explanation: This denotes an anonymous function with a parameter x and body E. For example, λx. x + 1 represents a function that takes an argument x and returns x + 1. 2. Application: Syntax: (F A) Explanation: This denotes the application of function F to argument A. For example, (λx. x + 1) 2 applies the function λx. x + 1 to 2, resulting in 3. 3. Variables: Syntax: x Explanation: Variables are placeholders for values or other expressions. In λx. x, x is a variable. ExpressionsIn lambda calculus, expressions are built using variables, lambda abstractions, and applications. These are called lambda expressions or terms. The grammar of lambda expressions is defined as: Variables: x, y, z, ... Lambda Abstraction: λx. E where x is a variable and E is a lambda expression. Application: (E1 E2) where E1 and E2 are lambda expressions. ReductionLambda calculus defines computation through the process of reduction, which simplifies lambda expressions. There are two main types of reduction: Alpha Conversion (α conversion): Explanation: Renaming the bound variables in a lambda expression. For example, λx. x can be alpha-converted to λy. y. Purpose: Avoids name collisions. Beta Reduction (β reduction): Explanation: Applying a lambda function to an argument. For example, (λx. x + 1) 2 reduces to 2 + 1 which further reduces to 3. Process: Replace the bound variable with the argument in the body of the abstraction. (λx. E1) E2 reduces to E1[E2/x], where E1[E2/x] denotes substitution of E2 for x in E1. Example of Beta ReductionConsider the expression: (λx. (λy. x + y) 2) 3. Apply the outer function (λx. (λy. x + y) 2) to 3: ((λx. (λy. x + y) 2) 3) Substitute 3 for x in (λy. x + y) 2: (λy. 3 + y) 2. Apply the inner function (λy. 3 + y) to 2: ((λy. 3 + y) 2) Substitute 2 for y in 3 + y: 3 + 2. Simplify the expression: 3 + 2 reduces to 5. Significance in Computer ScienceLambda calculus serves as the foundation for understanding computation and functional programming languages. Key aspects include: Functional Programming: Languages like Haskell, Lisp, and Scheme are heavily influenced by lambda calculus. Programming Language Theory: Lambda calculus provides a framework for studying the properties of functions and recursive definitions. Type Systems: Extensions of lambda calculus, such as the simply typed lambda calculus, form the basis for type systems in programming languages. Church-Turing (Alonzo Church and Alan Turing) ThesisLambda calculus is also central to the Church-Turing thesis, which posits that any computable function can be computed by a Turing machine, and equivalently, can be expressed in lambda calculus. This establishes lambda calculus as a universal model of computation. In summary, lambda calculus is a powerful mathematical formalism for defining and studying computation. Its simplicity and expressiveness make it a cornerstone of theoretical computer science and a valuable tool for understanding the base of programming languages. In A NutshellThe contributions of Alonzo Church and Alan Turing, particularly their work on the Church-Turing thesis and the development of lambda calculus and Turing machines, can be discussed under the broader umbrella of problem-solving and logical reasoning. Their ideas fit well within the same conceptual framework as traditional syllogistic logic, such as propositional logic and predicate logic, since all of these approaches are aimed at formalizing methods of reasoning and solving problems in structured, logical ways.","categories":[{"name":"Linguistics","slug":"Linguistics","permalink":"https://shiyisteezin.github.io/nlp-docs/categories/Linguistics/"}],"tags":[{"name":"semantics","slug":"semantics","permalink":"https://shiyisteezin.github.io/nlp-docs/tags/semantics/"}]},{"title":"Markov Processes","slug":"markov-chains","date":"2024-07-16T12:13:20.000Z","updated":"2024-12-05T18:39:15.374Z","comments":true,"path":"2024/07/16/markov-chains/","permalink":"https://shiyisteezin.github.io/nlp-docs/2024/07/16/markov-chains/","excerpt":"","text":"Discussions on Markov Processes ContinuedIn a different blog, I noted the use of a markov processes in the context of natural language processing. Now in this blog, we will be going through some important details with regard to the concept. We will go through some code in the subsequent paragraph with respect to how to simulate Markov Chain in coding. Markov Chain BasicsA Markov chain is a mathematical system that undergoes transitions from one state to another within a finite or countable number of states. It is a stochastic process that satisfies the Markov property, which states that the future state depends only on the current state and not on the sequence of events that preceded it. Components of a Markov Chain States: The different possible conditions or configurations the system can be in. Let’s denote the set of all states as $ S &#x3D; { s_1, s_2, \\ldots, s_n } $. Transition Probabilities: The probabilities of moving from one state to another. Denoted by $ P_{ij} &#x3D; P(X_{t+1} &#x3D; s_j \\mid X_t &#x3D; s_i) $, where $ P_{ij} $ is the probability of transitioning from state $ s_i $ to state $ s_j $. Transition Matrix: A matrix $ P $ where each element $ P_{ij} $ represents the transition probability from state $ s_i $ to state $ s_j $.$$ P &#x3D; \\begin{pmatrix} P_{11} &amp; P_{12} &amp; \\cdots &amp; P_{1n} \\cr P_{21} &amp; P_{22} &amp; \\cdots &amp; P_{2n} \\cr \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\cr P_{n1} &amp; P_{n2} &amp; \\cdots &amp; P_{nn} \\end{pmatrix}$$ Markov PropertyThe Markov property states that the probability of transitioning to the next state depends only on the current state and not on the past states:$$ P(X_{t+1} &#x3D; s_j \\mid X_t &#x3D; s_i, X_{t-1} &#x3D; s_{i-1}, \\ldots, X_0 &#x3D; s_0) $$$$ \\qquad \\quad &#x3D; P(X_{t+1} &#x3D; s_j \\mid X_t &#x3D; s_i) $$ Step-by-Step ProcessLet’s go through the process of a Markov chain step by step. Step 1: Define the StatesIdentify all possible states of the system. Suppose we have a simple weather system with three states: $ s_1 $: Sunny $ s_2 $: Cloudy $ s_3 $: Rainy Step 2: Define the Transition ProbabilitiesDetermine the probabilities of moving from one state to another. For example, the transition probabilities might be: $ P_{11} &#x3D; 0.7 $ (probability of sunny to sunny) $ P_{12} &#x3D; 0.2 $ (probability of sunny to cloudy) $ P_{13} &#x3D; 0.1 $ (probability of sunny to rainy) $ P_{21} &#x3D; 0.3 $ (probability of cloudy to sunny) $ P_{22} &#x3D; 0.4 $ (probability of cloudy to cloudy) $ P_{23} &#x3D; 0.3 $ (probability of cloudy to rainy) $ P_{31} &#x3D; 0.2 $ (probability of rainy to sunny) $ P_{32} &#x3D; 0.3 $ (probability of rainy to cloudy) $ P_{33} &#x3D; 0.5 $ (probability of rainy to rainy) These can be represented in the transition matrix $ P $: $$ P &#x3D; \\begin{pmatrix} 0.7 &amp; 0.2 &amp; 0.1 \\cr 0.3 &amp; 0.4 &amp; 0.3 \\cr 0.2 &amp; 0.3 &amp; 0.5\\end{pmatrix} $$ Step 3: Initial State DistributionDefine the initial state distribution vector $ \\pi $, which represents the probability distribution of starting in each state. For example, if we start with a 100% chance of it being sunny: $$ \\pi &#x3D; \\begin{pmatrix} 1 \\cr 0 \\cr 0\\end{pmatrix} $$ Step 4: State PredictionTo predict the state distribution after one step, multiply the initial state distribution vector $ \\pi $ by the transition matrix $ P $: $$ \\pi^{(1)} &#x3D; \\pi P $$ $$ \\pi^{(1)} &#x3D; \\begin{pmatrix} 1 &amp; 0 &amp; 0\\end{pmatrix} \\begin{pmatrix} 0.7 &amp; 0.2 &amp; 0.1 \\cr 0.3 &amp; 0.4 &amp; 0.3 \\cr 0.2 &amp; 0.3 &amp; 0.5\\end{pmatrix} $$ $$ \\pi^{(1)} &#x3D; \\begin{pmatrix} 0.7 &amp; 0.2 &amp; 0.1\\end{pmatrix} $$ This tells us that after one step, there’s a 70% chance of it being sunny, a 20% chance of it being cloudy, and a 10% chance of it being rainy. Step 5: Long-Term BehaviorTo find the steady-state distribution (long-term behavior), solve for $ \\pi $ in the equation:$ \\pi P &#x3D; \\pi $This often involves solving a system of linear equations. The steady-state distribution is the vector $ \\pi $ that remains unchanged after application of the transition matrix $ P $. Example CalculationIf we continue the prediction for multiple steps, we can see how the state distribution evolves over time. For example, after two steps: $$ \\pi^{(2)} &#x3D; \\pi^{(1)} P $$ $$ \\pi^{(2)} &#x3D; \\begin{pmatrix} 0.7 &amp; 0.2 &amp; 0.1\\end{pmatrix} \\begin{pmatrix} 0.7 &amp; 0.2 &amp; 0.1 \\cr 0.3 &amp; 0.4 &amp; 0.3 \\cr 0.2 &amp; 0.3 &amp; 0.5\\end{pmatrix} $$ $$ \\pi^{(2)} &#x3D; \\begin{pmatrix} 0.7 \\cdot 0.7 + 0.2 \\cdot 0.3 + 0.1 \\cdot 0.2 \\cr 0.7 \\cdot 0.2 + 0.2 \\cdot 0.4 + 0.1 \\cdot 0.3 \\cr 0.7 \\cdot 0.1 + 0.2 \\cdot 0.3 + 0.1 \\cdot 0.5\\end{pmatrix} $$ $$ \\pi^{(2)} &#x3D; \\begin{pmatrix} 0.53 &amp; 0.26 &amp; 0.21\\end{pmatrix} $$ So after two steps, there’s a 53% chance of it being sunny, a 26% chance of it being cloudy, and a 21% chance of it being rainy. SummaryA Markov chain is a powerful tool for modeling stochastic processes where the next state depends only on the current state. The key components include states, transition probabilities, and the transition matrix. The process involves defining the states and transition probabilities, computing state predictions, and analyzing long-term behavior through steady-state distributions. In this example, we define a transition matrix P for a 3-state Markov process. We then define a function simulate_markov that takes the transition matrix, the number of states, and the number of steps to simulate as input, and returns a list of the system’s states at each time step. The function initializes the state vector to all zeros, with a 1 in the first position to indicate that the system starts in state 0. It then simulates the Markov process by iteratively selecting the next state based on the current state and the transition probabilities. The current state is added to a history list at each time step. Finally, we simulate the Markov process for 100 steps and print the resulting state history. In What Scenarios Is Markov Chain Applicable?Simulating all these processes using Markov processes can be quite extensive. However, I can provide a basic framework and example for a few of these applications, demonstrating how Markov processes can be applied. We will use Python and some common libraries such as NumPy for these simulations. This section breaks down a simple example of how to build a simple markov chain in code example. Sure! Here’s an updated version of all the examples where we retain the calculated state vector (probability distribution) between steps, avoiding the unnecessary reset to a one-hot encoded vector. Updated Example 1: Modeling Stock Prices with Dot Product12345678910111213141516171819202122232425262728293031323334353637383940414243import numpy as npimport matplotlib.pyplot as plt# Define states (price levels)states = np.array([90, 100, 110, 120])n_states = len(states)# Define the transition matrixP = np.array([ [0.7, 0.2, 0.1, 0.0], # From 90 [0.1, 0.7, 0.2, 0.0], # From 100 [0.0, 0.2, 0.7, 0.1], # From 110 [0.0, 0.1, 0.2, 0.7], # From 120])# Initial state (one-hot vector for state 100)state_vector = np.zeros(n_states)state_vector[1] = 1 # Start in state 100# Number of steps to simulaten_steps = 100# Track price historyprice_history = [states[np.argmax(state_vector)]] # Start price historyfor _ in range(n_steps): # Perform dot product to get the next state probabilities state_vector = np.dot(state_vector, P) # Choose the next state based on the resulting probabilities next_state = np.random.choice(n_states, p=state_vector) # Store the price history for the next state price_history.append(states[next_state])# Plot the resultsplt.figure(figsize=(10, 6))plt.plot(price_history)plt.title(&#x27;Simulated Stock Prices Using Markov Process with Dot Product&#x27;)plt.xlabel(&#x27;Time Steps&#x27;)plt.ylabel(&#x27;Price&#x27;)plt.grid(True)plt.show() Updated Example 2: Disease Progression with Dot Product123456789101112131415161718192021222324252627282930313233343536import numpy as np# Define states (health conditions)states = [&quot;Healthy&quot;, &quot;Sick&quot;, &quot;Recovered&quot;]n_states = len(states)# Define the transition matrixP = np.array([ [0.85, 0.10, 0.05], # From Healthy [0.15, 0.70, 0.15], # From Sick [0.05, 0.10, 0.85], # From Recovered])# Initial state (one-hot vector for Healthy)state_vector = np.zeros(n_states)state_vector[0] = 1 # Start as Healthy# Number of steps to simulaten_steps = 50# Track health historyhealth_history = [states[np.argmax(state_vector)]]for _ in range(n_steps): # Perform dot product to get the next state probabilities state_vector = np.dot(state_vector, P) # Choose the next state based on the resulting probabilities next_state = np.random.choice(n_states, p=state_vector) # Store the health history health_history.append(states[next_state])# Print the resultsprint(&quot;Health Condition Over Time:&quot;)print(&quot; -&gt; &quot;.join(health_history)) Updated Example 3: Queueing Systems with Dot Product123456789101112131415161718192021222324252627282930313233343536373839404142434445import numpy as npimport matplotlib.pyplot as plt# Define states (number of customers in queue)states = [0, 1, 2, 3, 4, 5] # Queue capacity is 5n_states = len(states)# Define the transition matrixP = np.array([ [0.1, 0.9, 0.0, 0.0, 0.0, 0.0], # From 0 customers [0.5, 0.4, 0.1, 0.0, 0.0, 0.0], # From 1 customer [0.2, 0.5, 0.3, 0.0, 0.0, 0.0], # From 2 customers [0.0, 0.3, 0.5, 0.2, 0.0, 0.0], # From 3 customers [0.0, 0.0, 0.4, 0.5, 0.1, 0.0], # From 4 customers [0.0, 0.0, 0.0, 0.5, 0.4, 0.1], # From 5 customers])# Initial state (one-hot vector for an empty queue)state_vector = np.zeros(n_states)state_vector[0] = 1 # Start with empty queue# Number of steps to simulaten_steps = 50# Track queue historyqueue_history = [states[np.argmax(state_vector)]]for _ in range(n_steps): # Perform dot product to get the next state probabilities state_vector = np.dot(state_vector, P) # Choose the next state based on the resulting probabilities next_state = np.random.choice(n_states, p=state_vector) # Store the queue history queue_history.append(states[next_state])# Plot the resultsplt.figure(figsize=(10, 6))plt.plot(queue_history)plt.title(&#x27;Simulated Queue Length Using Markov Process with Dot Product&#x27;)plt.xlabel(&#x27;Time Steps&#x27;)plt.ylabel(&#x27;Number of Customers in Queue&#x27;)plt.grid(True)plt.show() Updated Example 4: Customer Loyalty with Dot Product123456789101112131415161718192021222324252627282930313233343536import numpy as np# Define states (customer states)states = [&quot;Active&quot;, &quot;Inactive&quot;, &quot;Loyal&quot;]n_states = len(states)# Define the transition matrixP = np.array([ [0.7, 0.2, 0.1], # From Active [0.3, 0.6, 0.1], # From Inactive [0.1, 0.2, 0.7], # From Loyal])# Initial state (one-hot vector for Active)state_vector = np.zeros(n_states)state_vector[0] = 1 # Start as Active# Number of steps to simulaten_steps = 50# Track loyalty historyloyalty_history = [states[np.argmax(state_vector)]]for _ in range(n_steps): # Perform dot product to get the next state probabilities state_vector = np.dot(state_vector, P) # Choose the next state based on the resulting probabilities next_state = np.random.choice(n_states, p=state_vector) # Store the loyalty history loyalty_history.append(states[next_state])# Print the resultsprint(&quot;Customer Loyalty Over Time:&quot;)print(&quot; -&gt; &quot;.join(loyalty_history)) SummaryThis framework can be extended to simulate other processes like economic forecasting, pharmacokinetics, network protocols, etc. Each simulation contains these properties: Number Concept Description 1. States A system can exist in different states, representing distinct configurations or conditions. Denoted by symbols, numbers, or labels. 2. Transition Probabilities Markov processes are characterized by transition probabilities, which determine the likelihood of moving from one state to another in the next time step. These probabilities are often organized into a transition probability matrix. 3. Transition Probability Matrix A square matrix where each element represents the probability of transitioning from one state to another. Rows correspond to the current state, and columns correspond to the next state. 4. Markov Property The key feature of Markov processes is the Markov property, stating that the future evolution of the system depends only on its current state and is independent of how the system reached its current state. 5. Homogeneity Markov processes are often assumed to be homogeneous, meaning that transition probabilities do not change over time. The system’s dynamics are consistent throughout. 6. Continuous and Discrete Time Markov processes can be classified into continuous-time and discrete-time processes based on whether the state transitions occur at every instant or at discrete time intervals. 7. Stationary Distribution In a steady state, the system may reach a stationary distribution, where the probabilities of being in each state remain constant over time. 8. Absorbing and Transient States Some states may be absorbing, meaning that once entered, the system stays in that state permanently. Transient states are those from which the system may leave and not return. 9. Applications Markov processes find applications in various fields, including physics, economics, biology, and computer science, for modeling dynamic systems with probabilistic transitions. 10. Markov Chain A specific type of Markov process where the state space is discrete and the time parameter takes on discrete values. This basic approach can be adapted and extended to suit the specific characteristics and requirements of each application. Markov ChainA Markov chain is a specific type of Markov process that deals with discrete states and discrete time steps. It is characterized by the following: Discrete State Space: The set of possible states $ S &#x3D; {s_1, s_2, \\ldots, s_n} $ is finite or countable. Discrete Time Steps: The process evolves in discrete time steps $ t &#x3D; 0, 1, 2, \\ldots $. Markov Property: The probability of transitioning to the next state depends only on the current state and not on the sequence of events that preceded it. Formal DefinitionA Markov chain is defined by: A set of states $ S $. A transition probability matrix $ P $, where $ P_{ij} $ represents the probability of moving from state $ s_i $ to state $ s_j $. The full formula looks like below. $$ P(X_{t+1} &#x3D; s_j \\mid X_t &#x3D; s_i) &#x3D; P_{ij} $$ Markov ProcessA Markov process is a more general concept that includes both discrete and continuous cases. It is characterized by: State Space: The set of possible states can be discrete (finite or countable) or continuous. Time Steps: The process can evolve in either discrete time steps (like in a Markov chain) or continuous time. Markov Property: Similar to the Markov chain, the future state depends only on the current state and not on past states. Types of Markov Processes Discrete-Time Markov Process (Markov Chain): As described above, it deals with discrete states and discrete time steps. Continuous-Time Markov Process: The state space can be discrete or continuous. The process evolves continuously over time. Transition probabilities are often described using a rate matrix (or generator matrix) instead of a transition matrix. Continuous-Time Markov Chain (CTMC) A CTMC is a specific type of Markov process where: · The state space is discrete.· Time is continuous.· The transitions are governed by rates, often described using a rate matrix $ Q $. Summary of Differences State Space: Markov Chain: Discrete state space. Markov Process: Can be discrete or continuous state space. Time: Markov Chain: Discrete time steps. Markov Process: Can be discrete or continuous time. Transition Mechanism: Markov Chain: Defined by a transition probability matrix. Markov Process: Defined by transition probabilities for discrete time or transition rates for continuous time. Markov Chain (Discrete-Time, Discrete State)Consider a simple weather model with three states: Sunny, Cloudy, and Rainy. The transitions are defined for each day (discrete time steps). States: $ S &#x3D; { \\text{Sunny}, \\text{Cloudy}, \\text{Rainy} } $ Transition Matrix $ P $: $$P &#x3D; \\begin{pmatrix}0.7 &amp; 0.2 &amp; 0.1 \\cr0.3 &amp; 0.4 &amp; 0.3 \\cr0.2 &amp; 0.3 &amp; 0.5\\end{pmatrix}$$ Continuous-Time Markov Process (Continuous-Time, Discrete State)Consider a population model where individuals can be in different health states: Healthy, Sick, and Recovered. The transitions happen continuously over time, with certain rates. States: $ S &#x3D; { \\text{Healthy}, \\text{Sick}, \\text{Recovered} } $ Rate Matrix $ Q $: $$Q &#x3D; \\begin{pmatrix}-\\lambda &amp; \\lambda &amp; 0 \\cr0 &amp; -\\mu &amp; \\mu \\cr0 &amp; 0 &amp; 0\\end{pmatrix}$$ where $\\lambda$ is the rate of getting sick, and $\\mu$ is the rate of recovery. In summary, a Markov chain is a special case of a Markov process with discrete states and discrete time steps, whereas a Markov process can have a broader definition, encompassing both discrete and continuous states and time.","categories":[{"name":"Math","slug":"Math","permalink":"https://shiyisteezin.github.io/nlp-docs/categories/Math/"}],"tags":[{"name":"math","slug":"math","permalink":"https://shiyisteezin.github.io/nlp-docs/tags/math/"},{"name":"stochastic","slug":"stochastic","permalink":"https://shiyisteezin.github.io/nlp-docs/tags/stochastic/"}]},{"title":"Auto Differentiation","slug":"audo-diff","date":"2024-07-14T17:40:00.000Z","updated":"2024-12-05T18:39:15.371Z","comments":true,"path":"2024/07/14/audo-diff/","permalink":"https://shiyisteezin.github.io/nlp-docs/2024/07/14/audo-diff/","excerpt":"","text":"Intro to Automatic DifferentiationIn this blog, we will go through the foundations behind Automatic Differentiation. The Breakdown of The Video Into Parts var player; // This function is called by the YouTube IFrame API when it's ready function onYouTubeIframeAPIReady() { player = new YT.Player('iframe-yt-video', { events: { 'onReady': onPlayerReady } }); } // This function is called when the player is ready function onPlayerReady(event) { console.log(\"YouTube Player is ready\"); // Attach event listeners to the buttons document.querySelectorAll('.btn-timestamp').forEach(function(button) { button.addEventListener('click', function() { var seconds = parseInt(this.getAttribute('data-seconds'), 10); seekTo(seconds); }); }); } // Function to seek to the specified time in the video function seekTo(seconds) { if (player && typeof player.seekTo === 'function') { player.pauseVideo(); // Pause the video first player.seekTo(seconds, true); // Seek to the specified time setTimeout(function() { player.playVideo(); // Attempt to play after a short delay }, 500); // Adjust delay as needed console.log(\"Seeking to:\", seconds, \"seconds\"); } else { console.log(\"Player is not ready yet\"); } } Introduction Topic 1 - Diff in ML Topic 2 - Numerical Diff Topic 2 - Gradient Checking Topic 3 - Symbolic Diff Topic 4 - Computational Graphs Topic 5 - Forward Automatic Diff Topic 6 - Reverse Auto Diff Topic 6 - Reverse AD Algorithm Topic 7 - RAD vs Backprop Important Transcripts of the Video","categories":[{"name":"Math","slug":"Math","permalink":"https://shiyisteezin.github.io/nlp-docs/categories/Math/"}],"tags":[{"name":"math","slug":"math","permalink":"https://shiyisteezin.github.io/nlp-docs/tags/math/"}]},{"title":"Intro & Overview","slug":"place-holder","date":"2024-07-11T21:43:00.000Z","updated":"2024-12-05T18:39:15.377Z","comments":true,"path":"2024/07/11/place-holder/","permalink":"https://shiyisteezin.github.io/nlp-docs/2024/07/11/place-holder/","excerpt":"","text":"Hello There! Welcome To This Blog.I’m Shiyi. I’m deeply passionate about the intricate dance between data and innovation. With a fervent zeal for leveraging technology to extract insights and create a meaningful impact, I’ve embarked on a journey that spans the realms of Data Science, research, and creative expression. Q What drives my work in Data Science and Machine Learning? A At the heart of my endeavors lies a profound appreciation for machine learning, deep learning, and Natural Language Processing. As an advocate for data-driven decision-making, I thrive on unraveling the complexities of algorithms and patterns, harnessing their power to transform raw data into actionable intelligence. From predictive modeling in finance to image recognition tasks using deep learning architectures, I relish the challenge of pushing the boundaries of what’s possible with data. Q What is my expertise in Natural Language Processing? A My expertise extends to the captivating domain of Natural Language Processing. In an era inundated with information, I’m committed to empowering systems to understand, analyze, and generate insights from vast textual data. Whether it’s sentiment analysis to decipher the mood of social media conversations or language translation to bridge communication gaps, I’m fascinated by the potential of NLP to revolutionize how we interact with language. Q How do I combine creativity with my data-driven work? A My passion for crafting extends beyond the digital realm to embrace a hands-on approach to creativity. From knitting intricate patterns inspired by my rabbit’s playful nature to sketching designed to stimulate curiosity and exploration, I find immense fulfillment in blending data-driven insights with the artistry of crafting. Q What is the essence of my journey? A In essence, my journey is defined by a relentless pursuit of innovation, fueled by the boundless possibilities that arise at the nexus of data, creativity, and companionship. With each endeavor, I strive to push the boundaries of what’s possible, shaping a future where technology not only empowers but also enriches our lives in meaningful and unexpected ways. To channel such a passion, I created this blog to document the things I find helpful and important in understanding some of the most crucial concepts. I hope in such a format, I can grow with you or someone who is interested in learning these cool things. Q Is this the only blog? A I have also created a separate blog for documenting the gists of NLP. This page will summarize important theories that lay the foundation for the development of AI, speech &amp; language processing, and computational linguistics to provide more context.","categories":[],"tags":[{"name":"intro","slug":"intro","permalink":"https://shiyisteezin.github.io/nlp-docs/tags/intro/"}]}],"categories":[{"name":"Math","slug":"Math","permalink":"https://shiyisteezin.github.io/nlp-docs/categories/Math/"},{"name":"Other","slug":"Other","permalink":"https://shiyisteezin.github.io/nlp-docs/categories/Other/"},{"name":"Linguistics","slug":"Linguistics","permalink":"https://shiyisteezin.github.io/nlp-docs/categories/Linguistics/"},{"name":"NLP Related","slug":"NLP-Related","permalink":"https://shiyisteezin.github.io/nlp-docs/categories/NLP-Related/"}],"tags":[{"name":"math","slug":"math","permalink":"https://shiyisteezin.github.io/nlp-docs/tags/math/"},{"name":"stochastic","slug":"stochastic","permalink":"https://shiyisteezin.github.io/nlp-docs/tags/stochastic/"},{"name":"determinism","slug":"determinism","permalink":"https://shiyisteezin.github.io/nlp-docs/tags/determinism/"},{"name":"nlp-theories","slug":"nlp-theories","permalink":"https://shiyisteezin.github.io/nlp-docs/tags/nlp-theories/"},{"name":"semantics","slug":"semantics","permalink":"https://shiyisteezin.github.io/nlp-docs/tags/semantics/"},{"name":"intro","slug":"intro","permalink":"https://shiyisteezin.github.io/nlp-docs/tags/intro/"}]}